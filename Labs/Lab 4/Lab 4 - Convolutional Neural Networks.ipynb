{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import *\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a couple of parameters \"meaningfully\" -> consult the tutorial for help\n",
    "https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "\n",
    "We want to use the MNIST dataset (not CIFAR)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chosen to have 20 epochs and 1000 as my training batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size_train = 4\n",
    "batch_size_test = 100\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we need to load the MNIST dataset using the dataloader functions\n",
    "for CIFAR the following transforms are used:\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "It turns out that for MNIST this transform works better due to the difference in value range:\n",
    "torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])\n",
    "                             \n",
    "Observe that the RGB image for CIFAR needs three channels (all 0.5) MNIST has only one gray channel!\n",
    "     \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the dataset. And dividing the dataset in training and testing with the respective batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_set = torchvision.datasets.MNIST('DATA_MNIST/', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size_train, shuffle=True, num_workers=2)\n",
    "\n",
    "test_set = datasets.MNIST('DATA_MNIST/', download=True, train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size_test,shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I recommend testing if the dataloader worked, render one of the images and put the numeral in the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "training_data = enumerate(train_loader)\n",
    "batch_idx, (images, labels) = next(training_data)\n",
    "print(type(images)) # Checking the datatype \n",
    "print(images.shape) # the size of the image\n",
    "print(labels.shape) # the size of the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below plot shows a sample image from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Test Image')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAARK0lEQVR4nO3df6zV9X3H8efLn1MpzB8TGai0FrMtrV4UGUndtJoaRRMw1h/ECFvX4ZKSqVtMtaJgprExYqd2WlGJUBygIhO7OnXi1DWuEZEpSq3omCI3XFEmMK0KvPfH+d7ugPd8zuX8vnxej+TmnvN9n+85b4+87vfH53vORxGBme359mp3A2bWGg67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TD3sEkbS372SHpk7L7F9fwfP8m6buJ+khJIWmf+jq3TuT/qR0sIgb13pa0FvhuRPxr+zqygcxb9gFI0l6SrpL0lqQPJD0o6ZCi9juS5hfL/0fSi5KGSroR+BPgx8WewY/78Tr3S7pT0uPFOr+QdISkv5e0SdKvJI0ue3xvT1skvS7p3LLa3pJmSdoo6b8kTSvfi5A0RNJ9krolvSfpBkl7N/7dy5fDPjD9NTAROAX4fWAT8A9FbQowBDgSOBT4K+CTiLgGeB6YFhGDImJaP1/rAmA6cBjwKfACsKK4/zBwa9lj36L0B2UIcD0wX9KwovaXwFlAF3BC0X+5ucA24KvAaOAMoOIhh+0+h31guhS4JiLWRcSnwEzg28VW8nNKIf9qRGyPiJciYnMdr7WkeI7fAEuA30TEvIjYDiyiFEwAIuKhiFgfETsiYhHwJjC2KF8A3Fb0vAn4Ye96koZS+kNweUT8b0T0AD8CLqqjb9uFj9kHpqOBJZJ2lC3bDgwFfkppq75Q0u8C8yn9Yfi8xtfaUHb7kz7ul59XmAz8DTCyWDSI0h4AlPZA3i1bt/z20cC+QLek3mV77fIYq5PDPjC9C3wnIn5RoX49cL2kkcDPgTeA+4CmfcRR0tHAPcDpwAsRsV3SSqA3vd3AiLJVjiy7/S6lQ4TDImJbs3rMnXfjB6afADcWAUPS70maUNz+pqSvFye3NlPard9erLcB+EqTejqI0h+T94s+/hz4Wln9QeAyScOLPY7v9xYioht4EpglaXBxAvIYSac0qdcsOewD023AUuBJSVuA/wD+uKgdQenE2WZgNfAspV353vW+XZxJv72RDUXE68AsSifwNgBfB8r3PO6hFOhXgJcp7XFs4///EE0G9gNep3TC8WFgGNYw8pdXWDtIOgv4SUQc3e5ecuEtu7WEpAMkjZe0j6ThwAxKZ/etRbxlt5aQdCClQ4o/oHQW/5+By+ocFrTd4LCbZcK78WaZaOk4uyTvRpg1WUSor+V1bdklnSnpDUlrJF1Vz3OZWXPVfMxeXLTxa+BbwDrgRWBSMd5aaR1v2c2arBlb9rHAmoh4OyI+AxYCE+p4PjNronrCPpydP6iwrli2E0lTJS2XtLyO1zKzOtVzgq6vXYUv7KZHxGxgNng33qyd6tmyr2PnTy6NANbX146ZNUs9YX8RGCXpy5L2o/RFA0sb05aZNVrNu/ERsU3SNOAJYG9gTkS81rDOzKyhWnq5rI/ZzZqvKRfVmNnA4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMtnbLZ9jwnnnhisj5t2rSKtcmTJyfXnTdvXrJ+xx13JOsrVqxI1nPjLbtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgnP4mpJXV1dyfqyZcuS9cGDBzewm5199NFHyfqhhx7atNfuZJVmca3rohpJa4EtwHZgW0SMqef5zKx5GnEF3TcjYmMDnsfMmsjH7GaZqDfsATwp6SVJU/t6gKSpkpZLWl7na5lZHerdjf9GRKyXdDjwlKRfRcRz5Q+IiNnAbPAJOrN2qmvLHhHri989wBJgbCOaMrPGqznskg6S9KXe28AZwKpGNWZmjVXPbvxQYImk3uf5x4j4l4Z0ZS0zdmx6Z2zx4sXJ+pAhQ5L11HUcW7ZsSa772WefJevVxtHHjRtXsVbts+7VXnsgqjnsEfE2cHwDezGzJvLQm1kmHHazTDjsZplw2M0y4bCbZcIfcd0DHHjggRVrJ5xwQnLd+fPnJ+sjRoxI1ouh14pS/76qDX/dfPPNyfrChQuT9VRv06dPT6570003JeudrNJHXL1lN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4Smb9wB33313xdqkSZNa2MnuqXYNwKBBg5L1Z599Nlk/9dRTK9aOO+645Lp7Im/ZzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMeJx9ADjxxBOT9bPPPrtirdrnzaupNpb92GOPJeu33HJLxdr69euT67788svJ+qZNm5L10047rWKt3vdlIPKW3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhL83vgN0dXUl68uWLUvWBw8eXPNrP/7448l6tc/Dn3LKKcl66nPj9957b3Ld999/P1mvZvv27RVrH3/8cXLdav9d1b7zvp1q/t54SXMk9UhaVbbsEElPSXqz+H1wI5s1s8brz278/cCZuyy7Cng6IkYBTxf3zayDVQ17RDwHfLjL4gnA3OL2XGBiY9sys0ar9dr4oRHRDRAR3ZIOr/RASVOBqTW+jpk1SNM/CBMRs4HZ4BN0Zu1U69DbBknDAIrfPY1rycyaodawLwWmFLenAI82ph0za5aq4+ySFgCnAocBG4AZwD8BDwJHAe8A50fErifx+nquLHfjjz322GR9xowZyfpFF12UrG/cuLFirbu7O7nuDTfckKw//PDDyXonS42zV/t3v2jRomT94osvrqmnVqg0zl71mD0iKl1VcXpdHZlZS/lyWbNMOOxmmXDYzTLhsJtlwmE3y4S/SroB9t9//2Q99XXKAOPHj0/Wt2zZkqxPnjy5Ym358uXJdQ844IBkPVdHHXVUu1toOG/ZzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMeJy9AUaPHp2sVxtHr2bChAnJerVplc3AW3azbDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMeZ2+AW2+9NVmX+vxm39+qNk7ucfTa7LVX5W3Zjh07WthJZ/CW3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMfZ++mcc86pWOvq6kquW2164KVLl9bSklWRGkuv9v9k5cqVDe6m/apu2SXNkdQjaVXZspmS3pO0svip79sZzKzp+rMbfz9wZh/LfxQRXcXPzxvblpk1WtWwR8RzwIct6MXMmqieE3TTJL1S7OYfXOlBkqZKWi4pPemYmTVVrWG/CzgG6AK6gVmVHhgRsyNiTESMqfG1zKwBagp7RGyIiO0RsQO4Bxjb2LbMrNFqCrukYWV3zwVWVXqsmXWGquPskhYApwKHSVoHzABOldQFBLAWuLR5LXaG1Dzm++23X3Ldnp6eZH3RokU19bSnqzbv/cyZM2t+7mXLliXrV199dc3P3amqhj0iJvWx+L4m9GJmTeTLZc0y4bCbZcJhN8uEw26WCYfdLBP+iGsLfPrpp8l6d3d3izrpLNWG1qZPn56sX3nllcn6unXrKtZmzap40ScAW7duTdYHIm/ZzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMeJy9BXL+qujU12xXGye/8MILk/VHH300WT/vvPOS9dx4y26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcLj7P0kqaYawMSJE5P1yy67rJaWOsIVV1yRrF977bUVa0OGDEmu+8ADDyTrkydPTtZtZ96ym2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZ6M+UzUcC84AjgB3A7Ii4TdIhwCJgJKVpmy+IiE3Na7W9IqKmGsARRxyRrN9+++3J+pw5c5L1Dz74oGJt3LhxyXUvueSSZP34449P1keMGJGsv/POOxVrTzzxRHLdO++8M1m33dOfLfs24G8j4g+BccD3JP0RcBXwdESMAp4u7ptZh6oa9ojojogVxe0twGpgODABmFs8bC4wsUk9mlkD7NYxu6SRwGjgl8DQiOiG0h8E4PCGd2dmDdPva+MlDQIWA5dHxOZq14OXrTcVmFpbe2bWKP3askval1LQH4iIR4rFGyQNK+rDgJ6+1o2I2RExJiLGNKJhM6tN1bCrtAm/D1gdEbeWlZYCU4rbU4D0V32aWVup2rCRpJOB54FXKQ29AfyA0nH7g8BRwDvA+RHxYZXnSr9YBzv//PMr1hYsWNDU196wYUOyvnnz5oq1UaNGNbqdnbzwwgvJ+jPPPFOxdt111zW6HQMios9j7KrH7BHx70ClA/TT62nKzFrHV9CZZcJhN8uEw26WCYfdLBMOu1kmHHazTFQdZ2/oiw3gcfbURzkfeuih5LonnXRSXa9d7dLkev4fpj4eC7Bw4cJkfSB/DfaeqtI4u7fsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmPM7eAMOGDUvWL7300mR9+vTpyXo94+y33XZbct277rorWV+zZk2ybp3H4+xmmXPYzTLhsJtlwmE3y4TDbpYJh90sEw67WSY8zm62h/E4u1nmHHazTDjsZplw2M0y4bCbZcJhN8uEw26Wiaphl3SkpGckrZb0mqTLiuUzJb0naWXxM7757ZpZrapeVCNpGDAsIlZI+hLwEjARuADYGhG39PvFfFGNWdNVuqhmn36s2A10F7e3SFoNDG9se2bWbLt1zC5pJDAa+GWxaJqkVyTNkXRwhXWmSlouaXl9rZpZPfp9bbykQcCzwI0R8YikocBGIIC/o7Sr/50qz+HdeLMmq7Qb36+wS9oX+BnwRETc2kd9JPCziPhaledx2M2arOYPwqj01ab3AavLg16cuOt1LrCq3ibNrHn6czb+ZOB54FVgR7H4B8AkoIvSbvxa4NLiZF7qubxlN2uyunbjG8VhN2s+f57dLHMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZaLqF0422Ebgv8vuH1Ys60Sd2lun9gXurVaN7O3oSoWWfp79Cy8uLY+IMW1rIKFTe+vUvsC91apVvXk33iwTDrtZJtod9tltfv2UTu2tU/sC91arlvTW1mN2M2uddm/ZzaxFHHazTLQl7JLOlPSGpDWSrmpHD5VIWivp1WIa6rbOT1fModcjaVXZskMkPSXpzeJ3n3Pstam3jpjGOzHNeFvfu3ZPf97yY3ZJewO/Br4FrANeBCZFxOstbaQCSWuBMRHR9gswJP0psBWY1zu1lqSbgQ8j4ofFH8qDI+L7HdLbTHZzGu8m9VZpmvE/o43vXSOnP69FO7bsY4E1EfF2RHwGLAQmtKGPjhcRzwEf7rJ4AjC3uD2X0j+WlqvQW0eIiO6IWFHc3gL0TjPe1vcu0VdLtCPsw4F3y+6vo7Pmew/gSUkvSZra7mb6MLR3mq3i9+Ft7mdXVafxbqVdphnvmPeulunP69WOsPc1NU0njf99IyJOAM4Cvlfsrlr/3AUcQ2kOwG5gVjubKaYZXwxcHhGb29lLuT76asn71o6wrwOOLLs/Aljfhj76FBHri989wBJKhx2dZEPvDLrF75429/NbEbEhIrZHxA7gHtr43hXTjC8GHoiIR4rFbX/v+uqrVe9bO8L+IjBK0pcl7QdcBCxtQx9fIOmg4sQJkg4CzqDzpqJeCkwpbk8BHm1jLzvplGm8K00zTpvfu7ZPfx4RLf8BxlM6I/8WcE07eqjQ11eA/yx+Xmt3b8ACSrt1n1PaI/oL4FDgaeDN4vchHdTbTylN7f0KpWANa1NvJ1M6NHwFWFn8jG/3e5foqyXvmy+XNcuEr6Azy4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLxf4DleP6KXzxjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_set[1][0][0], cmap = \"gray\" )\n",
    "plt.title('Test Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'label 2')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.imshow(images[62][0], cmap='gray')\n",
    "plt.title(\"label {}\".format(labels[62]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define our convolutional neural network, it should have 1 convolutional layer (I recommend handing over the kernel size as a parameter at construction) and 1 pooling layer followed by other normal layers\n",
    "\n",
    "You only have one in channel (gray)\n",
    "\n",
    "the number of out channels is up to you - pick wisely, the same applies to the other weights of the other layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My below class will be used as my CNN where I have declared all the variables which is been required. I chose to have 10 output features. I chosen to have relu actavitaion function with the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCNN(nn.Module):\n",
    "    def __init__(self,kernelSize=2):\n",
    "        super(myCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32,kernel_size=kernelSize)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=kernelSize)\n",
    "        self.fc1 = nn.Linear(in_features=32*13*13, out_features=64)          \n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # I filled in the first two layers\n",
    "        # do first x=self.conv1(x)\n",
    "        self.features=F.relu(self.conv1(x))\n",
    "        x = self.pool(self.features)\n",
    "        #we need to flatten the whole thing\n",
    "#         a = x.view(-1, numpy.product(x.shape))\n",
    "#         x = x.view(x.size(0),-1)\n",
    "        x = x.view(-1, 32*13*13)\n",
    "        a = self.fc1(x)\n",
    "        a = F.relu(a)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "        a = self.fc2(a)\n",
    "        a = F.log_softmax(a,dim=1)\n",
    "        \n",
    "        #code is missing here!\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters definition\n",
    "n_epochs = 10\n",
    "batch_size_train = 4\n",
    "batch_size_test = 100\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data\\MNIST\\raw\\train-images-idx3-ubyte.gz to mnist_data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to mnist_data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to mnist_data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%C:\\Users\\makn0023\\Anaconda3\\envs\\mlc\\lib\\site-packages\\torchvision\\datasets\\mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to mnist_data\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "Trans=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])\n",
    "train_set = torchvision.datasets.MNIST(root='mnist_data', train=True, download=True,\n",
    "                             transform=Trans)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size_train, shuffle = True)\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(root='mnist_data', train=False, download=True,\n",
    "                             transform=Trans)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size_test, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCNN(nn.Module):\n",
    "    def __init__(self,kernelSize):\n",
    "        super(myCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels= 6, kernel_size=kernelSize)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(6*6*6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.output = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 6*6*6)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myCNN(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=216, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (output): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = myCNN(2)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My below variables have the Classification Cross-Entropy loss and SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now implement the training and testing code. Toggle the training and testing for the convolutional neural network\n",
    "using the model.train() and model.eval() functions!\n",
    "Choose your own optimiser and loss function!\n",
    "\n",
    "Collect training and testing accuracy over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traintest_function():\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "    train_acc = []\n",
    "    train_final_acc = []\n",
    "    test_acc = []\n",
    "    train_counter=[]\n",
    "\n",
    "    epochs = 4\n",
    "    test_counter = [i*len(train_loader.dataset) for i in range(epochs)]\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        train_corrects = 0.0 \n",
    "        train_batch_loss = 0.0\n",
    "        train_epoch_loss = 0.0\n",
    "        val_corrects = 0.0\n",
    "        val_epoch_loss = 0.0\n",
    "\n",
    "        # loop through 60000 samples of the train set, 100 at a time\n",
    "        for batch_idx, data in enumerate(train_loader, start=1):\n",
    "            inputs = data[0]\n",
    "            labels = data[1]\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            #define optimizer and loss function\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Return the index of the highest possibility\n",
    "            # which are the predicted labels\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_batch_loss += loss.item()\n",
    "\n",
    "            # sum up all the correct prediction\n",
    "            train_corrects += (preds == labels).sum().item()\n",
    "\n",
    "            # print training loss every 100 mini-batch\n",
    "            # train_batch_loss is the average loss for 100 mini-batch\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.2f}'.format(\n",
    "                        e + 1 , \n",
    "                        batch_idx * len(data[0]), \n",
    "                        len(train_loader.dataset),\n",
    "                        100.* batch_idx * len(data[0]) / len(train_loader.dataset), \n",
    "                        train_batch_loss / 100 ))\n",
    "                train_counter.append(\n",
    "                 (batch_idx*64) + ((epochs)*len(train_loader.dataset)))\n",
    "                tr_acc= 100.* batch_idx * len(data[0]) / len(train_loader.dataset)\n",
    "                train_acc.append(tr_acc)\n",
    "                # accumulate loss for the epoch\n",
    "                train_epoch_loss += train_batch_loss\n",
    "                # reset the loss for every mini-batch\n",
    "                train_batch_loss = 0.0\n",
    "        else:\n",
    "            # torch.no_grad deactivate the auograd engine, \n",
    "            # reduce memory usage and speed up computations\n",
    "            with torch.no_grad():\n",
    "                for val_data in test_loader:\n",
    "                    val_inputs = val_data[0]\n",
    "                    val_labels = val_data[1]\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "                    _, val_preds = torch.max(val_outputs, 1)\n",
    "                    val_epoch_loss += val_loss.item()\n",
    "                    val_corrects += (val_preds == val_labels).sum().item()\n",
    "\n",
    "\n",
    "            # print result for every epoch \n",
    "            train_accuracy = 100. * train_corrects / len(train_loader.dataset)\n",
    "            train_final_acc.append(train_accuracy)\n",
    "            # here batch_idx is the total number of mini-batch = 600\n",
    "            train_epoch_loss /= batch_idx\n",
    "\n",
    "            #print('epoch :', (e+1))\n",
    "            #print('Train set: Accuracy: {}/{} ({:.0f}%), Average Loss: {:.2f}'.format(\n",
    "             #       train_corrects, len(train_loader.dataset),\n",
    "              #      train_accuracy, train_epoch_loss))\n",
    "\n",
    "\n",
    "\n",
    "            val_accuracy = 100. * val_corrects / len(test_loader.dataset)\n",
    "            test_acc.append(val_accuracy)\n",
    "            val_epoch_loss /= batch_idx\n",
    "\n",
    "            print('Test set: Accuracy: {}/{} ({:.0f}%), Average Loss: {:.2f}'.format(\n",
    "                    val_corrects, len(test_loader.dataset),\n",
    "                    val_accuracy, val_epoch_loss))\n",
    "    return train_final_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [400/60000 (1%)] Loss: 2.28\n",
      "Train Epoch: 1 [800/60000 (1%)] Loss: 1.34\n",
      "Train Epoch: 1 [1200/60000 (2%)] Loss: 0.99\n",
      "Train Epoch: 1 [1600/60000 (3%)] Loss: 0.78\n",
      "Train Epoch: 1 [2000/60000 (3%)] Loss: 0.72\n",
      "Train Epoch: 1 [2400/60000 (4%)] Loss: 0.51\n",
      "Train Epoch: 1 [2800/60000 (5%)] Loss: 0.44\n",
      "Train Epoch: 1 [3200/60000 (5%)] Loss: 0.60\n",
      "Train Epoch: 1 [3600/60000 (6%)] Loss: 0.44\n",
      "Train Epoch: 1 [4000/60000 (7%)] Loss: 0.45\n",
      "Train Epoch: 1 [4400/60000 (7%)] Loss: 0.36\n",
      "Train Epoch: 1 [4800/60000 (8%)] Loss: 0.46\n",
      "Train Epoch: 1 [5200/60000 (9%)] Loss: 0.48\n",
      "Train Epoch: 1 [5600/60000 (9%)] Loss: 0.40\n",
      "Train Epoch: 1 [6000/60000 (10%)] Loss: 0.43\n",
      "Train Epoch: 1 [6400/60000 (11%)] Loss: 0.44\n",
      "Train Epoch: 1 [6800/60000 (11%)] Loss: 0.40\n",
      "Train Epoch: 1 [7200/60000 (12%)] Loss: 0.38\n",
      "Train Epoch: 1 [7600/60000 (13%)] Loss: 0.33\n",
      "Train Epoch: 1 [8000/60000 (13%)] Loss: 0.45\n",
      "Train Epoch: 1 [8400/60000 (14%)] Loss: 0.44\n",
      "Train Epoch: 1 [8800/60000 (15%)] Loss: 0.46\n",
      "Train Epoch: 1 [9200/60000 (15%)] Loss: 0.36\n",
      "Train Epoch: 1 [9600/60000 (16%)] Loss: 0.38\n",
      "Train Epoch: 1 [10000/60000 (17%)] Loss: 0.44\n",
      "Train Epoch: 1 [10400/60000 (17%)] Loss: 0.39\n",
      "Train Epoch: 1 [10800/60000 (18%)] Loss: 0.52\n",
      "Train Epoch: 1 [11200/60000 (19%)] Loss: 0.34\n",
      "Train Epoch: 1 [11600/60000 (19%)] Loss: 0.27\n",
      "Train Epoch: 1 [12000/60000 (20%)] Loss: 0.32\n",
      "Train Epoch: 1 [12400/60000 (21%)] Loss: 0.43\n",
      "Train Epoch: 1 [12800/60000 (21%)] Loss: 0.38\n",
      "Train Epoch: 1 [13200/60000 (22%)] Loss: 0.33\n",
      "Train Epoch: 1 [13600/60000 (23%)] Loss: 0.35\n",
      "Train Epoch: 1 [14000/60000 (23%)] Loss: 0.27\n",
      "Train Epoch: 1 [14400/60000 (24%)] Loss: 0.39\n",
      "Train Epoch: 1 [14800/60000 (25%)] Loss: 0.49\n",
      "Train Epoch: 1 [15200/60000 (25%)] Loss: 0.29\n",
      "Train Epoch: 1 [15600/60000 (26%)] Loss: 0.30\n",
      "Train Epoch: 1 [16000/60000 (27%)] Loss: 0.39\n",
      "Train Epoch: 1 [16400/60000 (27%)] Loss: 0.30\n",
      "Train Epoch: 1 [16800/60000 (28%)] Loss: 0.28\n",
      "Train Epoch: 1 [17200/60000 (29%)] Loss: 0.38\n",
      "Train Epoch: 1 [17600/60000 (29%)] Loss: 0.34\n",
      "Train Epoch: 1 [18000/60000 (30%)] Loss: 0.37\n",
      "Train Epoch: 1 [18400/60000 (31%)] Loss: 0.47\n",
      "Train Epoch: 1 [18800/60000 (31%)] Loss: 0.39\n",
      "Train Epoch: 1 [19200/60000 (32%)] Loss: 0.38\n",
      "Train Epoch: 1 [19600/60000 (33%)] Loss: 0.32\n",
      "Train Epoch: 1 [20000/60000 (33%)] Loss: 0.32\n",
      "Train Epoch: 1 [20400/60000 (34%)] Loss: 0.45\n",
      "Train Epoch: 1 [20800/60000 (35%)] Loss: 0.34\n",
      "Train Epoch: 1 [21200/60000 (35%)] Loss: 0.37\n",
      "Train Epoch: 1 [21600/60000 (36%)] Loss: 0.37\n",
      "Train Epoch: 1 [22000/60000 (37%)] Loss: 0.36\n",
      "Train Epoch: 1 [22400/60000 (37%)] Loss: 0.30\n",
      "Train Epoch: 1 [22800/60000 (38%)] Loss: 0.32\n",
      "Train Epoch: 1 [23200/60000 (39%)] Loss: 0.44\n",
      "Train Epoch: 1 [23600/60000 (39%)] Loss: 0.21\n",
      "Train Epoch: 1 [24000/60000 (40%)] Loss: 0.42\n",
      "Train Epoch: 1 [24400/60000 (41%)] Loss: 0.35\n",
      "Train Epoch: 1 [24800/60000 (41%)] Loss: 0.34\n",
      "Train Epoch: 1 [25200/60000 (42%)] Loss: 0.43\n",
      "Train Epoch: 1 [25600/60000 (43%)] Loss: 0.42\n",
      "Train Epoch: 1 [26000/60000 (43%)] Loss: 0.29\n",
      "Train Epoch: 1 [26400/60000 (44%)] Loss: 0.33\n",
      "Train Epoch: 1 [26800/60000 (45%)] Loss: 0.30\n",
      "Train Epoch: 1 [27200/60000 (45%)] Loss: 0.42\n",
      "Train Epoch: 1 [27600/60000 (46%)] Loss: 0.23\n",
      "Train Epoch: 1 [28000/60000 (47%)] Loss: 0.36\n",
      "Train Epoch: 1 [28400/60000 (47%)] Loss: 0.53\n",
      "Train Epoch: 1 [28800/60000 (48%)] Loss: 0.31\n",
      "Train Epoch: 1 [29200/60000 (49%)] Loss: 0.34\n",
      "Train Epoch: 1 [29600/60000 (49%)] Loss: 0.34\n",
      "Train Epoch: 1 [30000/60000 (50%)] Loss: 0.35\n",
      "Train Epoch: 1 [30400/60000 (51%)] Loss: 0.30\n",
      "Train Epoch: 1 [30800/60000 (51%)] Loss: 0.32\n",
      "Train Epoch: 1 [31200/60000 (52%)] Loss: 0.36\n",
      "Train Epoch: 1 [31600/60000 (53%)] Loss: 0.29\n",
      "Train Epoch: 1 [32000/60000 (53%)] Loss: 0.40\n",
      "Train Epoch: 1 [32400/60000 (54%)] Loss: 0.36\n",
      "Train Epoch: 1 [32800/60000 (55%)] Loss: 0.27\n",
      "Train Epoch: 1 [33200/60000 (55%)] Loss: 0.34\n",
      "Train Epoch: 1 [33600/60000 (56%)] Loss: 0.33\n",
      "Train Epoch: 1 [34000/60000 (57%)] Loss: 0.27\n",
      "Train Epoch: 1 [34400/60000 (57%)] Loss: 0.34\n",
      "Train Epoch: 1 [34800/60000 (58%)] Loss: 0.27\n",
      "Train Epoch: 1 [35200/60000 (59%)] Loss: 0.38\n",
      "Train Epoch: 1 [35600/60000 (59%)] Loss: 0.21\n",
      "Train Epoch: 1 [36000/60000 (60%)] Loss: 0.30\n",
      "Train Epoch: 1 [36400/60000 (61%)] Loss: 0.33\n",
      "Train Epoch: 1 [36800/60000 (61%)] Loss: 0.29\n",
      "Train Epoch: 1 [37200/60000 (62%)] Loss: 0.44\n",
      "Train Epoch: 1 [37600/60000 (63%)] Loss: 0.44\n",
      "Train Epoch: 1 [38000/60000 (63%)] Loss: 0.27\n",
      "Train Epoch: 1 [38400/60000 (64%)] Loss: 0.39\n",
      "Train Epoch: 1 [38800/60000 (65%)] Loss: 0.45\n",
      "Train Epoch: 1 [39200/60000 (65%)] Loss: 0.24\n",
      "Train Epoch: 1 [39600/60000 (66%)] Loss: 0.32\n",
      "Train Epoch: 1 [40000/60000 (67%)] Loss: 0.31\n",
      "Train Epoch: 1 [40400/60000 (67%)] Loss: 0.28\n",
      "Train Epoch: 1 [40800/60000 (68%)] Loss: 0.35\n",
      "Train Epoch: 1 [41200/60000 (69%)] Loss: 0.38\n",
      "Train Epoch: 1 [41600/60000 (69%)] Loss: 0.42\n",
      "Train Epoch: 1 [42000/60000 (70%)] Loss: 0.42\n",
      "Train Epoch: 1 [42400/60000 (71%)] Loss: 0.32\n",
      "Train Epoch: 1 [42800/60000 (71%)] Loss: 0.36\n",
      "Train Epoch: 1 [43200/60000 (72%)] Loss: 0.35\n",
      "Train Epoch: 1 [43600/60000 (73%)] Loss: 0.17\n",
      "Train Epoch: 1 [44000/60000 (73%)] Loss: 0.45\n",
      "Train Epoch: 1 [44400/60000 (74%)] Loss: 0.38\n",
      "Train Epoch: 1 [44800/60000 (75%)] Loss: 0.33\n",
      "Train Epoch: 1 [45200/60000 (75%)] Loss: 0.40\n",
      "Train Epoch: 1 [45600/60000 (76%)] Loss: 0.27\n",
      "Train Epoch: 1 [46000/60000 (77%)] Loss: 0.33\n",
      "Train Epoch: 1 [46400/60000 (77%)] Loss: 0.47\n",
      "Train Epoch: 1 [46800/60000 (78%)] Loss: 0.38\n",
      "Train Epoch: 1 [47200/60000 (79%)] Loss: 0.27\n",
      "Train Epoch: 1 [47600/60000 (79%)] Loss: 0.30\n",
      "Train Epoch: 1 [48000/60000 (80%)] Loss: 0.29\n",
      "Train Epoch: 1 [48400/60000 (81%)] Loss: 0.30\n",
      "Train Epoch: 1 [48800/60000 (81%)] Loss: 0.24\n",
      "Train Epoch: 1 [49200/60000 (82%)] Loss: 0.34\n",
      "Train Epoch: 1 [49600/60000 (83%)] Loss: 0.35\n",
      "Train Epoch: 1 [50000/60000 (83%)] Loss: 0.42\n",
      "Train Epoch: 1 [50400/60000 (84%)] Loss: 0.41\n",
      "Train Epoch: 1 [50800/60000 (85%)] Loss: 0.32\n",
      "Train Epoch: 1 [51200/60000 (85%)] Loss: 0.30\n",
      "Train Epoch: 1 [51600/60000 (86%)] Loss: 0.34\n",
      "Train Epoch: 1 [52000/60000 (87%)] Loss: 0.33\n",
      "Train Epoch: 1 [52400/60000 (87%)] Loss: 0.20\n",
      "Train Epoch: 1 [52800/60000 (88%)] Loss: 0.24\n",
      "Train Epoch: 1 [53200/60000 (89%)] Loss: 0.25\n",
      "Train Epoch: 1 [53600/60000 (89%)] Loss: 0.34\n",
      "Train Epoch: 1 [54000/60000 (90%)] Loss: 0.40\n",
      "Train Epoch: 1 [54400/60000 (91%)] Loss: 0.40\n",
      "Train Epoch: 1 [54800/60000 (91%)] Loss: 0.27\n",
      "Train Epoch: 1 [55200/60000 (92%)] Loss: 0.40\n",
      "Train Epoch: 1 [55600/60000 (93%)] Loss: 0.37\n",
      "Train Epoch: 1 [56000/60000 (93%)] Loss: 0.25\n",
      "Train Epoch: 1 [56400/60000 (94%)] Loss: 0.32\n",
      "Train Epoch: 1 [56800/60000 (95%)] Loss: 0.27\n",
      "Train Epoch: 1 [57200/60000 (95%)] Loss: 0.32\n",
      "Train Epoch: 1 [57600/60000 (96%)] Loss: 0.33\n",
      "Train Epoch: 1 [58000/60000 (97%)] Loss: 0.26\n",
      "Train Epoch: 1 [58400/60000 (97%)] Loss: 0.30\n",
      "Train Epoch: 1 [58800/60000 (98%)] Loss: 0.26\n",
      "Train Epoch: 1 [59200/60000 (99%)] Loss: 0.42\n",
      "Train Epoch: 1 [59600/60000 (99%)] Loss: 0.26\n",
      "Train Epoch: 1 [60000/60000 (100%)] Loss: 0.43\n",
      "Test set: Accuracy: 9156.0/10000 (92%), Average Loss: 0.00\n",
      "Train Epoch: 2 [400/60000 (1%)] Loss: 0.38\n",
      "Train Epoch: 2 [800/60000 (1%)] Loss: 0.24\n",
      "Train Epoch: 2 [1200/60000 (2%)] Loss: 0.22\n",
      "Train Epoch: 2 [1600/60000 (3%)] Loss: 0.44\n",
      "Train Epoch: 2 [2000/60000 (3%)] Loss: 0.28\n",
      "Train Epoch: 2 [2400/60000 (4%)] Loss: 0.24\n",
      "Train Epoch: 2 [2800/60000 (5%)] Loss: 0.34\n",
      "Train Epoch: 2 [3200/60000 (5%)] Loss: 0.33\n",
      "Train Epoch: 2 [3600/60000 (6%)] Loss: 0.31\n",
      "Train Epoch: 2 [4000/60000 (7%)] Loss: 0.31\n",
      "Train Epoch: 2 [4400/60000 (7%)] Loss: 0.23\n",
      "Train Epoch: 2 [4800/60000 (8%)] Loss: 0.33\n",
      "Train Epoch: 2 [5200/60000 (9%)] Loss: 0.39\n",
      "Train Epoch: 2 [5600/60000 (9%)] Loss: 0.23\n",
      "Train Epoch: 2 [6000/60000 (10%)] Loss: 0.36\n",
      "Train Epoch: 2 [6400/60000 (11%)] Loss: 0.34\n",
      "Train Epoch: 2 [6800/60000 (11%)] Loss: 0.34\n",
      "Train Epoch: 2 [7200/60000 (12%)] Loss: 0.28\n",
      "Train Epoch: 2 [7600/60000 (13%)] Loss: 0.45\n",
      "Train Epoch: 2 [8000/60000 (13%)] Loss: 0.23\n",
      "Train Epoch: 2 [8400/60000 (14%)] Loss: 0.35\n",
      "Train Epoch: 2 [8800/60000 (15%)] Loss: 0.29\n",
      "Train Epoch: 2 [9200/60000 (15%)] Loss: 0.30\n",
      "Train Epoch: 2 [9600/60000 (16%)] Loss: 0.22\n",
      "Train Epoch: 2 [10000/60000 (17%)] Loss: 0.32\n",
      "Train Epoch: 2 [10400/60000 (17%)] Loss: 0.65\n",
      "Train Epoch: 2 [10800/60000 (18%)] Loss: 0.26\n",
      "Train Epoch: 2 [11200/60000 (19%)] Loss: 0.36\n",
      "Train Epoch: 2 [11600/60000 (19%)] Loss: 0.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [12000/60000 (20%)] Loss: 0.25\n",
      "Train Epoch: 2 [12400/60000 (21%)] Loss: 0.19\n",
      "Train Epoch: 2 [12800/60000 (21%)] Loss: 0.28\n",
      "Train Epoch: 2 [13200/60000 (22%)] Loss: 0.31\n",
      "Train Epoch: 2 [13600/60000 (23%)] Loss: 0.37\n",
      "Train Epoch: 2 [14000/60000 (23%)] Loss: 0.18\n",
      "Train Epoch: 2 [14400/60000 (24%)] Loss: 0.33\n",
      "Train Epoch: 2 [14800/60000 (25%)] Loss: 0.55\n",
      "Train Epoch: 2 [15200/60000 (25%)] Loss: 0.41\n",
      "Train Epoch: 2 [15600/60000 (26%)] Loss: 0.29\n",
      "Train Epoch: 2 [16000/60000 (27%)] Loss: 0.35\n",
      "Train Epoch: 2 [16400/60000 (27%)] Loss: 0.40\n",
      "Train Epoch: 2 [16800/60000 (28%)] Loss: 0.34\n",
      "Train Epoch: 2 [17200/60000 (29%)] Loss: 0.33\n",
      "Train Epoch: 2 [17600/60000 (29%)] Loss: 0.25\n",
      "Train Epoch: 2 [18000/60000 (30%)] Loss: 0.31\n",
      "Train Epoch: 2 [18400/60000 (31%)] Loss: 0.43\n",
      "Train Epoch: 2 [18800/60000 (31%)] Loss: 0.21\n",
      "Train Epoch: 2 [19200/60000 (32%)] Loss: 0.41\n",
      "Train Epoch: 2 [19600/60000 (33%)] Loss: 0.25\n",
      "Train Epoch: 2 [20000/60000 (33%)] Loss: 0.37\n",
      "Train Epoch: 2 [20400/60000 (34%)] Loss: 0.46\n",
      "Train Epoch: 2 [20800/60000 (35%)] Loss: 0.29\n",
      "Train Epoch: 2 [21200/60000 (35%)] Loss: 0.46\n",
      "Train Epoch: 2 [21600/60000 (36%)] Loss: 0.22\n",
      "Train Epoch: 2 [22000/60000 (37%)] Loss: 0.29\n",
      "Train Epoch: 2 [22400/60000 (37%)] Loss: 0.32\n",
      "Train Epoch: 2 [22800/60000 (38%)] Loss: 0.42\n",
      "Train Epoch: 2 [23200/60000 (39%)] Loss: 0.31\n",
      "Train Epoch: 2 [23600/60000 (39%)] Loss: 0.21\n",
      "Train Epoch: 2 [24000/60000 (40%)] Loss: 0.32\n",
      "Train Epoch: 2 [24400/60000 (41%)] Loss: 0.28\n",
      "Train Epoch: 2 [24800/60000 (41%)] Loss: 0.35\n",
      "Train Epoch: 2 [25200/60000 (42%)] Loss: 0.23\n",
      "Train Epoch: 2 [25600/60000 (43%)] Loss: 0.30\n",
      "Train Epoch: 2 [26000/60000 (43%)] Loss: 0.26\n",
      "Train Epoch: 2 [26400/60000 (44%)] Loss: 0.39\n",
      "Train Epoch: 2 [26800/60000 (45%)] Loss: 0.21\n",
      "Train Epoch: 2 [27200/60000 (45%)] Loss: 0.45\n",
      "Train Epoch: 2 [27600/60000 (46%)] Loss: 0.36\n",
      "Train Epoch: 2 [28000/60000 (47%)] Loss: 0.25\n",
      "Train Epoch: 2 [28400/60000 (47%)] Loss: 0.28\n",
      "Train Epoch: 2 [28800/60000 (48%)] Loss: 0.25\n",
      "Train Epoch: 2 [29200/60000 (49%)] Loss: 0.38\n",
      "Train Epoch: 2 [29600/60000 (49%)] Loss: 0.35\n",
      "Train Epoch: 2 [30000/60000 (50%)] Loss: 0.41\n",
      "Train Epoch: 2 [30400/60000 (51%)] Loss: 0.36\n",
      "Train Epoch: 2 [30800/60000 (51%)] Loss: 0.25\n",
      "Train Epoch: 2 [31200/60000 (52%)] Loss: 0.30\n",
      "Train Epoch: 2 [31600/60000 (53%)] Loss: 0.35\n",
      "Train Epoch: 2 [32000/60000 (53%)] Loss: 0.40\n",
      "Train Epoch: 2 [32400/60000 (54%)] Loss: 0.37\n",
      "Train Epoch: 2 [32800/60000 (55%)] Loss: 0.30\n",
      "Train Epoch: 2 [33200/60000 (55%)] Loss: 0.29\n",
      "Train Epoch: 2 [33600/60000 (56%)] Loss: 0.30\n",
      "Train Epoch: 2 [34000/60000 (57%)] Loss: 0.29\n",
      "Train Epoch: 2 [34400/60000 (57%)] Loss: 0.26\n",
      "Train Epoch: 2 [34800/60000 (58%)] Loss: 0.54\n",
      "Train Epoch: 2 [35200/60000 (59%)] Loss: 0.33\n",
      "Train Epoch: 2 [35600/60000 (59%)] Loss: 0.25\n",
      "Train Epoch: 2 [36000/60000 (60%)] Loss: 0.40\n",
      "Train Epoch: 2 [36400/60000 (61%)] Loss: 0.25\n",
      "Train Epoch: 2 [36800/60000 (61%)] Loss: 0.30\n",
      "Train Epoch: 2 [37200/60000 (62%)] Loss: 0.26\n",
      "Train Epoch: 2 [37600/60000 (63%)] Loss: 0.32\n",
      "Train Epoch: 2 [38000/60000 (63%)] Loss: 0.36\n",
      "Train Epoch: 2 [38400/60000 (64%)] Loss: 0.34\n",
      "Train Epoch: 2 [38800/60000 (65%)] Loss: 0.18\n",
      "Train Epoch: 2 [39200/60000 (65%)] Loss: 0.33\n",
      "Train Epoch: 2 [39600/60000 (66%)] Loss: 0.35\n",
      "Train Epoch: 2 [40000/60000 (67%)] Loss: 0.21\n",
      "Train Epoch: 2 [40400/60000 (67%)] Loss: 0.33\n",
      "Train Epoch: 2 [40800/60000 (68%)] Loss: 0.48\n",
      "Train Epoch: 2 [41200/60000 (69%)] Loss: 0.21\n",
      "Train Epoch: 2 [41600/60000 (69%)] Loss: 0.31\n",
      "Train Epoch: 2 [42000/60000 (70%)] Loss: 0.31\n",
      "Train Epoch: 2 [42400/60000 (71%)] Loss: 0.34\n",
      "Train Epoch: 2 [42800/60000 (71%)] Loss: 0.24\n",
      "Train Epoch: 2 [43200/60000 (72%)] Loss: 0.38\n",
      "Train Epoch: 2 [43600/60000 (73%)] Loss: 0.30\n",
      "Train Epoch: 2 [44000/60000 (73%)] Loss: 0.26\n",
      "Train Epoch: 2 [44400/60000 (74%)] Loss: 0.42\n",
      "Train Epoch: 2 [44800/60000 (75%)] Loss: 0.58\n",
      "Train Epoch: 2 [45200/60000 (75%)] Loss: 0.33\n",
      "Train Epoch: 2 [45600/60000 (76%)] Loss: 0.25\n",
      "Train Epoch: 2 [46000/60000 (77%)] Loss: 0.35\n",
      "Train Epoch: 2 [46400/60000 (77%)] Loss: 0.41\n",
      "Train Epoch: 2 [46800/60000 (78%)] Loss: 0.20\n",
      "Train Epoch: 2 [47200/60000 (79%)] Loss: 0.20\n",
      "Train Epoch: 2 [47600/60000 (79%)] Loss: 0.24\n",
      "Train Epoch: 2 [48000/60000 (80%)] Loss: 0.29\n",
      "Train Epoch: 2 [48400/60000 (81%)] Loss: 0.21\n",
      "Train Epoch: 2 [48800/60000 (81%)] Loss: 0.20\n",
      "Train Epoch: 2 [49200/60000 (82%)] Loss: 0.37\n",
      "Train Epoch: 2 [49600/60000 (83%)] Loss: 0.43\n",
      "Train Epoch: 2 [50000/60000 (83%)] Loss: 0.24\n",
      "Train Epoch: 2 [50400/60000 (84%)] Loss: 0.32\n",
      "Train Epoch: 2 [50800/60000 (85%)] Loss: 0.29\n",
      "Train Epoch: 2 [51200/60000 (85%)] Loss: 0.32\n",
      "Train Epoch: 2 [51600/60000 (86%)] Loss: 0.29\n",
      "Train Epoch: 2 [52000/60000 (87%)] Loss: 0.23\n",
      "Train Epoch: 2 [52400/60000 (87%)] Loss: 0.27\n",
      "Train Epoch: 2 [52800/60000 (88%)] Loss: 0.23\n",
      "Train Epoch: 2 [53200/60000 (89%)] Loss: 0.27\n",
      "Train Epoch: 2 [53600/60000 (89%)] Loss: 0.29\n",
      "Train Epoch: 2 [54000/60000 (90%)] Loss: 0.36\n",
      "Train Epoch: 2 [54400/60000 (91%)] Loss: 0.39\n",
      "Train Epoch: 2 [54800/60000 (91%)] Loss: 0.31\n",
      "Train Epoch: 2 [55200/60000 (92%)] Loss: 0.43\n",
      "Train Epoch: 2 [55600/60000 (93%)] Loss: 0.26\n",
      "Train Epoch: 2 [56000/60000 (93%)] Loss: 0.48\n",
      "Train Epoch: 2 [56400/60000 (94%)] Loss: 0.25\n",
      "Train Epoch: 2 [56800/60000 (95%)] Loss: 0.22\n",
      "Train Epoch: 2 [57200/60000 (95%)] Loss: 0.20\n",
      "Train Epoch: 2 [57600/60000 (96%)] Loss: 0.34\n",
      "Train Epoch: 2 [58000/60000 (97%)] Loss: 0.29\n",
      "Train Epoch: 2 [58400/60000 (97%)] Loss: 0.37\n",
      "Train Epoch: 2 [58800/60000 (98%)] Loss: 0.28\n",
      "Train Epoch: 2 [59200/60000 (99%)] Loss: 0.34\n",
      "Train Epoch: 2 [59600/60000 (99%)] Loss: 0.33\n",
      "Train Epoch: 2 [60000/60000 (100%)] Loss: 0.31\n",
      "Test set: Accuracy: 9419.0/10000 (94%), Average Loss: 0.00\n",
      "Train Epoch: 3 [400/60000 (1%)] Loss: 0.28\n",
      "Train Epoch: 3 [800/60000 (1%)] Loss: 0.37\n",
      "Train Epoch: 3 [1200/60000 (2%)] Loss: 0.40\n",
      "Train Epoch: 3 [1600/60000 (3%)] Loss: 0.22\n",
      "Train Epoch: 3 [2000/60000 (3%)] Loss: 0.26\n",
      "Train Epoch: 3 [2400/60000 (4%)] Loss: 0.31\n",
      "Train Epoch: 3 [2800/60000 (5%)] Loss: 0.25\n",
      "Train Epoch: 3 [3200/60000 (5%)] Loss: 0.24\n",
      "Train Epoch: 3 [3600/60000 (6%)] Loss: 0.28\n",
      "Train Epoch: 3 [4000/60000 (7%)] Loss: 0.43\n",
      "Train Epoch: 3 [4400/60000 (7%)] Loss: 0.28\n",
      "Train Epoch: 3 [4800/60000 (8%)] Loss: 0.34\n",
      "Train Epoch: 3 [5200/60000 (9%)] Loss: 0.21\n",
      "Train Epoch: 3 [5600/60000 (9%)] Loss: 0.25\n",
      "Train Epoch: 3 [6000/60000 (10%)] Loss: 0.22\n",
      "Train Epoch: 3 [6400/60000 (11%)] Loss: 0.37\n",
      "Train Epoch: 3 [6800/60000 (11%)] Loss: 0.32\n",
      "Train Epoch: 3 [7200/60000 (12%)] Loss: 0.31\n",
      "Train Epoch: 3 [7600/60000 (13%)] Loss: 0.29\n",
      "Train Epoch: 3 [8000/60000 (13%)] Loss: 0.44\n",
      "Train Epoch: 3 [8400/60000 (14%)] Loss: 0.46\n",
      "Train Epoch: 3 [8800/60000 (15%)] Loss: 0.42\n",
      "Train Epoch: 3 [9200/60000 (15%)] Loss: 0.39\n",
      "Train Epoch: 3 [9600/60000 (16%)] Loss: 0.34\n",
      "Train Epoch: 3 [10000/60000 (17%)] Loss: 0.30\n",
      "Train Epoch: 3 [10400/60000 (17%)] Loss: 0.22\n",
      "Train Epoch: 3 [10800/60000 (18%)] Loss: 0.26\n",
      "Train Epoch: 3 [11200/60000 (19%)] Loss: 0.18\n",
      "Train Epoch: 3 [11600/60000 (19%)] Loss: 0.25\n",
      "Train Epoch: 3 [12000/60000 (20%)] Loss: 0.41\n",
      "Train Epoch: 3 [12400/60000 (21%)] Loss: 0.31\n",
      "Train Epoch: 3 [12800/60000 (21%)] Loss: 0.33\n",
      "Train Epoch: 3 [13200/60000 (22%)] Loss: 0.25\n",
      "Train Epoch: 3 [13600/60000 (23%)] Loss: 0.26\n",
      "Train Epoch: 3 [14000/60000 (23%)] Loss: 0.21\n",
      "Train Epoch: 3 [14400/60000 (24%)] Loss: 0.25\n",
      "Train Epoch: 3 [14800/60000 (25%)] Loss: 0.37\n",
      "Train Epoch: 3 [15200/60000 (25%)] Loss: 0.24\n",
      "Train Epoch: 3 [15600/60000 (26%)] Loss: 0.28\n",
      "Train Epoch: 3 [16000/60000 (27%)] Loss: 0.30\n",
      "Train Epoch: 3 [16400/60000 (27%)] Loss: 0.34\n",
      "Train Epoch: 3 [16800/60000 (28%)] Loss: 0.21\n",
      "Train Epoch: 3 [17200/60000 (29%)] Loss: 0.24\n",
      "Train Epoch: 3 [17600/60000 (29%)] Loss: 0.21\n",
      "Train Epoch: 3 [18000/60000 (30%)] Loss: 0.34\n",
      "Train Epoch: 3 [18400/60000 (31%)] Loss: 0.35\n",
      "Train Epoch: 3 [18800/60000 (31%)] Loss: 0.31\n",
      "Train Epoch: 3 [19200/60000 (32%)] Loss: 0.28\n",
      "Train Epoch: 3 [19600/60000 (33%)] Loss: 0.36\n",
      "Train Epoch: 3 [20000/60000 (33%)] Loss: 0.31\n",
      "Train Epoch: 3 [20400/60000 (34%)] Loss: 0.31\n",
      "Train Epoch: 3 [20800/60000 (35%)] Loss: 0.22\n",
      "Train Epoch: 3 [21200/60000 (35%)] Loss: 0.28\n",
      "Train Epoch: 3 [21600/60000 (36%)] Loss: 0.24\n",
      "Train Epoch: 3 [22000/60000 (37%)] Loss: 0.35\n",
      "Train Epoch: 3 [22400/60000 (37%)] Loss: 0.23\n",
      "Train Epoch: 3 [22800/60000 (38%)] Loss: 0.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [23200/60000 (39%)] Loss: 0.36\n",
      "Train Epoch: 3 [23600/60000 (39%)] Loss: 0.29\n",
      "Train Epoch: 3 [24000/60000 (40%)] Loss: 0.32\n",
      "Train Epoch: 3 [24400/60000 (41%)] Loss: 0.24\n",
      "Train Epoch: 3 [24800/60000 (41%)] Loss: 0.19\n",
      "Train Epoch: 3 [25200/60000 (42%)] Loss: 0.25\n",
      "Train Epoch: 3 [25600/60000 (43%)] Loss: 0.23\n",
      "Train Epoch: 3 [26000/60000 (43%)] Loss: 0.43\n",
      "Train Epoch: 3 [26400/60000 (44%)] Loss: 0.48\n",
      "Train Epoch: 3 [26800/60000 (45%)] Loss: 0.18\n",
      "Train Epoch: 3 [27200/60000 (45%)] Loss: 0.31\n",
      "Train Epoch: 3 [27600/60000 (46%)] Loss: 0.35\n",
      "Train Epoch: 3 [28000/60000 (47%)] Loss: 0.23\n",
      "Train Epoch: 3 [28400/60000 (47%)] Loss: 0.24\n",
      "Train Epoch: 3 [28800/60000 (48%)] Loss: 0.34\n",
      "Train Epoch: 3 [29200/60000 (49%)] Loss: 0.32\n",
      "Train Epoch: 3 [29600/60000 (49%)] Loss: 0.40\n",
      "Train Epoch: 3 [30000/60000 (50%)] Loss: 0.26\n",
      "Train Epoch: 3 [30400/60000 (51%)] Loss: 0.25\n",
      "Train Epoch: 3 [30800/60000 (51%)] Loss: 0.30\n",
      "Train Epoch: 3 [31200/60000 (52%)] Loss: 0.20\n",
      "Train Epoch: 3 [31600/60000 (53%)] Loss: 0.29\n",
      "Train Epoch: 3 [32000/60000 (53%)] Loss: 0.21\n",
      "Train Epoch: 3 [32400/60000 (54%)] Loss: 0.25\n",
      "Train Epoch: 3 [32800/60000 (55%)] Loss: 0.44\n",
      "Train Epoch: 3 [33200/60000 (55%)] Loss: 0.28\n",
      "Train Epoch: 3 [33600/60000 (56%)] Loss: 0.34\n",
      "Train Epoch: 3 [34000/60000 (57%)] Loss: 0.26\n",
      "Train Epoch: 3 [34400/60000 (57%)] Loss: 0.29\n",
      "Train Epoch: 3 [34800/60000 (58%)] Loss: 0.40\n",
      "Train Epoch: 3 [35200/60000 (59%)] Loss: 0.36\n",
      "Train Epoch: 3 [35600/60000 (59%)] Loss: 0.29\n",
      "Train Epoch: 3 [36000/60000 (60%)] Loss: 0.48\n",
      "Train Epoch: 3 [36400/60000 (61%)] Loss: 0.28\n",
      "Train Epoch: 3 [36800/60000 (61%)] Loss: 0.35\n",
      "Train Epoch: 3 [37200/60000 (62%)] Loss: 0.36\n",
      "Train Epoch: 3 [37600/60000 (63%)] Loss: 0.33\n",
      "Train Epoch: 3 [38000/60000 (63%)] Loss: 0.17\n",
      "Train Epoch: 3 [38400/60000 (64%)] Loss: 0.26\n",
      "Train Epoch: 3 [38800/60000 (65%)] Loss: 0.21\n",
      "Train Epoch: 3 [39200/60000 (65%)] Loss: 0.29\n",
      "Train Epoch: 3 [39600/60000 (66%)] Loss: 0.30\n",
      "Train Epoch: 3 [40000/60000 (67%)] Loss: 0.18\n",
      "Train Epoch: 3 [40400/60000 (67%)] Loss: 0.27\n",
      "Train Epoch: 3 [40800/60000 (68%)] Loss: 0.35\n",
      "Train Epoch: 3 [41200/60000 (69%)] Loss: 0.32\n",
      "Train Epoch: 3 [41600/60000 (69%)] Loss: 0.32\n",
      "Train Epoch: 3 [42000/60000 (70%)] Loss: 0.37\n",
      "Train Epoch: 3 [42400/60000 (71%)] Loss: 0.31\n",
      "Train Epoch: 3 [42800/60000 (71%)] Loss: 0.31\n",
      "Train Epoch: 3 [43200/60000 (72%)] Loss: 0.30\n",
      "Train Epoch: 3 [43600/60000 (73%)] Loss: 0.29\n",
      "Train Epoch: 3 [44000/60000 (73%)] Loss: 0.24\n",
      "Train Epoch: 3 [44400/60000 (74%)] Loss: 0.26\n",
      "Train Epoch: 3 [44800/60000 (75%)] Loss: 0.29\n",
      "Train Epoch: 3 [45200/60000 (75%)] Loss: 0.29\n",
      "Train Epoch: 3 [45600/60000 (76%)] Loss: 0.21\n",
      "Train Epoch: 3 [46000/60000 (77%)] Loss: 0.33\n",
      "Train Epoch: 3 [46400/60000 (77%)] Loss: 0.36\n",
      "Train Epoch: 3 [46800/60000 (78%)] Loss: 0.27\n",
      "Train Epoch: 3 [47200/60000 (79%)] Loss: 0.32\n",
      "Train Epoch: 3 [47600/60000 (79%)] Loss: 0.30\n",
      "Train Epoch: 3 [48000/60000 (80%)] Loss: 0.22\n",
      "Train Epoch: 3 [48400/60000 (81%)] Loss: 0.24\n",
      "Train Epoch: 3 [48800/60000 (81%)] Loss: 0.37\n",
      "Train Epoch: 3 [49200/60000 (82%)] Loss: 0.39\n",
      "Train Epoch: 3 [49600/60000 (83%)] Loss: 0.39\n",
      "Train Epoch: 3 [50000/60000 (83%)] Loss: 0.27\n",
      "Train Epoch: 3 [50400/60000 (84%)] Loss: 0.35\n",
      "Train Epoch: 3 [50800/60000 (85%)] Loss: 0.29\n",
      "Train Epoch: 3 [51200/60000 (85%)] Loss: 0.31\n",
      "Train Epoch: 3 [51600/60000 (86%)] Loss: 0.36\n",
      "Train Epoch: 3 [52000/60000 (87%)] Loss: 0.32\n",
      "Train Epoch: 3 [52400/60000 (87%)] Loss: 0.26\n",
      "Train Epoch: 3 [52800/60000 (88%)] Loss: 0.42\n",
      "Train Epoch: 3 [53200/60000 (89%)] Loss: 0.21\n",
      "Train Epoch: 3 [53600/60000 (89%)] Loss: 0.16\n",
      "Train Epoch: 3 [54000/60000 (90%)] Loss: 0.25\n",
      "Train Epoch: 3 [54400/60000 (91%)] Loss: 0.63\n",
      "Train Epoch: 3 [54800/60000 (91%)] Loss: 0.19\n",
      "Train Epoch: 3 [55200/60000 (92%)] Loss: 0.21\n",
      "Train Epoch: 3 [55600/60000 (93%)] Loss: 0.35\n",
      "Train Epoch: 3 [56000/60000 (93%)] Loss: 0.26\n",
      "Train Epoch: 3 [56400/60000 (94%)] Loss: 0.18\n",
      "Train Epoch: 3 [56800/60000 (95%)] Loss: 0.38\n",
      "Train Epoch: 3 [57200/60000 (95%)] Loss: 0.20\n",
      "Train Epoch: 3 [57600/60000 (96%)] Loss: 0.16\n",
      "Train Epoch: 3 [58000/60000 (97%)] Loss: 0.33\n",
      "Train Epoch: 3 [58400/60000 (97%)] Loss: 0.22\n",
      "Train Epoch: 3 [58800/60000 (98%)] Loss: 0.22\n",
      "Train Epoch: 3 [59200/60000 (99%)] Loss: 0.23\n",
      "Train Epoch: 3 [59600/60000 (99%)] Loss: 0.37\n",
      "Train Epoch: 3 [60000/60000 (100%)] Loss: 0.22\n",
      "Test set: Accuracy: 9448.0/10000 (94%), Average Loss: 0.00\n",
      "Train Epoch: 4 [400/60000 (1%)] Loss: 0.33\n",
      "Train Epoch: 4 [800/60000 (1%)] Loss: 0.33\n",
      "Train Epoch: 4 [1200/60000 (2%)] Loss: 0.26\n",
      "Train Epoch: 4 [1600/60000 (3%)] Loss: 0.24\n",
      "Train Epoch: 4 [2000/60000 (3%)] Loss: 0.52\n",
      "Train Epoch: 4 [2400/60000 (4%)] Loss: 0.20\n",
      "Train Epoch: 4 [2800/60000 (5%)] Loss: 0.21\n",
      "Train Epoch: 4 [3200/60000 (5%)] Loss: 0.23\n",
      "Train Epoch: 4 [3600/60000 (6%)] Loss: 0.19\n",
      "Train Epoch: 4 [4000/60000 (7%)] Loss: 0.29\n",
      "Train Epoch: 4 [4400/60000 (7%)] Loss: 0.36\n",
      "Train Epoch: 4 [4800/60000 (8%)] Loss: 0.29\n",
      "Train Epoch: 4 [5200/60000 (9%)] Loss: 0.30\n",
      "Train Epoch: 4 [5600/60000 (9%)] Loss: 0.41\n",
      "Train Epoch: 4 [6000/60000 (10%)] Loss: 0.39\n",
      "Train Epoch: 4 [6400/60000 (11%)] Loss: 0.20\n",
      "Train Epoch: 4 [6800/60000 (11%)] Loss: 0.35\n",
      "Train Epoch: 4 [7200/60000 (12%)] Loss: 0.21\n",
      "Train Epoch: 4 [7600/60000 (13%)] Loss: 0.21\n",
      "Train Epoch: 4 [8000/60000 (13%)] Loss: 0.26\n",
      "Train Epoch: 4 [8400/60000 (14%)] Loss: 0.78\n",
      "Train Epoch: 4 [8800/60000 (15%)] Loss: 0.40\n",
      "Train Epoch: 4 [9200/60000 (15%)] Loss: 0.31\n",
      "Train Epoch: 4 [9600/60000 (16%)] Loss: 0.24\n",
      "Train Epoch: 4 [10000/60000 (17%)] Loss: 0.31\n",
      "Train Epoch: 4 [10400/60000 (17%)] Loss: 0.41\n",
      "Train Epoch: 4 [10800/60000 (18%)] Loss: 0.26\n",
      "Train Epoch: 4 [11200/60000 (19%)] Loss: 0.23\n",
      "Train Epoch: 4 [11600/60000 (19%)] Loss: 0.24\n",
      "Train Epoch: 4 [12000/60000 (20%)] Loss: 0.23\n",
      "Train Epoch: 4 [12400/60000 (21%)] Loss: 0.21\n",
      "Train Epoch: 4 [12800/60000 (21%)] Loss: 0.26\n",
      "Train Epoch: 4 [13200/60000 (22%)] Loss: 0.22\n",
      "Train Epoch: 4 [13600/60000 (23%)] Loss: 0.18\n",
      "Train Epoch: 4 [14000/60000 (23%)] Loss: 0.32\n",
      "Train Epoch: 4 [14400/60000 (24%)] Loss: 0.31\n",
      "Train Epoch: 4 [14800/60000 (25%)] Loss: 0.36\n",
      "Train Epoch: 4 [15200/60000 (25%)] Loss: 0.33\n",
      "Train Epoch: 4 [15600/60000 (26%)] Loss: 0.31\n",
      "Train Epoch: 4 [16000/60000 (27%)] Loss: 0.38\n",
      "Train Epoch: 4 [16400/60000 (27%)] Loss: 0.25\n",
      "Train Epoch: 4 [16800/60000 (28%)] Loss: 0.50\n",
      "Train Epoch: 4 [17200/60000 (29%)] Loss: 0.53\n",
      "Train Epoch: 4 [17600/60000 (29%)] Loss: 0.33\n",
      "Train Epoch: 4 [18000/60000 (30%)] Loss: 0.25\n",
      "Train Epoch: 4 [18400/60000 (31%)] Loss: 0.41\n",
      "Train Epoch: 4 [18800/60000 (31%)] Loss: 0.47\n",
      "Train Epoch: 4 [19200/60000 (32%)] Loss: 0.19\n",
      "Train Epoch: 4 [19600/60000 (33%)] Loss: 0.41\n",
      "Train Epoch: 4 [20000/60000 (33%)] Loss: 0.28\n",
      "Train Epoch: 4 [20400/60000 (34%)] Loss: 0.31\n",
      "Train Epoch: 4 [20800/60000 (35%)] Loss: 0.31\n",
      "Train Epoch: 4 [21200/60000 (35%)] Loss: 0.37\n",
      "Train Epoch: 4 [21600/60000 (36%)] Loss: 0.25\n",
      "Train Epoch: 4 [22000/60000 (37%)] Loss: 0.20\n",
      "Train Epoch: 4 [22400/60000 (37%)] Loss: 0.18\n",
      "Train Epoch: 4 [22800/60000 (38%)] Loss: 0.27\n",
      "Train Epoch: 4 [23200/60000 (39%)] Loss: 0.27\n",
      "Train Epoch: 4 [23600/60000 (39%)] Loss: 0.26\n",
      "Train Epoch: 4 [24000/60000 (40%)] Loss: 0.27\n",
      "Train Epoch: 4 [24400/60000 (41%)] Loss: 0.30\n",
      "Train Epoch: 4 [24800/60000 (41%)] Loss: 0.22\n",
      "Train Epoch: 4 [25200/60000 (42%)] Loss: 0.30\n",
      "Train Epoch: 4 [25600/60000 (43%)] Loss: 0.33\n",
      "Train Epoch: 4 [26000/60000 (43%)] Loss: 0.33\n",
      "Train Epoch: 4 [26400/60000 (44%)] Loss: 0.37\n",
      "Train Epoch: 4 [26800/60000 (45%)] Loss: 0.26\n",
      "Train Epoch: 4 [27200/60000 (45%)] Loss: 0.21\n",
      "Train Epoch: 4 [27600/60000 (46%)] Loss: 0.38\n",
      "Train Epoch: 4 [28000/60000 (47%)] Loss: 0.31\n",
      "Train Epoch: 4 [28400/60000 (47%)] Loss: 0.28\n",
      "Train Epoch: 4 [28800/60000 (48%)] Loss: 0.32\n",
      "Train Epoch: 4 [29200/60000 (49%)] Loss: 0.54\n",
      "Train Epoch: 4 [29600/60000 (49%)] Loss: 0.51\n",
      "Train Epoch: 4 [30000/60000 (50%)] Loss: 0.25\n",
      "Train Epoch: 4 [30400/60000 (51%)] Loss: 0.34\n",
      "Train Epoch: 4 [30800/60000 (51%)] Loss: 0.27\n",
      "Train Epoch: 4 [31200/60000 (52%)] Loss: 0.34\n",
      "Train Epoch: 4 [31600/60000 (53%)] Loss: 0.29\n",
      "Train Epoch: 4 [32000/60000 (53%)] Loss: 0.29\n",
      "Train Epoch: 4 [32400/60000 (54%)] Loss: 0.31\n",
      "Train Epoch: 4 [32800/60000 (55%)] Loss: 0.22\n",
      "Train Epoch: 4 [33200/60000 (55%)] Loss: 0.29\n",
      "Train Epoch: 4 [33600/60000 (56%)] Loss: 0.36\n",
      "Train Epoch: 4 [34000/60000 (57%)] Loss: 0.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [34400/60000 (57%)] Loss: 0.24\n",
      "Train Epoch: 4 [34800/60000 (58%)] Loss: 0.31\n",
      "Train Epoch: 4 [35200/60000 (59%)] Loss: 0.20\n",
      "Train Epoch: 4 [35600/60000 (59%)] Loss: 0.24\n",
      "Train Epoch: 4 [36000/60000 (60%)] Loss: 0.30\n",
      "Train Epoch: 4 [36400/60000 (61%)] Loss: 0.31\n",
      "Train Epoch: 4 [36800/60000 (61%)] Loss: 0.26\n",
      "Train Epoch: 4 [37200/60000 (62%)] Loss: 0.20\n",
      "Train Epoch: 4 [37600/60000 (63%)] Loss: 0.22\n",
      "Train Epoch: 4 [38000/60000 (63%)] Loss: 0.26\n",
      "Train Epoch: 4 [38400/60000 (64%)] Loss: 0.18\n",
      "Train Epoch: 4 [38800/60000 (65%)] Loss: 0.31\n",
      "Train Epoch: 4 [39200/60000 (65%)] Loss: 0.33\n",
      "Train Epoch: 4 [39600/60000 (66%)] Loss: 0.25\n",
      "Train Epoch: 4 [40000/60000 (67%)] Loss: 0.21\n",
      "Train Epoch: 4 [40400/60000 (67%)] Loss: 0.37\n",
      "Train Epoch: 4 [40800/60000 (68%)] Loss: 0.28\n",
      "Train Epoch: 4 [41200/60000 (69%)] Loss: 0.29\n",
      "Train Epoch: 4 [41600/60000 (69%)] Loss: 0.32\n",
      "Train Epoch: 4 [42000/60000 (70%)] Loss: 0.32\n",
      "Train Epoch: 4 [42400/60000 (71%)] Loss: 0.29\n",
      "Train Epoch: 4 [42800/60000 (71%)] Loss: 0.27\n",
      "Train Epoch: 4 [43200/60000 (72%)] Loss: 0.22\n",
      "Train Epoch: 4 [43600/60000 (73%)] Loss: 0.22\n",
      "Train Epoch: 4 [44000/60000 (73%)] Loss: 0.39\n",
      "Train Epoch: 4 [44400/60000 (74%)] Loss: 0.29\n",
      "Train Epoch: 4 [44800/60000 (75%)] Loss: 0.21\n",
      "Train Epoch: 4 [45200/60000 (75%)] Loss: 0.23\n",
      "Train Epoch: 4 [45600/60000 (76%)] Loss: 0.23\n",
      "Train Epoch: 4 [46000/60000 (77%)] Loss: 0.72\n",
      "Train Epoch: 4 [46400/60000 (77%)] Loss: 0.77\n",
      "Train Epoch: 4 [46800/60000 (78%)] Loss: 0.50\n",
      "Train Epoch: 4 [47200/60000 (79%)] Loss: 0.23\n",
      "Train Epoch: 4 [47600/60000 (79%)] Loss: 0.31\n",
      "Train Epoch: 4 [48000/60000 (80%)] Loss: 0.37\n",
      "Train Epoch: 4 [48400/60000 (81%)] Loss: 0.32\n",
      "Train Epoch: 4 [48800/60000 (81%)] Loss: 0.22\n",
      "Train Epoch: 4 [49200/60000 (82%)] Loss: 0.24\n",
      "Train Epoch: 4 [49600/60000 (83%)] Loss: 0.23\n",
      "Train Epoch: 4 [50000/60000 (83%)] Loss: 0.19\n",
      "Train Epoch: 4 [50400/60000 (84%)] Loss: 0.22\n",
      "Train Epoch: 4 [50800/60000 (85%)] Loss: 0.42\n",
      "Train Epoch: 4 [51200/60000 (85%)] Loss: 0.19\n",
      "Train Epoch: 4 [51600/60000 (86%)] Loss: 0.28\n",
      "Train Epoch: 4 [52000/60000 (87%)] Loss: 0.17\n",
      "Train Epoch: 4 [52400/60000 (87%)] Loss: 0.24\n",
      "Train Epoch: 4 [52800/60000 (88%)] Loss: 0.38\n",
      "Train Epoch: 4 [53200/60000 (89%)] Loss: 0.22\n",
      "Train Epoch: 4 [53600/60000 (89%)] Loss: 0.15\n",
      "Train Epoch: 4 [54000/60000 (90%)] Loss: 0.27\n",
      "Train Epoch: 4 [54400/60000 (91%)] Loss: 0.45\n",
      "Train Epoch: 4 [54800/60000 (91%)] Loss: 0.33\n",
      "Train Epoch: 4 [55200/60000 (92%)] Loss: 0.37\n",
      "Train Epoch: 4 [55600/60000 (93%)] Loss: 0.23\n",
      "Train Epoch: 4 [56000/60000 (93%)] Loss: 0.26\n",
      "Train Epoch: 4 [56400/60000 (94%)] Loss: 0.34\n",
      "Train Epoch: 4 [56800/60000 (95%)] Loss: 0.20\n",
      "Train Epoch: 4 [57200/60000 (95%)] Loss: 0.16\n",
      "Train Epoch: 4 [57600/60000 (96%)] Loss: 0.37\n",
      "Train Epoch: 4 [58000/60000 (97%)] Loss: 0.37\n",
      "Train Epoch: 4 [58400/60000 (97%)] Loss: 0.34\n",
      "Train Epoch: 4 [58800/60000 (98%)] Loss: 0.29\n",
      "Train Epoch: 4 [59200/60000 (99%)] Loss: 0.43\n",
      "Train Epoch: 4 [59600/60000 (99%)] Loss: 0.29\n",
      "Train Epoch: 4 [60000/60000 (100%)] Loss: 0.36\n",
      "Test set: Accuracy: 9112.0/10000 (91%), Average Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "model = myCNN(2)\n",
    "trainAccuracy, testAccuracy = traintest_function() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set figure size   \n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "row = len(2)\n",
    "column = 1\n",
    "subcnt = 4\n",
    "for i in range(1,2):\n",
    "    plt.subplot(row, column, i)\n",
    "    plt.title(\"Kernal Size\"+str(kernelSize))\n",
    "    plt.plot(trainAccuracy,'b--',label='train')\n",
    "    plt.plot(testAccuracy,'g--',label='test')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [400/60000 (1%)] Loss: 1.96\n",
      "Train Epoch: 1 [800/60000 (1%)] Loss: 1.06\n",
      "Train Epoch: 1 [1200/60000 (2%)] Loss: 0.75\n",
      "Train Epoch: 1 [1600/60000 (3%)] Loss: 0.62\n",
      "Train Epoch: 1 [2000/60000 (3%)] Loss: 0.63\n",
      "Train Epoch: 1 [2400/60000 (4%)] Loss: 0.71\n",
      "Train Epoch: 1 [2800/60000 (5%)] Loss: 0.44\n",
      "Train Epoch: 1 [3200/60000 (5%)] Loss: 0.43\n",
      "Train Epoch: 1 [3600/60000 (6%)] Loss: 0.45\n",
      "Train Epoch: 1 [4000/60000 (7%)] Loss: 0.40\n",
      "Train Epoch: 1 [4400/60000 (7%)] Loss: 0.55\n",
      "Train Epoch: 1 [4800/60000 (8%)] Loss: 0.41\n",
      "Train Epoch: 1 [5200/60000 (9%)] Loss: 0.44\n",
      "Train Epoch: 1 [5600/60000 (9%)] Loss: 0.46\n",
      "Train Epoch: 1 [6000/60000 (10%)] Loss: 0.31\n",
      "Train Epoch: 1 [6400/60000 (11%)] Loss: 0.29\n",
      "Train Epoch: 1 [6800/60000 (11%)] Loss: 0.43\n",
      "Train Epoch: 1 [7200/60000 (12%)] Loss: 0.30\n",
      "Train Epoch: 1 [7600/60000 (13%)] Loss: 0.35\n",
      "Train Epoch: 1 [8000/60000 (13%)] Loss: 0.32\n",
      "Train Epoch: 1 [8400/60000 (14%)] Loss: 0.46\n",
      "Train Epoch: 1 [8800/60000 (15%)] Loss: 0.29\n",
      "Train Epoch: 1 [9200/60000 (15%)] Loss: 0.35\n",
      "Train Epoch: 1 [9600/60000 (16%)] Loss: 0.26\n",
      "Train Epoch: 1 [10000/60000 (17%)] Loss: 0.31\n",
      "Train Epoch: 1 [10400/60000 (17%)] Loss: 0.29\n",
      "Train Epoch: 1 [10800/60000 (18%)] Loss: 0.34\n",
      "Train Epoch: 1 [11200/60000 (19%)] Loss: 0.32\n",
      "Train Epoch: 1 [11600/60000 (19%)] Loss: 0.36\n",
      "Train Epoch: 1 [12000/60000 (20%)] Loss: 0.41\n",
      "Train Epoch: 1 [12400/60000 (21%)] Loss: 0.35\n",
      "Train Epoch: 1 [12800/60000 (21%)] Loss: 0.41\n",
      "Train Epoch: 1 [13200/60000 (22%)] Loss: 0.32\n",
      "Train Epoch: 1 [13600/60000 (23%)] Loss: 0.36\n",
      "Train Epoch: 1 [14000/60000 (23%)] Loss: 0.29\n",
      "Train Epoch: 1 [14400/60000 (24%)] Loss: 0.26\n",
      "Train Epoch: 1 [14800/60000 (25%)] Loss: 0.27\n",
      "Train Epoch: 1 [15200/60000 (25%)] Loss: 0.21\n",
      "Train Epoch: 1 [15600/60000 (26%)] Loss: 0.29\n",
      "Train Epoch: 1 [16000/60000 (27%)] Loss: 0.26\n",
      "Train Epoch: 1 [16400/60000 (27%)] Loss: 0.40\n",
      "Train Epoch: 1 [16800/60000 (28%)] Loss: 0.28\n",
      "Train Epoch: 1 [17200/60000 (29%)] Loss: 0.33\n",
      "Train Epoch: 1 [17600/60000 (29%)] Loss: 0.30\n",
      "Train Epoch: 1 [18000/60000 (30%)] Loss: 0.34\n",
      "Train Epoch: 1 [18400/60000 (31%)] Loss: 0.28\n",
      "Train Epoch: 1 [18800/60000 (31%)] Loss: 0.34\n",
      "Train Epoch: 1 [19200/60000 (32%)] Loss: 0.37\n",
      "Train Epoch: 1 [19600/60000 (33%)] Loss: 0.21\n",
      "Train Epoch: 1 [20000/60000 (33%)] Loss: 0.33\n",
      "Train Epoch: 1 [20400/60000 (34%)] Loss: 0.28\n",
      "Train Epoch: 1 [20800/60000 (35%)] Loss: 0.29\n",
      "Train Epoch: 1 [21200/60000 (35%)] Loss: 0.36\n",
      "Train Epoch: 1 [21600/60000 (36%)] Loss: 0.34\n",
      "Train Epoch: 1 [22000/60000 (37%)] Loss: 0.32\n",
      "Train Epoch: 1 [22400/60000 (37%)] Loss: 0.31\n",
      "Train Epoch: 1 [22800/60000 (38%)] Loss: 0.40\n",
      "Train Epoch: 1 [23200/60000 (39%)] Loss: 0.45\n",
      "Train Epoch: 1 [23600/60000 (39%)] Loss: 0.36\n",
      "Train Epoch: 1 [24000/60000 (40%)] Loss: 0.49\n",
      "Train Epoch: 1 [24400/60000 (41%)] Loss: 0.44\n",
      "Train Epoch: 1 [24800/60000 (41%)] Loss: 0.20\n",
      "Train Epoch: 1 [25200/60000 (42%)] Loss: 0.30\n",
      "Train Epoch: 1 [25600/60000 (43%)] Loss: 0.28\n",
      "Train Epoch: 1 [26000/60000 (43%)] Loss: 0.45\n",
      "Train Epoch: 1 [26400/60000 (44%)] Loss: 0.45\n",
      "Train Epoch: 1 [26800/60000 (45%)] Loss: 0.29\n",
      "Train Epoch: 1 [27200/60000 (45%)] Loss: 0.39\n",
      "Train Epoch: 1 [27600/60000 (46%)] Loss: 0.33\n",
      "Train Epoch: 1 [28000/60000 (47%)] Loss: 0.31\n",
      "Train Epoch: 1 [28400/60000 (47%)] Loss: 0.34\n",
      "Train Epoch: 1 [28800/60000 (48%)] Loss: 0.25\n",
      "Train Epoch: 1 [29200/60000 (49%)] Loss: 0.32\n",
      "Train Epoch: 1 [29600/60000 (49%)] Loss: 0.26\n",
      "Train Epoch: 1 [30000/60000 (50%)] Loss: 0.32\n",
      "Train Epoch: 1 [30400/60000 (51%)] Loss: 0.30\n",
      "Train Epoch: 1 [30800/60000 (51%)] Loss: 0.25\n",
      "Train Epoch: 1 [31200/60000 (52%)] Loss: 0.34\n",
      "Train Epoch: 1 [31600/60000 (53%)] Loss: 0.24\n",
      "Train Epoch: 1 [32000/60000 (53%)] Loss: 0.22\n",
      "Train Epoch: 1 [32400/60000 (54%)] Loss: 0.26\n",
      "Train Epoch: 1 [32800/60000 (55%)] Loss: 0.30\n",
      "Train Epoch: 1 [33200/60000 (55%)] Loss: 0.30\n",
      "Train Epoch: 1 [33600/60000 (56%)] Loss: 0.30\n",
      "Train Epoch: 1 [34000/60000 (57%)] Loss: 0.17\n",
      "Train Epoch: 1 [34400/60000 (57%)] Loss: 0.27\n",
      "Train Epoch: 1 [34800/60000 (58%)] Loss: 0.28\n",
      "Train Epoch: 1 [35200/60000 (59%)] Loss: 0.39\n",
      "Train Epoch: 1 [35600/60000 (59%)] Loss: 0.29\n",
      "Train Epoch: 1 [36000/60000 (60%)] Loss: 0.39\n",
      "Train Epoch: 1 [36400/60000 (61%)] Loss: 0.31\n",
      "Train Epoch: 1 [36800/60000 (61%)] Loss: 0.45\n",
      "Train Epoch: 1 [37200/60000 (62%)] Loss: 0.25\n",
      "Train Epoch: 1 [37600/60000 (63%)] Loss: 0.22\n",
      "Train Epoch: 1 [38000/60000 (63%)] Loss: 0.22\n",
      "Train Epoch: 1 [38400/60000 (64%)] Loss: 0.36\n",
      "Train Epoch: 1 [38800/60000 (65%)] Loss: 0.26\n",
      "Train Epoch: 1 [39200/60000 (65%)] Loss: 0.31\n",
      "Train Epoch: 1 [39600/60000 (66%)] Loss: 0.33\n",
      "Train Epoch: 1 [40000/60000 (67%)] Loss: 0.53\n",
      "Train Epoch: 1 [40400/60000 (67%)] Loss: 0.35\n",
      "Train Epoch: 1 [40800/60000 (68%)] Loss: 0.34\n",
      "Train Epoch: 1 [41200/60000 (69%)] Loss: 0.40\n",
      "Train Epoch: 1 [41600/60000 (69%)] Loss: 0.21\n",
      "Train Epoch: 1 [42000/60000 (70%)] Loss: 0.30\n",
      "Train Epoch: 1 [42400/60000 (71%)] Loss: 0.23\n",
      "Train Epoch: 1 [42800/60000 (71%)] Loss: 0.25\n",
      "Train Epoch: 1 [43200/60000 (72%)] Loss: 0.21\n",
      "Train Epoch: 1 [43600/60000 (73%)] Loss: 0.34\n",
      "Train Epoch: 1 [44000/60000 (73%)] Loss: 0.24\n",
      "Train Epoch: 1 [44400/60000 (74%)] Loss: 0.34\n",
      "Train Epoch: 1 [44800/60000 (75%)] Loss: 0.31\n",
      "Train Epoch: 1 [45200/60000 (75%)] Loss: 0.29\n",
      "Train Epoch: 1 [45600/60000 (76%)] Loss: 0.46\n",
      "Train Epoch: 1 [46000/60000 (77%)] Loss: 0.28\n",
      "Train Epoch: 1 [46400/60000 (77%)] Loss: 0.27\n",
      "Train Epoch: 1 [46800/60000 (78%)] Loss: 0.28\n",
      "Train Epoch: 1 [47200/60000 (79%)] Loss: 0.22\n",
      "Train Epoch: 1 [47600/60000 (79%)] Loss: 0.29\n",
      "Train Epoch: 1 [48000/60000 (80%)] Loss: 0.34\n",
      "Train Epoch: 1 [48400/60000 (81%)] Loss: 0.29\n",
      "Train Epoch: 1 [48800/60000 (81%)] Loss: 0.37\n",
      "Train Epoch: 1 [49200/60000 (82%)] Loss: 0.25\n",
      "Train Epoch: 1 [49600/60000 (83%)] Loss: 0.27\n",
      "Train Epoch: 1 [50000/60000 (83%)] Loss: 0.30\n",
      "Train Epoch: 1 [50400/60000 (84%)] Loss: 0.35\n",
      "Train Epoch: 1 [50800/60000 (85%)] Loss: 0.36\n",
      "Train Epoch: 1 [51200/60000 (85%)] Loss: 0.36\n",
      "Train Epoch: 1 [51600/60000 (86%)] Loss: 0.31\n",
      "Train Epoch: 1 [52000/60000 (87%)] Loss: 0.27\n",
      "Train Epoch: 1 [52400/60000 (87%)] Loss: 0.38\n",
      "Train Epoch: 1 [52800/60000 (88%)] Loss: 0.39\n",
      "Train Epoch: 1 [53200/60000 (89%)] Loss: 0.30\n",
      "Train Epoch: 1 [53600/60000 (89%)] Loss: 0.30\n",
      "Train Epoch: 1 [54000/60000 (90%)] Loss: 0.27\n",
      "Train Epoch: 1 [54400/60000 (91%)] Loss: 0.19\n",
      "Train Epoch: 1 [54800/60000 (91%)] Loss: 0.26\n",
      "Train Epoch: 1 [55200/60000 (92%)] Loss: 0.24\n",
      "Train Epoch: 1 [55600/60000 (93%)] Loss: 0.50\n",
      "Train Epoch: 1 [56000/60000 (93%)] Loss: 0.22\n",
      "Train Epoch: 1 [56400/60000 (94%)] Loss: 0.47\n",
      "Train Epoch: 1 [56800/60000 (95%)] Loss: 0.25\n",
      "Train Epoch: 1 [57200/60000 (95%)] Loss: 0.35\n",
      "Train Epoch: 1 [57600/60000 (96%)] Loss: 0.33\n",
      "Train Epoch: 1 [58000/60000 (97%)] Loss: 0.27\n",
      "Train Epoch: 1 [58400/60000 (97%)] Loss: 0.22\n",
      "Train Epoch: 1 [58800/60000 (98%)] Loss: 0.32\n",
      "Train Epoch: 1 [59200/60000 (99%)] Loss: 0.25\n",
      "Train Epoch: 1 [59600/60000 (99%)] Loss: 0.27\n",
      "Train Epoch: 1 [60000/60000 (100%)] Loss: 0.34\n",
      "Test set: Accuracy: 9204.0/10000 (92%), Average Loss: 0.00\n",
      "Train Epoch: 2 [400/60000 (1%)] Loss: 0.29\n",
      "Train Epoch: 2 [800/60000 (1%)] Loss: 0.44\n",
      "Train Epoch: 2 [1200/60000 (2%)] Loss: 0.16\n",
      "Train Epoch: 2 [1600/60000 (3%)] Loss: 0.30\n",
      "Train Epoch: 2 [2000/60000 (3%)] Loss: 0.60\n",
      "Train Epoch: 2 [2400/60000 (4%)] Loss: 0.32\n",
      "Train Epoch: 2 [2800/60000 (5%)] Loss: 0.18\n",
      "Train Epoch: 2 [3200/60000 (5%)] Loss: 0.25\n",
      "Train Epoch: 2 [3600/60000 (6%)] Loss: 0.32\n",
      "Train Epoch: 2 [4000/60000 (7%)] Loss: 0.22\n",
      "Train Epoch: 2 [4400/60000 (7%)] Loss: 0.28\n",
      "Train Epoch: 2 [4800/60000 (8%)] Loss: 0.37\n",
      "Train Epoch: 2 [5200/60000 (9%)] Loss: 0.18\n",
      "Train Epoch: 2 [5600/60000 (9%)] Loss: 0.26\n",
      "Train Epoch: 2 [6000/60000 (10%)] Loss: 0.21\n",
      "Train Epoch: 2 [6400/60000 (11%)] Loss: 0.23\n",
      "Train Epoch: 2 [6800/60000 (11%)] Loss: 0.19\n",
      "Train Epoch: 2 [7200/60000 (12%)] Loss: 0.27\n",
      "Train Epoch: 2 [7600/60000 (13%)] Loss: 0.23\n",
      "Train Epoch: 2 [8000/60000 (13%)] Loss: 0.39\n",
      "Train Epoch: 2 [8400/60000 (14%)] Loss: 0.26\n",
      "Train Epoch: 2 [8800/60000 (15%)] Loss: 0.44\n",
      "Train Epoch: 2 [9200/60000 (15%)] Loss: 0.33\n",
      "Train Epoch: 2 [9600/60000 (16%)] Loss: 0.33\n",
      "Train Epoch: 2 [10000/60000 (17%)] Loss: 0.24\n",
      "Train Epoch: 2 [10400/60000 (17%)] Loss: 0.31\n",
      "Train Epoch: 2 [10800/60000 (18%)] Loss: 0.24\n",
      "Train Epoch: 2 [11200/60000 (19%)] Loss: 0.25\n",
      "Train Epoch: 2 [11600/60000 (19%)] Loss: 0.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [12000/60000 (20%)] Loss: 0.29\n",
      "Train Epoch: 2 [12400/60000 (21%)] Loss: 0.38\n",
      "Train Epoch: 2 [12800/60000 (21%)] Loss: 0.39\n",
      "Train Epoch: 2 [13200/60000 (22%)] Loss: 0.36\n",
      "Train Epoch: 2 [13600/60000 (23%)] Loss: 0.44\n",
      "Train Epoch: 2 [14000/60000 (23%)] Loss: 0.45\n",
      "Train Epoch: 2 [14400/60000 (24%)] Loss: 0.50\n",
      "Train Epoch: 2 [14800/60000 (25%)] Loss: 0.37\n",
      "Train Epoch: 2 [15200/60000 (25%)] Loss: 0.34\n",
      "Train Epoch: 2 [15600/60000 (26%)] Loss: 0.22\n",
      "Train Epoch: 2 [16000/60000 (27%)] Loss: 0.21\n",
      "Train Epoch: 2 [16400/60000 (27%)] Loss: 0.35\n",
      "Train Epoch: 2 [16800/60000 (28%)] Loss: 0.53\n",
      "Train Epoch: 2 [17200/60000 (29%)] Loss: 0.29\n",
      "Train Epoch: 2 [17600/60000 (29%)] Loss: 0.26\n",
      "Train Epoch: 2 [18000/60000 (30%)] Loss: 0.41\n",
      "Train Epoch: 2 [18400/60000 (31%)] Loss: 0.33\n",
      "Train Epoch: 2 [18800/60000 (31%)] Loss: 0.43\n",
      "Train Epoch: 2 [19200/60000 (32%)] Loss: 0.22\n",
      "Train Epoch: 2 [19600/60000 (33%)] Loss: 0.26\n",
      "Train Epoch: 2 [20000/60000 (33%)] Loss: 0.26\n",
      "Train Epoch: 2 [20400/60000 (34%)] Loss: 0.35\n",
      "Train Epoch: 2 [20800/60000 (35%)] Loss: 0.28\n",
      "Train Epoch: 2 [21200/60000 (35%)] Loss: 0.27\n",
      "Train Epoch: 2 [21600/60000 (36%)] Loss: 0.25\n",
      "Train Epoch: 2 [22000/60000 (37%)] Loss: 0.28\n",
      "Train Epoch: 2 [22400/60000 (37%)] Loss: 0.18\n",
      "Train Epoch: 2 [22800/60000 (38%)] Loss: 0.36\n",
      "Train Epoch: 2 [23200/60000 (39%)] Loss: 0.30\n",
      "Train Epoch: 2 [23600/60000 (39%)] Loss: 0.41\n",
      "Train Epoch: 2 [24000/60000 (40%)] Loss: 0.29\n",
      "Train Epoch: 2 [24400/60000 (41%)] Loss: 0.24\n",
      "Train Epoch: 2 [24800/60000 (41%)] Loss: 0.30\n",
      "Train Epoch: 2 [25200/60000 (42%)] Loss: 0.31\n",
      "Train Epoch: 2 [25600/60000 (43%)] Loss: 0.28\n",
      "Train Epoch: 2 [26000/60000 (43%)] Loss: 0.26\n",
      "Train Epoch: 2 [26400/60000 (44%)] Loss: 0.38\n",
      "Train Epoch: 2 [26800/60000 (45%)] Loss: 0.34\n",
      "Train Epoch: 2 [27200/60000 (45%)] Loss: 0.29\n",
      "Train Epoch: 2 [27600/60000 (46%)] Loss: 0.25\n",
      "Train Epoch: 2 [28000/60000 (47%)] Loss: 0.42\n",
      "Train Epoch: 2 [28400/60000 (47%)] Loss: 0.31\n",
      "Train Epoch: 2 [28800/60000 (48%)] Loss: 0.22\n",
      "Train Epoch: 2 [29200/60000 (49%)] Loss: 0.40\n",
      "Train Epoch: 2 [29600/60000 (49%)] Loss: 0.40\n",
      "Train Epoch: 2 [30000/60000 (50%)] Loss: 0.30\n",
      "Train Epoch: 2 [30400/60000 (51%)] Loss: 0.34\n",
      "Train Epoch: 2 [30800/60000 (51%)] Loss: 0.23\n",
      "Train Epoch: 2 [31200/60000 (52%)] Loss: 0.32\n",
      "Train Epoch: 2 [31600/60000 (53%)] Loss: 0.21\n",
      "Train Epoch: 2 [32000/60000 (53%)] Loss: 0.36\n",
      "Train Epoch: 2 [32400/60000 (54%)] Loss: 0.26\n",
      "Train Epoch: 2 [32800/60000 (55%)] Loss: 0.29\n",
      "Train Epoch: 2 [33200/60000 (55%)] Loss: 0.40\n",
      "Train Epoch: 2 [33600/60000 (56%)] Loss: 0.30\n",
      "Train Epoch: 2 [34000/60000 (57%)] Loss: 0.49\n",
      "Train Epoch: 2 [34400/60000 (57%)] Loss: 0.41\n",
      "Train Epoch: 2 [34800/60000 (58%)] Loss: 0.24\n",
      "Train Epoch: 2 [35200/60000 (59%)] Loss: 0.21\n",
      "Train Epoch: 2 [35600/60000 (59%)] Loss: 0.31\n",
      "Train Epoch: 2 [36000/60000 (60%)] Loss: 0.18\n",
      "Train Epoch: 2 [36400/60000 (61%)] Loss: 0.22\n",
      "Train Epoch: 2 [36800/60000 (61%)] Loss: 0.19\n",
      "Train Epoch: 2 [37200/60000 (62%)] Loss: 0.45\n",
      "Train Epoch: 2 [37600/60000 (63%)] Loss: 0.33\n",
      "Train Epoch: 2 [38000/60000 (63%)] Loss: 0.41\n",
      "Train Epoch: 2 [38400/60000 (64%)] Loss: 0.32\n",
      "Train Epoch: 2 [38800/60000 (65%)] Loss: 0.36\n",
      "Train Epoch: 2 [39200/60000 (65%)] Loss: 0.29\n",
      "Train Epoch: 2 [39600/60000 (66%)] Loss: 0.27\n",
      "Train Epoch: 2 [40000/60000 (67%)] Loss: 0.33\n",
      "Train Epoch: 2 [40400/60000 (67%)] Loss: 0.20\n",
      "Train Epoch: 2 [40800/60000 (68%)] Loss: 0.34\n",
      "Train Epoch: 2 [41200/60000 (69%)] Loss: 0.35\n",
      "Train Epoch: 2 [41600/60000 (69%)] Loss: 0.20\n",
      "Train Epoch: 2 [42000/60000 (70%)] Loss: 0.14\n",
      "Train Epoch: 2 [42400/60000 (71%)] Loss: 0.22\n",
      "Train Epoch: 2 [42800/60000 (71%)] Loss: 0.44\n",
      "Train Epoch: 2 [43200/60000 (72%)] Loss: 0.30\n",
      "Train Epoch: 2 [43600/60000 (73%)] Loss: 0.20\n",
      "Train Epoch: 2 [44000/60000 (73%)] Loss: 0.60\n",
      "Train Epoch: 2 [44400/60000 (74%)] Loss: 0.42\n",
      "Train Epoch: 2 [44800/60000 (75%)] Loss: 0.42\n",
      "Train Epoch: 2 [45200/60000 (75%)] Loss: 0.39\n",
      "Train Epoch: 2 [45600/60000 (76%)] Loss: 0.27\n",
      "Train Epoch: 2 [46000/60000 (77%)] Loss: 0.21\n",
      "Train Epoch: 2 [46400/60000 (77%)] Loss: 0.18\n",
      "Train Epoch: 2 [46800/60000 (78%)] Loss: 0.22\n",
      "Train Epoch: 2 [47200/60000 (79%)] Loss: 0.39\n",
      "Train Epoch: 2 [47600/60000 (79%)] Loss: 0.22\n",
      "Train Epoch: 2 [48000/60000 (80%)] Loss: 0.22\n",
      "Train Epoch: 2 [48400/60000 (81%)] Loss: 0.19\n",
      "Train Epoch: 2 [48800/60000 (81%)] Loss: 0.35\n",
      "Train Epoch: 2 [49200/60000 (82%)] Loss: 0.32\n",
      "Train Epoch: 2 [49600/60000 (83%)] Loss: 0.19\n",
      "Train Epoch: 2 [50000/60000 (83%)] Loss: 0.48\n",
      "Train Epoch: 2 [50400/60000 (84%)] Loss: 0.29\n",
      "Train Epoch: 2 [50800/60000 (85%)] Loss: 0.41\n",
      "Train Epoch: 2 [51200/60000 (85%)] Loss: 0.26\n",
      "Train Epoch: 2 [51600/60000 (86%)] Loss: 0.29\n",
      "Train Epoch: 2 [52000/60000 (87%)] Loss: 0.23\n",
      "Train Epoch: 2 [52400/60000 (87%)] Loss: 0.17\n",
      "Train Epoch: 2 [52800/60000 (88%)] Loss: 0.25\n",
      "Train Epoch: 2 [53200/60000 (89%)] Loss: 0.28\n",
      "Train Epoch: 2 [53600/60000 (89%)] Loss: 0.21\n",
      "Train Epoch: 2 [54000/60000 (90%)] Loss: 0.29\n",
      "Train Epoch: 2 [54400/60000 (91%)] Loss: 0.22\n",
      "Train Epoch: 2 [54800/60000 (91%)] Loss: 0.25\n",
      "Train Epoch: 2 [55200/60000 (92%)] Loss: 0.17\n",
      "Train Epoch: 2 [55600/60000 (93%)] Loss: 0.24\n",
      "Train Epoch: 2 [56000/60000 (93%)] Loss: 0.21\n",
      "Train Epoch: 2 [56400/60000 (94%)] Loss: 0.22\n",
      "Train Epoch: 2 [56800/60000 (95%)] Loss: 0.19\n",
      "Train Epoch: 2 [57200/60000 (95%)] Loss: 0.25\n",
      "Train Epoch: 2 [57600/60000 (96%)] Loss: 0.31\n",
      "Train Epoch: 2 [58000/60000 (97%)] Loss: 0.25\n",
      "Train Epoch: 2 [58400/60000 (97%)] Loss: 0.23\n",
      "Train Epoch: 2 [58800/60000 (98%)] Loss: 0.38\n",
      "Train Epoch: 2 [59200/60000 (99%)] Loss: 0.43\n",
      "Train Epoch: 2 [59600/60000 (99%)] Loss: 0.27\n",
      "Train Epoch: 2 [60000/60000 (100%)] Loss: 0.31\n",
      "Test set: Accuracy: 9473.0/10000 (95%), Average Loss: 0.00\n",
      "Train Epoch: 3 [400/60000 (1%)] Loss: 0.26\n",
      "Train Epoch: 3 [800/60000 (1%)] Loss: 0.34\n",
      "Train Epoch: 3 [1200/60000 (2%)] Loss: 0.38\n",
      "Train Epoch: 3 [1600/60000 (3%)] Loss: 0.21\n",
      "Train Epoch: 3 [2000/60000 (3%)] Loss: 0.22\n",
      "Train Epoch: 3 [2400/60000 (4%)] Loss: 0.19\n",
      "Train Epoch: 3 [2800/60000 (5%)] Loss: 0.29\n",
      "Train Epoch: 3 [3200/60000 (5%)] Loss: 0.28\n",
      "Train Epoch: 3 [3600/60000 (6%)] Loss: 0.26\n",
      "Train Epoch: 3 [4000/60000 (7%)] Loss: 0.24\n",
      "Train Epoch: 3 [4400/60000 (7%)] Loss: 0.29\n",
      "Train Epoch: 3 [4800/60000 (8%)] Loss: 0.36\n",
      "Train Epoch: 3 [5200/60000 (9%)] Loss: 0.20\n",
      "Train Epoch: 3 [5600/60000 (9%)] Loss: 0.28\n",
      "Train Epoch: 3 [6000/60000 (10%)] Loss: 0.24\n",
      "Train Epoch: 3 [6400/60000 (11%)] Loss: 0.26\n",
      "Train Epoch: 3 [6800/60000 (11%)] Loss: 0.27\n",
      "Train Epoch: 3 [7200/60000 (12%)] Loss: 0.20\n",
      "Train Epoch: 3 [7600/60000 (13%)] Loss: 0.23\n",
      "Train Epoch: 3 [8000/60000 (13%)] Loss: 0.20\n",
      "Train Epoch: 3 [8400/60000 (14%)] Loss: 0.19\n",
      "Train Epoch: 3 [8800/60000 (15%)] Loss: 0.24\n",
      "Train Epoch: 3 [9200/60000 (15%)] Loss: 0.25\n",
      "Train Epoch: 3 [9600/60000 (16%)] Loss: 0.52\n",
      "Train Epoch: 3 [10000/60000 (17%)] Loss: 0.29\n",
      "Train Epoch: 3 [10400/60000 (17%)] Loss: 0.34\n",
      "Train Epoch: 3 [10800/60000 (18%)] Loss: 0.28\n",
      "Train Epoch: 3 [11200/60000 (19%)] Loss: 0.32\n",
      "Train Epoch: 3 [11600/60000 (19%)] Loss: 0.21\n",
      "Train Epoch: 3 [12000/60000 (20%)] Loss: 0.21\n",
      "Train Epoch: 3 [12400/60000 (21%)] Loss: 0.52\n",
      "Train Epoch: 3 [12800/60000 (21%)] Loss: 0.33\n",
      "Train Epoch: 3 [13200/60000 (22%)] Loss: 0.37\n",
      "Train Epoch: 3 [13600/60000 (23%)] Loss: 0.25\n",
      "Train Epoch: 3 [14000/60000 (23%)] Loss: 0.37\n",
      "Train Epoch: 3 [14400/60000 (24%)] Loss: 0.38\n",
      "Train Epoch: 3 [14800/60000 (25%)] Loss: 0.39\n",
      "Train Epoch: 3 [15200/60000 (25%)] Loss: 0.27\n",
      "Train Epoch: 3 [15600/60000 (26%)] Loss: 0.42\n",
      "Train Epoch: 3 [16000/60000 (27%)] Loss: 0.38\n",
      "Train Epoch: 3 [16400/60000 (27%)] Loss: 0.25\n",
      "Train Epoch: 3 [16800/60000 (28%)] Loss: 0.51\n",
      "Train Epoch: 3 [17200/60000 (29%)] Loss: 0.31\n",
      "Train Epoch: 3 [17600/60000 (29%)] Loss: 0.20\n",
      "Train Epoch: 3 [18000/60000 (30%)] Loss: 0.18\n",
      "Train Epoch: 3 [18400/60000 (31%)] Loss: 0.26\n",
      "Train Epoch: 3 [18800/60000 (31%)] Loss: 0.31\n",
      "Train Epoch: 3 [19200/60000 (32%)] Loss: 0.44\n",
      "Train Epoch: 3 [19600/60000 (33%)] Loss: 0.19\n",
      "Train Epoch: 3 [20000/60000 (33%)] Loss: 0.20\n",
      "Train Epoch: 3 [20400/60000 (34%)] Loss: 0.33\n",
      "Train Epoch: 3 [20800/60000 (35%)] Loss: 0.28\n",
      "Train Epoch: 3 [21200/60000 (35%)] Loss: 0.26\n",
      "Train Epoch: 3 [21600/60000 (36%)] Loss: 0.13\n",
      "Train Epoch: 3 [22000/60000 (37%)] Loss: 0.29\n",
      "Train Epoch: 3 [22400/60000 (37%)] Loss: 0.29\n",
      "Train Epoch: 3 [22800/60000 (38%)] Loss: 0.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [23200/60000 (39%)] Loss: 0.29\n",
      "Train Epoch: 3 [23600/60000 (39%)] Loss: 0.17\n",
      "Train Epoch: 3 [24000/60000 (40%)] Loss: 0.18\n",
      "Train Epoch: 3 [24400/60000 (41%)] Loss: 0.38\n",
      "Train Epoch: 3 [24800/60000 (41%)] Loss: 0.40\n",
      "Train Epoch: 3 [25200/60000 (42%)] Loss: 0.22\n",
      "Train Epoch: 3 [25600/60000 (43%)] Loss: 0.38\n",
      "Train Epoch: 3 [26000/60000 (43%)] Loss: 0.33\n",
      "Train Epoch: 3 [26400/60000 (44%)] Loss: 0.24\n",
      "Train Epoch: 3 [26800/60000 (45%)] Loss: 0.29\n",
      "Train Epoch: 3 [27200/60000 (45%)] Loss: 0.24\n",
      "Train Epoch: 3 [27600/60000 (46%)] Loss: 0.27\n",
      "Train Epoch: 3 [28000/60000 (47%)] Loss: 0.23\n",
      "Train Epoch: 3 [28400/60000 (47%)] Loss: 0.43\n",
      "Train Epoch: 3 [28800/60000 (48%)] Loss: 0.55\n",
      "Train Epoch: 3 [29200/60000 (49%)] Loss: 0.46\n",
      "Train Epoch: 3 [29600/60000 (49%)] Loss: 0.64\n",
      "Train Epoch: 3 [30000/60000 (50%)] Loss: 0.27\n",
      "Train Epoch: 3 [30400/60000 (51%)] Loss: 0.23\n",
      "Train Epoch: 3 [30800/60000 (51%)] Loss: 0.33\n",
      "Train Epoch: 3 [31200/60000 (52%)] Loss: 0.26\n",
      "Train Epoch: 3 [31600/60000 (53%)] Loss: 0.16\n",
      "Train Epoch: 3 [32000/60000 (53%)] Loss: 0.42\n",
      "Train Epoch: 3 [32400/60000 (54%)] Loss: 0.28\n",
      "Train Epoch: 3 [32800/60000 (55%)] Loss: 0.35\n",
      "Train Epoch: 3 [33200/60000 (55%)] Loss: 0.21\n",
      "Train Epoch: 3 [33600/60000 (56%)] Loss: 0.29\n",
      "Train Epoch: 3 [34000/60000 (57%)] Loss: 0.40\n",
      "Train Epoch: 3 [34400/60000 (57%)] Loss: 0.34\n",
      "Train Epoch: 3 [34800/60000 (58%)] Loss: 0.25\n",
      "Train Epoch: 3 [35200/60000 (59%)] Loss: 0.25\n",
      "Train Epoch: 3 [35600/60000 (59%)] Loss: 0.26\n",
      "Train Epoch: 3 [36000/60000 (60%)] Loss: 0.29\n",
      "Train Epoch: 3 [36400/60000 (61%)] Loss: 0.20\n",
      "Train Epoch: 3 [36800/60000 (61%)] Loss: 0.12\n",
      "Train Epoch: 3 [37200/60000 (62%)] Loss: 0.25\n",
      "Train Epoch: 3 [37600/60000 (63%)] Loss: 0.28\n",
      "Train Epoch: 3 [38000/60000 (63%)] Loss: 0.23\n",
      "Train Epoch: 3 [38400/60000 (64%)] Loss: 0.36\n",
      "Train Epoch: 3 [38800/60000 (65%)] Loss: 0.28\n",
      "Train Epoch: 3 [39200/60000 (65%)] Loss: 0.64\n",
      "Train Epoch: 3 [39600/60000 (66%)] Loss: 0.43\n",
      "Train Epoch: 3 [40000/60000 (67%)] Loss: 0.43\n",
      "Train Epoch: 3 [40400/60000 (67%)] Loss: 0.46\n",
      "Train Epoch: 3 [40800/60000 (68%)] Loss: 0.40\n",
      "Train Epoch: 3 [41200/60000 (69%)] Loss: 0.30\n",
      "Train Epoch: 3 [41600/60000 (69%)] Loss: 0.21\n",
      "Train Epoch: 3 [42000/60000 (70%)] Loss: 0.34\n",
      "Train Epoch: 3 [42400/60000 (71%)] Loss: 0.21\n",
      "Train Epoch: 3 [42800/60000 (71%)] Loss: 0.37\n",
      "Train Epoch: 3 [43200/60000 (72%)] Loss: 0.22\n",
      "Train Epoch: 3 [43600/60000 (73%)] Loss: 0.25\n",
      "Train Epoch: 3 [44000/60000 (73%)] Loss: 0.30\n",
      "Train Epoch: 3 [44400/60000 (74%)] Loss: 0.38\n",
      "Train Epoch: 3 [44800/60000 (75%)] Loss: 0.18\n",
      "Train Epoch: 3 [45200/60000 (75%)] Loss: 0.23\n",
      "Train Epoch: 3 [45600/60000 (76%)] Loss: 0.27\n",
      "Train Epoch: 3 [46000/60000 (77%)] Loss: 0.31\n",
      "Train Epoch: 3 [46400/60000 (77%)] Loss: 0.49\n",
      "Train Epoch: 3 [46800/60000 (78%)] Loss: 0.32\n",
      "Train Epoch: 3 [47200/60000 (79%)] Loss: 0.50\n",
      "Train Epoch: 3 [47600/60000 (79%)] Loss: 0.21\n",
      "Train Epoch: 3 [48000/60000 (80%)] Loss: 0.19\n",
      "Train Epoch: 3 [48400/60000 (81%)] Loss: 0.47\n",
      "Train Epoch: 3 [48800/60000 (81%)] Loss: 0.23\n",
      "Train Epoch: 3 [49200/60000 (82%)] Loss: 0.19\n",
      "Train Epoch: 3 [49600/60000 (83%)] Loss: 0.23\n",
      "Train Epoch: 3 [50000/60000 (83%)] Loss: 0.36\n",
      "Train Epoch: 3 [50400/60000 (84%)] Loss: 0.25\n",
      "Train Epoch: 3 [50800/60000 (85%)] Loss: 0.17\n",
      "Train Epoch: 3 [51200/60000 (85%)] Loss: 0.22\n",
      "Train Epoch: 3 [51600/60000 (86%)] Loss: 0.19\n",
      "Train Epoch: 3 [52000/60000 (87%)] Loss: 0.14\n",
      "Train Epoch: 3 [52400/60000 (87%)] Loss: 0.15\n",
      "Train Epoch: 3 [52800/60000 (88%)] Loss: 0.24\n",
      "Train Epoch: 3 [53200/60000 (89%)] Loss: 0.17\n",
      "Train Epoch: 3 [53600/60000 (89%)] Loss: 0.25\n",
      "Train Epoch: 3 [54000/60000 (90%)] Loss: 0.29\n",
      "Train Epoch: 3 [54400/60000 (91%)] Loss: 0.26\n",
      "Train Epoch: 3 [54800/60000 (91%)] Loss: 0.25\n",
      "Train Epoch: 3 [55200/60000 (92%)] Loss: 0.32\n",
      "Train Epoch: 3 [55600/60000 (93%)] Loss: 0.36\n",
      "Train Epoch: 3 [56000/60000 (93%)] Loss: 0.35\n",
      "Train Epoch: 3 [56400/60000 (94%)] Loss: 0.32\n",
      "Train Epoch: 3 [56800/60000 (95%)] Loss: 0.22\n",
      "Train Epoch: 3 [57200/60000 (95%)] Loss: 0.31\n",
      "Train Epoch: 3 [57600/60000 (96%)] Loss: 0.46\n",
      "Train Epoch: 3 [58000/60000 (97%)] Loss: 0.39\n",
      "Train Epoch: 3 [58400/60000 (97%)] Loss: 0.20\n",
      "Train Epoch: 3 [58800/60000 (98%)] Loss: 0.24\n",
      "Train Epoch: 3 [59200/60000 (99%)] Loss: 0.26\n",
      "Train Epoch: 3 [59600/60000 (99%)] Loss: 0.24\n",
      "Train Epoch: 3 [60000/60000 (100%)] Loss: 0.40\n",
      "Test set: Accuracy: 9373.0/10000 (94%), Average Loss: 0.00\n",
      "Train Epoch: 4 [400/60000 (1%)] Loss: 0.30\n",
      "Train Epoch: 4 [800/60000 (1%)] Loss: 0.22\n",
      "Train Epoch: 4 [1200/60000 (2%)] Loss: 0.22\n",
      "Train Epoch: 4 [1600/60000 (3%)] Loss: 0.30\n",
      "Train Epoch: 4 [2000/60000 (3%)] Loss: 0.23\n",
      "Train Epoch: 4 [2400/60000 (4%)] Loss: 0.21\n",
      "Train Epoch: 4 [2800/60000 (5%)] Loss: 0.23\n",
      "Train Epoch: 4 [3200/60000 (5%)] Loss: 0.20\n",
      "Train Epoch: 4 [3600/60000 (6%)] Loss: 0.19\n",
      "Train Epoch: 4 [4000/60000 (7%)] Loss: 0.25\n",
      "Train Epoch: 4 [4400/60000 (7%)] Loss: 0.35\n",
      "Train Epoch: 4 [4800/60000 (8%)] Loss: 0.21\n",
      "Train Epoch: 4 [5200/60000 (9%)] Loss: 0.30\n",
      "Train Epoch: 4 [5600/60000 (9%)] Loss: 0.29\n",
      "Train Epoch: 4 [6000/60000 (10%)] Loss: 0.13\n",
      "Train Epoch: 4 [6400/60000 (11%)] Loss: 0.19\n",
      "Train Epoch: 4 [6800/60000 (11%)] Loss: 0.26\n",
      "Train Epoch: 4 [7200/60000 (12%)] Loss: 0.33\n",
      "Train Epoch: 4 [7600/60000 (13%)] Loss: 0.22\n",
      "Train Epoch: 4 [8000/60000 (13%)] Loss: 0.28\n",
      "Train Epoch: 4 [8400/60000 (14%)] Loss: 0.54\n",
      "Train Epoch: 4 [8800/60000 (15%)] Loss: 0.36\n",
      "Train Epoch: 4 [9200/60000 (15%)] Loss: 0.54\n",
      "Train Epoch: 4 [9600/60000 (16%)] Loss: 0.27\n",
      "Train Epoch: 4 [10000/60000 (17%)] Loss: 0.43\n",
      "Train Epoch: 4 [10400/60000 (17%)] Loss: 0.35\n",
      "Train Epoch: 4 [10800/60000 (18%)] Loss: 0.31\n",
      "Train Epoch: 4 [11200/60000 (19%)] Loss: 0.23\n",
      "Train Epoch: 4 [11600/60000 (19%)] Loss: 0.20\n",
      "Train Epoch: 4 [12000/60000 (20%)] Loss: 0.32\n",
      "Train Epoch: 4 [12400/60000 (21%)] Loss: 0.21\n",
      "Train Epoch: 4 [12800/60000 (21%)] Loss: 0.28\n",
      "Train Epoch: 4 [13200/60000 (22%)] Loss: 0.16\n",
      "Train Epoch: 4 [13600/60000 (23%)] Loss: 0.23\n",
      "Train Epoch: 4 [14000/60000 (23%)] Loss: 0.38\n",
      "Train Epoch: 4 [14400/60000 (24%)] Loss: 0.26\n",
      "Train Epoch: 4 [14800/60000 (25%)] Loss: 0.19\n",
      "Train Epoch: 4 [15200/60000 (25%)] Loss: 0.22\n",
      "Train Epoch: 4 [15600/60000 (26%)] Loss: 0.20\n",
      "Train Epoch: 4 [16000/60000 (27%)] Loss: 0.25\n",
      "Train Epoch: 4 [16400/60000 (27%)] Loss: 0.19\n",
      "Train Epoch: 4 [16800/60000 (28%)] Loss: 0.27\n",
      "Train Epoch: 4 [17200/60000 (29%)] Loss: 0.25\n",
      "Train Epoch: 4 [17600/60000 (29%)] Loss: 0.27\n",
      "Train Epoch: 4 [18000/60000 (30%)] Loss: 0.25\n",
      "Train Epoch: 4 [18400/60000 (31%)] Loss: 0.21\n",
      "Train Epoch: 4 [18800/60000 (31%)] Loss: 0.21\n",
      "Train Epoch: 4 [19200/60000 (32%)] Loss: 0.31\n",
      "Train Epoch: 4 [19600/60000 (33%)] Loss: 0.22\n",
      "Train Epoch: 4 [20000/60000 (33%)] Loss: 0.35\n",
      "Train Epoch: 4 [20400/60000 (34%)] Loss: 0.21\n",
      "Train Epoch: 4 [20800/60000 (35%)] Loss: 0.18\n",
      "Train Epoch: 4 [21200/60000 (35%)] Loss: 0.31\n",
      "Train Epoch: 4 [21600/60000 (36%)] Loss: 0.27\n",
      "Train Epoch: 4 [22000/60000 (37%)] Loss: 0.27\n",
      "Train Epoch: 4 [22400/60000 (37%)] Loss: 0.26\n",
      "Train Epoch: 4 [22800/60000 (38%)] Loss: 0.24\n",
      "Train Epoch: 4 [23200/60000 (39%)] Loss: 0.22\n",
      "Train Epoch: 4 [23600/60000 (39%)] Loss: 0.25\n",
      "Train Epoch: 4 [24000/60000 (40%)] Loss: 0.30\n",
      "Train Epoch: 4 [24400/60000 (41%)] Loss: 0.21\n",
      "Train Epoch: 4 [24800/60000 (41%)] Loss: 0.42\n",
      "Train Epoch: 4 [25200/60000 (42%)] Loss: 0.31\n",
      "Train Epoch: 4 [25600/60000 (43%)] Loss: 0.36\n",
      "Train Epoch: 4 [26000/60000 (43%)] Loss: 0.21\n",
      "Train Epoch: 4 [26400/60000 (44%)] Loss: 0.25\n",
      "Train Epoch: 4 [26800/60000 (45%)] Loss: 0.27\n",
      "Train Epoch: 4 [27200/60000 (45%)] Loss: 0.30\n",
      "Train Epoch: 4 [27600/60000 (46%)] Loss: 0.20\n",
      "Train Epoch: 4 [28000/60000 (47%)] Loss: 0.26\n",
      "Train Epoch: 4 [28400/60000 (47%)] Loss: 0.34\n",
      "Train Epoch: 4 [28800/60000 (48%)] Loss: 0.51\n",
      "Train Epoch: 4 [29200/60000 (49%)] Loss: 0.29\n",
      "Train Epoch: 4 [29600/60000 (49%)] Loss: 0.37\n",
      "Train Epoch: 4 [30000/60000 (50%)] Loss: 0.46\n",
      "Train Epoch: 4 [30400/60000 (51%)] Loss: 0.58\n",
      "Train Epoch: 4 [30800/60000 (51%)] Loss: 0.39\n",
      "Train Epoch: 4 [31200/60000 (52%)] Loss: 0.23\n",
      "Train Epoch: 4 [31600/60000 (53%)] Loss: 0.24\n",
      "Train Epoch: 4 [32000/60000 (53%)] Loss: 0.25\n",
      "Train Epoch: 4 [32400/60000 (54%)] Loss: 0.37\n",
      "Train Epoch: 4 [32800/60000 (55%)] Loss: 0.27\n",
      "Train Epoch: 4 [33200/60000 (55%)] Loss: 0.33\n",
      "Train Epoch: 4 [33600/60000 (56%)] Loss: 0.23\n",
      "Train Epoch: 4 [34000/60000 (57%)] Loss: 0.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [34400/60000 (57%)] Loss: 0.24\n",
      "Train Epoch: 4 [34800/60000 (58%)] Loss: 0.29\n",
      "Train Epoch: 4 [35200/60000 (59%)] Loss: 0.23\n",
      "Train Epoch: 4 [35600/60000 (59%)] Loss: 0.26\n",
      "Train Epoch: 4 [36000/60000 (60%)] Loss: 0.35\n",
      "Train Epoch: 4 [36400/60000 (61%)] Loss: 0.27\n",
      "Train Epoch: 4 [36800/60000 (61%)] Loss: 0.22\n",
      "Train Epoch: 4 [37200/60000 (62%)] Loss: 0.22\n",
      "Train Epoch: 4 [37600/60000 (63%)] Loss: 0.33\n",
      "Train Epoch: 4 [38000/60000 (63%)] Loss: 0.29\n",
      "Train Epoch: 4 [38400/60000 (64%)] Loss: 0.33\n",
      "Train Epoch: 4 [38800/60000 (65%)] Loss: 0.43\n",
      "Train Epoch: 4 [39200/60000 (65%)] Loss: 0.24\n",
      "Train Epoch: 4 [39600/60000 (66%)] Loss: 0.24\n",
      "Train Epoch: 4 [40000/60000 (67%)] Loss: 0.19\n",
      "Train Epoch: 4 [40400/60000 (67%)] Loss: 0.19\n",
      "Train Epoch: 4 [40800/60000 (68%)] Loss: 0.34\n",
      "Train Epoch: 4 [41200/60000 (69%)] Loss: 0.16\n",
      "Train Epoch: 4 [41600/60000 (69%)] Loss: 0.28\n",
      "Train Epoch: 4 [42000/60000 (70%)] Loss: 0.39\n",
      "Train Epoch: 4 [42400/60000 (71%)] Loss: 0.30\n",
      "Train Epoch: 4 [42800/60000 (71%)] Loss: 0.16\n",
      "Train Epoch: 4 [43200/60000 (72%)] Loss: 0.25\n",
      "Train Epoch: 4 [43600/60000 (73%)] Loss: 0.31\n",
      "Train Epoch: 4 [44000/60000 (73%)] Loss: 0.32\n",
      "Train Epoch: 4 [44400/60000 (74%)] Loss: 0.23\n",
      "Train Epoch: 4 [44800/60000 (75%)] Loss: 0.28\n",
      "Train Epoch: 4 [45200/60000 (75%)] Loss: 0.37\n",
      "Train Epoch: 4 [45600/60000 (76%)] Loss: 0.40\n",
      "Train Epoch: 4 [46000/60000 (77%)] Loss: 0.30\n",
      "Train Epoch: 4 [46400/60000 (77%)] Loss: 0.20\n",
      "Train Epoch: 4 [46800/60000 (78%)] Loss: 0.31\n",
      "Train Epoch: 4 [47200/60000 (79%)] Loss: 0.26\n",
      "Train Epoch: 4 [47600/60000 (79%)] Loss: 0.22\n",
      "Train Epoch: 4 [48000/60000 (80%)] Loss: 0.27\n",
      "Train Epoch: 4 [48400/60000 (81%)] Loss: 0.25\n",
      "Train Epoch: 4 [48800/60000 (81%)] Loss: 0.23\n",
      "Train Epoch: 4 [49200/60000 (82%)] Loss: 0.31\n",
      "Train Epoch: 4 [49600/60000 (83%)] Loss: 0.28\n",
      "Train Epoch: 4 [50000/60000 (83%)] Loss: 0.46\n",
      "Train Epoch: 4 [50400/60000 (84%)] Loss: 0.22\n",
      "Train Epoch: 4 [50800/60000 (85%)] Loss: 0.20\n",
      "Train Epoch: 4 [51200/60000 (85%)] Loss: 0.25\n",
      "Train Epoch: 4 [51600/60000 (86%)] Loss: 0.34\n",
      "Train Epoch: 4 [52000/60000 (87%)] Loss: 0.34\n",
      "Train Epoch: 4 [52400/60000 (87%)] Loss: 0.17\n",
      "Train Epoch: 4 [52800/60000 (88%)] Loss: 0.23\n",
      "Train Epoch: 4 [53200/60000 (89%)] Loss: 0.25\n",
      "Train Epoch: 4 [53600/60000 (89%)] Loss: 0.18\n",
      "Train Epoch: 4 [54000/60000 (90%)] Loss: 0.24\n",
      "Train Epoch: 4 [54400/60000 (91%)] Loss: 0.15\n",
      "Train Epoch: 4 [54800/60000 (91%)] Loss: 0.43\n",
      "Train Epoch: 4 [55200/60000 (92%)] Loss: 0.25\n",
      "Train Epoch: 4 [55600/60000 (93%)] Loss: 0.59\n",
      "Train Epoch: 4 [56000/60000 (93%)] Loss: 0.39\n",
      "Train Epoch: 4 [56400/60000 (94%)] Loss: 0.19\n",
      "Train Epoch: 4 [56800/60000 (95%)] Loss: 0.19\n",
      "Train Epoch: 4 [57200/60000 (95%)] Loss: 0.28\n",
      "Train Epoch: 4 [57600/60000 (96%)] Loss: 0.28\n",
      "Train Epoch: 4 [58000/60000 (97%)] Loss: 0.32\n",
      "Train Epoch: 4 [58400/60000 (97%)] Loss: 0.43\n",
      "Train Epoch: 4 [58800/60000 (98%)] Loss: 0.36\n",
      "Train Epoch: 4 [59200/60000 (99%)] Loss: 0.27\n",
      "Train Epoch: 4 [59600/60000 (99%)] Loss: 0.28\n",
      "Train Epoch: 4 [60000/60000 (100%)] Loss: 0.25\n",
      "Test set: Accuracy: 9492.0/10000 (95%), Average Loss: 0.00\n",
      "Train Epoch: 1 [400/60000 (1%)] Loss: 2.11\n",
      "Train Epoch: 1 [800/60000 (1%)] Loss: 1.47\n",
      "Train Epoch: 1 [1200/60000 (2%)] Loss: 0.97\n",
      "Train Epoch: 1 [1600/60000 (3%)] Loss: 0.71\n",
      "Train Epoch: 1 [2000/60000 (3%)] Loss: 0.49\n",
      "Train Epoch: 1 [2400/60000 (4%)] Loss: 0.63\n",
      "Train Epoch: 1 [2800/60000 (5%)] Loss: 0.39\n",
      "Train Epoch: 1 [3200/60000 (5%)] Loss: 0.40\n",
      "Train Epoch: 1 [3600/60000 (6%)] Loss: 0.72\n",
      "Train Epoch: 1 [4000/60000 (7%)] Loss: 0.44\n",
      "Train Epoch: 1 [4400/60000 (7%)] Loss: 0.38\n",
      "Train Epoch: 1 [4800/60000 (8%)] Loss: 0.35\n",
      "Train Epoch: 1 [5200/60000 (9%)] Loss: 0.35\n",
      "Train Epoch: 1 [5600/60000 (9%)] Loss: 0.25\n",
      "Train Epoch: 1 [6000/60000 (10%)] Loss: 0.33\n",
      "Train Epoch: 1 [6400/60000 (11%)] Loss: 0.26\n",
      "Train Epoch: 1 [6800/60000 (11%)] Loss: 0.42\n",
      "Train Epoch: 1 [7200/60000 (12%)] Loss: 0.26\n",
      "Train Epoch: 1 [7600/60000 (13%)] Loss: 0.31\n",
      "Train Epoch: 1 [8000/60000 (13%)] Loss: 0.47\n",
      "Train Epoch: 1 [8400/60000 (14%)] Loss: 0.34\n",
      "Train Epoch: 1 [8800/60000 (15%)] Loss: 0.37\n",
      "Train Epoch: 1 [9200/60000 (15%)] Loss: 0.33\n",
      "Train Epoch: 1 [9600/60000 (16%)] Loss: 0.39\n",
      "Train Epoch: 1 [10000/60000 (17%)] Loss: 0.36\n",
      "Train Epoch: 1 [10400/60000 (17%)] Loss: 0.32\n",
      "Train Epoch: 1 [10800/60000 (18%)] Loss: 0.36\n",
      "Train Epoch: 1 [11200/60000 (19%)] Loss: 0.26\n",
      "Train Epoch: 1 [11600/60000 (19%)] Loss: 0.33\n",
      "Train Epoch: 1 [12000/60000 (20%)] Loss: 0.18\n",
      "Train Epoch: 1 [12400/60000 (21%)] Loss: 0.30\n",
      "Train Epoch: 1 [12800/60000 (21%)] Loss: 0.31\n",
      "Train Epoch: 1 [13200/60000 (22%)] Loss: 0.27\n",
      "Train Epoch: 1 [13600/60000 (23%)] Loss: 0.30\n",
      "Train Epoch: 1 [14000/60000 (23%)] Loss: 0.33\n",
      "Train Epoch: 1 [14400/60000 (24%)] Loss: 0.48\n",
      "Train Epoch: 1 [14800/60000 (25%)] Loss: 0.31\n",
      "Train Epoch: 1 [15200/60000 (25%)] Loss: 0.32\n",
      "Train Epoch: 1 [15600/60000 (26%)] Loss: 0.29\n",
      "Train Epoch: 1 [16000/60000 (27%)] Loss: 0.37\n",
      "Train Epoch: 1 [16400/60000 (27%)] Loss: 0.35\n",
      "Train Epoch: 1 [16800/60000 (28%)] Loss: 0.32\n",
      "Train Epoch: 1 [17200/60000 (29%)] Loss: 0.28\n",
      "Train Epoch: 1 [17600/60000 (29%)] Loss: 0.37\n",
      "Train Epoch: 1 [18000/60000 (30%)] Loss: 0.41\n",
      "Train Epoch: 1 [18400/60000 (31%)] Loss: 0.34\n",
      "Train Epoch: 1 [18800/60000 (31%)] Loss: 0.30\n",
      "Train Epoch: 1 [19200/60000 (32%)] Loss: 0.25\n",
      "Train Epoch: 1 [19600/60000 (33%)] Loss: 0.34\n",
      "Train Epoch: 1 [20000/60000 (33%)] Loss: 0.27\n",
      "Train Epoch: 1 [20400/60000 (34%)] Loss: 0.36\n",
      "Train Epoch: 1 [20800/60000 (35%)] Loss: 0.23\n",
      "Train Epoch: 1 [21200/60000 (35%)] Loss: 0.13\n",
      "Train Epoch: 1 [21600/60000 (36%)] Loss: 0.45\n",
      "Train Epoch: 1 [22000/60000 (37%)] Loss: 0.34\n",
      "Train Epoch: 1 [22400/60000 (37%)] Loss: 0.21\n",
      "Train Epoch: 1 [22800/60000 (38%)] Loss: 0.32\n",
      "Train Epoch: 1 [23200/60000 (39%)] Loss: 0.28\n",
      "Train Epoch: 1 [23600/60000 (39%)] Loss: 0.33\n",
      "Train Epoch: 1 [24000/60000 (40%)] Loss: 0.20\n",
      "Train Epoch: 1 [24400/60000 (41%)] Loss: 0.31\n",
      "Train Epoch: 1 [24800/60000 (41%)] Loss: 0.33\n",
      "Train Epoch: 1 [25200/60000 (42%)] Loss: 0.31\n",
      "Train Epoch: 1 [25600/60000 (43%)] Loss: 0.27\n",
      "Train Epoch: 1 [26000/60000 (43%)] Loss: 0.20\n",
      "Train Epoch: 1 [26400/60000 (44%)] Loss: 0.25\n",
      "Train Epoch: 1 [26800/60000 (45%)] Loss: 0.28\n",
      "Train Epoch: 1 [27200/60000 (45%)] Loss: 0.25\n",
      "Train Epoch: 1 [27600/60000 (46%)] Loss: 0.23\n",
      "Train Epoch: 1 [28000/60000 (47%)] Loss: 0.22\n",
      "Train Epoch: 1 [28400/60000 (47%)] Loss: 0.37\n",
      "Train Epoch: 1 [28800/60000 (48%)] Loss: 0.37\n",
      "Train Epoch: 1 [29200/60000 (49%)] Loss: 0.28\n",
      "Train Epoch: 1 [29600/60000 (49%)] Loss: 0.30\n",
      "Train Epoch: 1 [30000/60000 (50%)] Loss: 0.37\n",
      "Train Epoch: 1 [30400/60000 (51%)] Loss: 0.31\n",
      "Train Epoch: 1 [30800/60000 (51%)] Loss: 0.20\n",
      "Train Epoch: 1 [31200/60000 (52%)] Loss: 0.29\n",
      "Train Epoch: 1 [31600/60000 (53%)] Loss: 0.25\n",
      "Train Epoch: 1 [32000/60000 (53%)] Loss: 0.29\n",
      "Train Epoch: 1 [32400/60000 (54%)] Loss: 0.31\n",
      "Train Epoch: 1 [32800/60000 (55%)] Loss: 0.22\n",
      "Train Epoch: 1 [33200/60000 (55%)] Loss: 0.23\n",
      "Train Epoch: 1 [33600/60000 (56%)] Loss: 0.32\n",
      "Train Epoch: 1 [34000/60000 (57%)] Loss: 0.40\n",
      "Train Epoch: 1 [34400/60000 (57%)] Loss: 0.30\n",
      "Train Epoch: 1 [34800/60000 (58%)] Loss: 0.34\n",
      "Train Epoch: 1 [35200/60000 (59%)] Loss: 0.31\n",
      "Train Epoch: 1 [35600/60000 (59%)] Loss: 0.32\n",
      "Train Epoch: 1 [36000/60000 (60%)] Loss: 0.22\n",
      "Train Epoch: 1 [36400/60000 (61%)] Loss: 0.31\n",
      "Train Epoch: 1 [36800/60000 (61%)] Loss: 0.34\n",
      "Train Epoch: 1 [37200/60000 (62%)] Loss: 0.37\n",
      "Train Epoch: 1 [37600/60000 (63%)] Loss: 0.33\n",
      "Train Epoch: 1 [38000/60000 (63%)] Loss: 0.19\n",
      "Train Epoch: 1 [38400/60000 (64%)] Loss: 0.29\n",
      "Train Epoch: 1 [38800/60000 (65%)] Loss: 0.29\n",
      "Train Epoch: 1 [39200/60000 (65%)] Loss: 0.28\n",
      "Train Epoch: 1 [39600/60000 (66%)] Loss: 0.18\n",
      "Train Epoch: 1 [40000/60000 (67%)] Loss: 0.53\n",
      "Train Epoch: 1 [40400/60000 (67%)] Loss: 0.28\n",
      "Train Epoch: 1 [40800/60000 (68%)] Loss: 0.25\n",
      "Train Epoch: 1 [41200/60000 (69%)] Loss: 0.22\n",
      "Train Epoch: 1 [41600/60000 (69%)] Loss: 0.16\n",
      "Train Epoch: 1 [42000/60000 (70%)] Loss: 0.26\n",
      "Train Epoch: 1 [42400/60000 (71%)] Loss: 0.19\n",
      "Train Epoch: 1 [42800/60000 (71%)] Loss: 0.19\n",
      "Train Epoch: 1 [43200/60000 (72%)] Loss: 0.36\n",
      "Train Epoch: 1 [43600/60000 (73%)] Loss: 0.22\n",
      "Train Epoch: 1 [44000/60000 (73%)] Loss: 0.27\n",
      "Train Epoch: 1 [44400/60000 (74%)] Loss: 0.29\n",
      "Train Epoch: 1 [44800/60000 (75%)] Loss: 0.33\n",
      "Train Epoch: 1 [45200/60000 (75%)] Loss: 0.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [45600/60000 (76%)] Loss: 0.20\n",
      "Train Epoch: 1 [46000/60000 (77%)] Loss: 0.39\n",
      "Train Epoch: 1 [46400/60000 (77%)] Loss: 0.26\n",
      "Train Epoch: 1 [46800/60000 (78%)] Loss: 0.38\n",
      "Train Epoch: 1 [47200/60000 (79%)] Loss: 0.37\n",
      "Train Epoch: 1 [47600/60000 (79%)] Loss: 0.26\n",
      "Train Epoch: 1 [48000/60000 (80%)] Loss: 0.19\n",
      "Train Epoch: 1 [48400/60000 (81%)] Loss: 0.34\n",
      "Train Epoch: 1 [48800/60000 (81%)] Loss: 0.33\n",
      "Train Epoch: 1 [49200/60000 (82%)] Loss: 0.19\n",
      "Train Epoch: 1 [49600/60000 (83%)] Loss: 0.31\n",
      "Train Epoch: 1 [50000/60000 (83%)] Loss: 0.26\n",
      "Train Epoch: 1 [50400/60000 (84%)] Loss: 0.64\n",
      "Train Epoch: 1 [50800/60000 (85%)] Loss: 0.24\n",
      "Train Epoch: 1 [51200/60000 (85%)] Loss: 0.37\n",
      "Train Epoch: 1 [51600/60000 (86%)] Loss: 0.32\n",
      "Train Epoch: 1 [52000/60000 (87%)] Loss: 0.25\n",
      "Train Epoch: 1 [52400/60000 (87%)] Loss: 0.25\n",
      "Train Epoch: 1 [52800/60000 (88%)] Loss: 0.19\n",
      "Train Epoch: 1 [53200/60000 (89%)] Loss: 0.41\n",
      "Train Epoch: 1 [53600/60000 (89%)] Loss: 0.37\n",
      "Train Epoch: 1 [54000/60000 (90%)] Loss: 0.16\n",
      "Train Epoch: 1 [54400/60000 (91%)] Loss: 0.38\n",
      "Train Epoch: 1 [54800/60000 (91%)] Loss: 0.42\n",
      "Train Epoch: 1 [55200/60000 (92%)] Loss: 0.43\n",
      "Train Epoch: 1 [55600/60000 (93%)] Loss: 0.28\n",
      "Train Epoch: 1 [56000/60000 (93%)] Loss: 0.24\n",
      "Train Epoch: 1 [56400/60000 (94%)] Loss: 0.22\n",
      "Train Epoch: 1 [56800/60000 (95%)] Loss: 0.33\n",
      "Train Epoch: 1 [57200/60000 (95%)] Loss: 0.28\n",
      "Train Epoch: 1 [57600/60000 (96%)] Loss: 0.19\n",
      "Train Epoch: 1 [58000/60000 (97%)] Loss: 0.39\n",
      "Train Epoch: 1 [58400/60000 (97%)] Loss: 0.25\n",
      "Train Epoch: 1 [58800/60000 (98%)] Loss: 0.23\n",
      "Train Epoch: 1 [59200/60000 (99%)] Loss: 0.24\n",
      "Train Epoch: 1 [59600/60000 (99%)] Loss: 0.19\n",
      "Train Epoch: 1 [60000/60000 (100%)] Loss: 0.27\n",
      "Test set: Accuracy: 9455.0/10000 (95%), Average Loss: 0.00\n",
      "Train Epoch: 2 [400/60000 (1%)] Loss: 0.21\n",
      "Train Epoch: 2 [800/60000 (1%)] Loss: 0.26\n",
      "Train Epoch: 2 [1200/60000 (2%)] Loss: 0.29\n",
      "Train Epoch: 2 [1600/60000 (3%)] Loss: 0.27\n",
      "Train Epoch: 2 [2000/60000 (3%)] Loss: 0.26\n",
      "Train Epoch: 2 [2400/60000 (4%)] Loss: 0.27\n",
      "Train Epoch: 2 [2800/60000 (5%)] Loss: 0.20\n",
      "Train Epoch: 2 [3200/60000 (5%)] Loss: 0.20\n",
      "Train Epoch: 2 [3600/60000 (6%)] Loss: 0.24\n",
      "Train Epoch: 2 [4000/60000 (7%)] Loss: 0.33\n",
      "Train Epoch: 2 [4400/60000 (7%)] Loss: 0.35\n",
      "Train Epoch: 2 [4800/60000 (8%)] Loss: 0.36\n",
      "Train Epoch: 2 [5200/60000 (9%)] Loss: 0.39\n",
      "Train Epoch: 2 [5600/60000 (9%)] Loss: 0.24\n",
      "Train Epoch: 2 [6000/60000 (10%)] Loss: 0.13\n",
      "Train Epoch: 2 [6400/60000 (11%)] Loss: 0.17\n",
      "Train Epoch: 2 [6800/60000 (11%)] Loss: 0.28\n",
      "Train Epoch: 2 [7200/60000 (12%)] Loss: 0.12\n",
      "Train Epoch: 2 [7600/60000 (13%)] Loss: 0.33\n",
      "Train Epoch: 2 [8000/60000 (13%)] Loss: 0.22\n",
      "Train Epoch: 2 [8400/60000 (14%)] Loss: 0.23\n",
      "Train Epoch: 2 [8800/60000 (15%)] Loss: 0.35\n",
      "Train Epoch: 2 [9200/60000 (15%)] Loss: 0.20\n",
      "Train Epoch: 2 [9600/60000 (16%)] Loss: 0.24\n",
      "Train Epoch: 2 [10000/60000 (17%)] Loss: 0.24\n",
      "Train Epoch: 2 [10400/60000 (17%)] Loss: 0.30\n",
      "Train Epoch: 2 [10800/60000 (18%)] Loss: 0.31\n",
      "Train Epoch: 2 [11200/60000 (19%)] Loss: 0.17\n",
      "Train Epoch: 2 [11600/60000 (19%)] Loss: 0.27\n",
      "Train Epoch: 2 [12000/60000 (20%)] Loss: 0.25\n",
      "Train Epoch: 2 [12400/60000 (21%)] Loss: 0.29\n",
      "Train Epoch: 2 [12800/60000 (21%)] Loss: 0.36\n",
      "Train Epoch: 2 [13200/60000 (22%)] Loss: 0.31\n",
      "Train Epoch: 2 [13600/60000 (23%)] Loss: 0.20\n",
      "Train Epoch: 2 [14000/60000 (23%)] Loss: 0.20\n",
      "Train Epoch: 2 [14400/60000 (24%)] Loss: 0.21\n",
      "Train Epoch: 2 [14800/60000 (25%)] Loss: 0.23\n",
      "Train Epoch: 2 [15200/60000 (25%)] Loss: 0.42\n",
      "Train Epoch: 2 [15600/60000 (26%)] Loss: 0.27\n",
      "Train Epoch: 2 [16000/60000 (27%)] Loss: 0.15\n",
      "Train Epoch: 2 [16400/60000 (27%)] Loss: 0.30\n",
      "Train Epoch: 2 [16800/60000 (28%)] Loss: 0.29\n",
      "Train Epoch: 2 [17200/60000 (29%)] Loss: 0.30\n",
      "Train Epoch: 2 [17600/60000 (29%)] Loss: 0.29\n",
      "Train Epoch: 2 [18000/60000 (30%)] Loss: 0.19\n",
      "Train Epoch: 2 [18400/60000 (31%)] Loss: 0.24\n",
      "Train Epoch: 2 [18800/60000 (31%)] Loss: 0.25\n",
      "Train Epoch: 2 [19200/60000 (32%)] Loss: 0.41\n",
      "Train Epoch: 2 [19600/60000 (33%)] Loss: 0.30\n",
      "Train Epoch: 2 [20000/60000 (33%)] Loss: 0.30\n",
      "Train Epoch: 2 [20400/60000 (34%)] Loss: 0.30\n",
      "Train Epoch: 2 [20800/60000 (35%)] Loss: 0.24\n",
      "Train Epoch: 2 [21200/60000 (35%)] Loss: 0.21\n",
      "Train Epoch: 2 [21600/60000 (36%)] Loss: 0.23\n",
      "Train Epoch: 2 [22000/60000 (37%)] Loss: 0.46\n",
      "Train Epoch: 2 [22400/60000 (37%)] Loss: 0.30\n",
      "Train Epoch: 2 [22800/60000 (38%)] Loss: 0.49\n",
      "Train Epoch: 2 [23200/60000 (39%)] Loss: 0.28\n",
      "Train Epoch: 2 [23600/60000 (39%)] Loss: 0.36\n",
      "Train Epoch: 2 [24000/60000 (40%)] Loss: 0.15\n",
      "Train Epoch: 2 [24400/60000 (41%)] Loss: 0.16\n",
      "Train Epoch: 2 [24800/60000 (41%)] Loss: 0.33\n",
      "Train Epoch: 2 [25200/60000 (42%)] Loss: 0.41\n",
      "Train Epoch: 2 [25600/60000 (43%)] Loss: 0.28\n",
      "Train Epoch: 2 [26000/60000 (43%)] Loss: 0.24\n",
      "Train Epoch: 2 [26400/60000 (44%)] Loss: 0.28\n",
      "Train Epoch: 2 [26800/60000 (45%)] Loss: 0.23\n",
      "Train Epoch: 2 [27200/60000 (45%)] Loss: 0.21\n",
      "Train Epoch: 2 [27600/60000 (46%)] Loss: 0.32\n",
      "Train Epoch: 2 [28000/60000 (47%)] Loss: 0.50\n",
      "Train Epoch: 2 [28400/60000 (47%)] Loss: 0.39\n",
      "Train Epoch: 2 [28800/60000 (48%)] Loss: 0.22\n",
      "Train Epoch: 2 [29200/60000 (49%)] Loss: 0.33\n",
      "Train Epoch: 2 [29600/60000 (49%)] Loss: 0.41\n",
      "Train Epoch: 2 [30000/60000 (50%)] Loss: 0.30\n",
      "Train Epoch: 2 [30400/60000 (51%)] Loss: 0.43\n",
      "Train Epoch: 2 [30800/60000 (51%)] Loss: 0.38\n",
      "Train Epoch: 2 [31200/60000 (52%)] Loss: 0.25\n",
      "Train Epoch: 2 [31600/60000 (53%)] Loss: 0.35\n",
      "Train Epoch: 2 [32000/60000 (53%)] Loss: 0.21\n",
      "Train Epoch: 2 [32400/60000 (54%)] Loss: 0.63\n",
      "Train Epoch: 2 [32800/60000 (55%)] Loss: 0.20\n",
      "Train Epoch: 2 [33200/60000 (55%)] Loss: 0.40\n",
      "Train Epoch: 2 [33600/60000 (56%)] Loss: 0.18\n",
      "Train Epoch: 2 [34000/60000 (57%)] Loss: 0.25\n",
      "Train Epoch: 2 [34400/60000 (57%)] Loss: 0.43\n",
      "Train Epoch: 2 [34800/60000 (58%)] Loss: 0.31\n",
      "Train Epoch: 2 [35200/60000 (59%)] Loss: 0.22\n",
      "Train Epoch: 2 [35600/60000 (59%)] Loss: 0.18\n",
      "Train Epoch: 2 [36000/60000 (60%)] Loss: 0.23\n",
      "Train Epoch: 2 [36400/60000 (61%)] Loss: 0.25\n",
      "Train Epoch: 2 [36800/60000 (61%)] Loss: 0.22\n",
      "Train Epoch: 2 [37200/60000 (62%)] Loss: 0.23\n",
      "Train Epoch: 2 [37600/60000 (63%)] Loss: 0.31\n",
      "Train Epoch: 2 [38000/60000 (63%)] Loss: 0.15\n",
      "Train Epoch: 2 [38400/60000 (64%)] Loss: 0.33\n",
      "Train Epoch: 2 [38800/60000 (65%)] Loss: 0.65\n",
      "Train Epoch: 2 [39200/60000 (65%)] Loss: 0.50\n",
      "Train Epoch: 2 [39600/60000 (66%)] Loss: 0.28\n",
      "Train Epoch: 2 [40000/60000 (67%)] Loss: 0.28\n",
      "Train Epoch: 2 [40400/60000 (67%)] Loss: 0.27\n",
      "Train Epoch: 2 [40800/60000 (68%)] Loss: 0.29\n",
      "Train Epoch: 2 [41200/60000 (69%)] Loss: 0.24\n",
      "Train Epoch: 2 [41600/60000 (69%)] Loss: 0.19\n",
      "Train Epoch: 2 [42000/60000 (70%)] Loss: 0.14\n",
      "Train Epoch: 2 [42400/60000 (71%)] Loss: 0.25\n",
      "Train Epoch: 2 [42800/60000 (71%)] Loss: 0.18\n",
      "Train Epoch: 2 [43200/60000 (72%)] Loss: 0.19\n",
      "Train Epoch: 2 [43600/60000 (73%)] Loss: 0.28\n",
      "Train Epoch: 2 [44000/60000 (73%)] Loss: 0.26\n",
      "Train Epoch: 2 [44400/60000 (74%)] Loss: 0.35\n",
      "Train Epoch: 2 [44800/60000 (75%)] Loss: 0.37\n",
      "Train Epoch: 2 [45200/60000 (75%)] Loss: 0.19\n",
      "Train Epoch: 2 [45600/60000 (76%)] Loss: 0.35\n",
      "Train Epoch: 2 [46000/60000 (77%)] Loss: 0.34\n",
      "Train Epoch: 2 [46400/60000 (77%)] Loss: 0.09\n",
      "Train Epoch: 2 [46800/60000 (78%)] Loss: 0.29\n",
      "Train Epoch: 2 [47200/60000 (79%)] Loss: 0.33\n",
      "Train Epoch: 2 [47600/60000 (79%)] Loss: 0.19\n",
      "Train Epoch: 2 [48000/60000 (80%)] Loss: 0.36\n",
      "Train Epoch: 2 [48400/60000 (81%)] Loss: 0.36\n",
      "Train Epoch: 2 [48800/60000 (81%)] Loss: 0.36\n",
      "Train Epoch: 2 [49200/60000 (82%)] Loss: 0.40\n",
      "Train Epoch: 2 [49600/60000 (83%)] Loss: 0.38\n",
      "Train Epoch: 2 [50000/60000 (83%)] Loss: 0.37\n",
      "Train Epoch: 2 [50400/60000 (84%)] Loss: 0.32\n",
      "Train Epoch: 2 [50800/60000 (85%)] Loss: 0.29\n",
      "Train Epoch: 2 [51200/60000 (85%)] Loss: 0.24\n",
      "Train Epoch: 2 [51600/60000 (86%)] Loss: 0.28\n",
      "Train Epoch: 2 [52000/60000 (87%)] Loss: 0.16\n",
      "Train Epoch: 2 [52400/60000 (87%)] Loss: 0.26\n",
      "Train Epoch: 2 [52800/60000 (88%)] Loss: 0.27\n",
      "Train Epoch: 2 [53200/60000 (89%)] Loss: 0.12\n",
      "Train Epoch: 2 [53600/60000 (89%)] Loss: 0.25\n",
      "Train Epoch: 2 [54000/60000 (90%)] Loss: 0.21\n",
      "Train Epoch: 2 [54400/60000 (91%)] Loss: 0.22\n",
      "Train Epoch: 2 [54800/60000 (91%)] Loss: 0.37\n",
      "Train Epoch: 2 [55200/60000 (92%)] Loss: 0.18\n",
      "Train Epoch: 2 [55600/60000 (93%)] Loss: 0.42\n",
      "Train Epoch: 2 [56000/60000 (93%)] Loss: 0.13\n",
      "Train Epoch: 2 [56400/60000 (94%)] Loss: 0.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [56800/60000 (95%)] Loss: 0.48\n",
      "Train Epoch: 2 [57200/60000 (95%)] Loss: 0.40\n",
      "Train Epoch: 2 [57600/60000 (96%)] Loss: 0.32\n",
      "Train Epoch: 2 [58000/60000 (97%)] Loss: 0.41\n",
      "Train Epoch: 2 [58400/60000 (97%)] Loss: 0.44\n",
      "Train Epoch: 2 [58800/60000 (98%)] Loss: 0.18\n",
      "Train Epoch: 2 [59200/60000 (99%)] Loss: 0.21\n",
      "Train Epoch: 2 [59600/60000 (99%)] Loss: 0.34\n",
      "Train Epoch: 2 [60000/60000 (100%)] Loss: 0.28\n",
      "Test set: Accuracy: 9484.0/10000 (95%), Average Loss: 0.00\n",
      "Train Epoch: 3 [400/60000 (1%)] Loss: 0.14\n",
      "Train Epoch: 3 [800/60000 (1%)] Loss: 0.19\n",
      "Train Epoch: 3 [1200/60000 (2%)] Loss: 0.40\n",
      "Train Epoch: 3 [1600/60000 (3%)] Loss: 0.29\n",
      "Train Epoch: 3 [2000/60000 (3%)] Loss: 0.23\n",
      "Train Epoch: 3 [2400/60000 (4%)] Loss: 0.16\n",
      "Train Epoch: 3 [2800/60000 (5%)] Loss: 0.21\n",
      "Train Epoch: 3 [3200/60000 (5%)] Loss: 0.25\n",
      "Train Epoch: 3 [3600/60000 (6%)] Loss: 0.19\n",
      "Train Epoch: 3 [4000/60000 (7%)] Loss: 0.23\n",
      "Train Epoch: 3 [4400/60000 (7%)] Loss: 0.24\n",
      "Train Epoch: 3 [4800/60000 (8%)] Loss: 0.24\n",
      "Train Epoch: 3 [5200/60000 (9%)] Loss: 0.28\n",
      "Train Epoch: 3 [5600/60000 (9%)] Loss: 0.24\n",
      "Train Epoch: 3 [6000/60000 (10%)] Loss: 0.39\n",
      "Train Epoch: 3 [6400/60000 (11%)] Loss: 0.22\n",
      "Train Epoch: 3 [6800/60000 (11%)] Loss: 0.31\n",
      "Train Epoch: 3 [7200/60000 (12%)] Loss: 0.26\n",
      "Train Epoch: 3 [7600/60000 (13%)] Loss: 0.16\n",
      "Train Epoch: 3 [8000/60000 (13%)] Loss: 0.34\n",
      "Train Epoch: 3 [8400/60000 (14%)] Loss: 0.20\n",
      "Train Epoch: 3 [8800/60000 (15%)] Loss: 0.20\n",
      "Train Epoch: 3 [9200/60000 (15%)] Loss: 0.24\n",
      "Train Epoch: 3 [9600/60000 (16%)] Loss: 0.30\n",
      "Train Epoch: 3 [10000/60000 (17%)] Loss: 0.31\n",
      "Train Epoch: 3 [10400/60000 (17%)] Loss: 0.42\n",
      "Train Epoch: 3 [10800/60000 (18%)] Loss: 0.23\n",
      "Train Epoch: 3 [11200/60000 (19%)] Loss: 0.28\n",
      "Train Epoch: 3 [11600/60000 (19%)] Loss: 0.32\n",
      "Train Epoch: 3 [12000/60000 (20%)] Loss: 0.25\n",
      "Train Epoch: 3 [12400/60000 (21%)] Loss: 0.32\n",
      "Train Epoch: 3 [12800/60000 (21%)] Loss: 0.43\n",
      "Train Epoch: 3 [13200/60000 (22%)] Loss: 0.26\n",
      "Train Epoch: 3 [13600/60000 (23%)] Loss: 0.31\n",
      "Train Epoch: 3 [14000/60000 (23%)] Loss: 0.22\n",
      "Train Epoch: 3 [14400/60000 (24%)] Loss: 0.26\n",
      "Train Epoch: 3 [14800/60000 (25%)] Loss: 0.24\n",
      "Train Epoch: 3 [15200/60000 (25%)] Loss: 0.19\n",
      "Train Epoch: 3 [15600/60000 (26%)] Loss: 0.26\n",
      "Train Epoch: 3 [16000/60000 (27%)] Loss: 0.20\n",
      "Train Epoch: 3 [16400/60000 (27%)] Loss: 0.44\n",
      "Train Epoch: 3 [16800/60000 (28%)] Loss: 0.26\n",
      "Train Epoch: 3 [17200/60000 (29%)] Loss: 0.25\n",
      "Train Epoch: 3 [17600/60000 (29%)] Loss: 0.26\n",
      "Train Epoch: 3 [18000/60000 (30%)] Loss: 0.23\n",
      "Train Epoch: 3 [18400/60000 (31%)] Loss: 0.10\n",
      "Train Epoch: 3 [18800/60000 (31%)] Loss: 0.19\n",
      "Train Epoch: 3 [19200/60000 (32%)] Loss: 0.26\n",
      "Train Epoch: 3 [19600/60000 (33%)] Loss: 0.21\n",
      "Train Epoch: 3 [20000/60000 (33%)] Loss: 0.32\n",
      "Train Epoch: 3 [20400/60000 (34%)] Loss: 0.19\n",
      "Train Epoch: 3 [20800/60000 (35%)] Loss: 0.15\n",
      "Train Epoch: 3 [21200/60000 (35%)] Loss: 0.42\n",
      "Train Epoch: 3 [21600/60000 (36%)] Loss: 0.32\n",
      "Train Epoch: 3 [22000/60000 (37%)] Loss: 0.25\n",
      "Train Epoch: 3 [22400/60000 (37%)] Loss: 0.47\n",
      "Train Epoch: 3 [22800/60000 (38%)] Loss: 0.34\n",
      "Train Epoch: 3 [23200/60000 (39%)] Loss: 0.38\n",
      "Train Epoch: 3 [23600/60000 (39%)] Loss: 0.28\n",
      "Train Epoch: 3 [24000/60000 (40%)] Loss: 0.19\n",
      "Train Epoch: 3 [24400/60000 (41%)] Loss: 0.28\n",
      "Train Epoch: 3 [24800/60000 (41%)] Loss: 0.19\n",
      "Train Epoch: 3 [25200/60000 (42%)] Loss: 0.24\n",
      "Train Epoch: 3 [25600/60000 (43%)] Loss: 0.22\n",
      "Train Epoch: 3 [26000/60000 (43%)] Loss: 0.19\n",
      "Train Epoch: 3 [26400/60000 (44%)] Loss: 0.25\n",
      "Train Epoch: 3 [26800/60000 (45%)] Loss: 0.19\n",
      "Train Epoch: 3 [27200/60000 (45%)] Loss: 0.21\n",
      "Train Epoch: 3 [27600/60000 (46%)] Loss: 0.29\n",
      "Train Epoch: 3 [28000/60000 (47%)] Loss: 0.33\n",
      "Train Epoch: 3 [28400/60000 (47%)] Loss: 0.32\n",
      "Train Epoch: 3 [28800/60000 (48%)] Loss: 0.37\n",
      "Train Epoch: 3 [29200/60000 (49%)] Loss: 0.30\n",
      "Train Epoch: 3 [29600/60000 (49%)] Loss: 0.34\n",
      "Train Epoch: 3 [30000/60000 (50%)] Loss: 0.22\n",
      "Train Epoch: 3 [30400/60000 (51%)] Loss: 0.22\n",
      "Train Epoch: 3 [30800/60000 (51%)] Loss: 0.36\n",
      "Train Epoch: 3 [31200/60000 (52%)] Loss: 0.15\n",
      "Train Epoch: 3 [31600/60000 (53%)] Loss: 0.22\n",
      "Train Epoch: 3 [32000/60000 (53%)] Loss: 0.45\n",
      "Train Epoch: 3 [32400/60000 (54%)] Loss: 0.33\n",
      "Train Epoch: 3 [32800/60000 (55%)] Loss: 0.50\n",
      "Train Epoch: 3 [33200/60000 (55%)] Loss: 0.22\n",
      "Train Epoch: 3 [33600/60000 (56%)] Loss: 0.26\n",
      "Train Epoch: 3 [34000/60000 (57%)] Loss: 0.37\n",
      "Train Epoch: 3 [34400/60000 (57%)] Loss: 0.36\n",
      "Train Epoch: 3 [34800/60000 (58%)] Loss: 0.13\n",
      "Train Epoch: 3 [35200/60000 (59%)] Loss: 0.98\n",
      "Train Epoch: 3 [35600/60000 (59%)] Loss: 0.67\n",
      "Train Epoch: 3 [36000/60000 (60%)] Loss: 0.45\n",
      "Train Epoch: 3 [36400/60000 (61%)] Loss: 0.24\n",
      "Train Epoch: 3 [36800/60000 (61%)] Loss: 0.33\n",
      "Train Epoch: 3 [37200/60000 (62%)] Loss: 0.54\n",
      "Train Epoch: 3 [37600/60000 (63%)] Loss: 0.27\n",
      "Train Epoch: 3 [38000/60000 (63%)] Loss: 0.21\n",
      "Train Epoch: 3 [38400/60000 (64%)] Loss: 0.18\n",
      "Train Epoch: 3 [38800/60000 (65%)] Loss: 0.29\n",
      "Train Epoch: 3 [39200/60000 (65%)] Loss: 0.25\n",
      "Train Epoch: 3 [39600/60000 (66%)] Loss: 0.31\n",
      "Train Epoch: 3 [40000/60000 (67%)] Loss: 0.26\n",
      "Train Epoch: 3 [40400/60000 (67%)] Loss: 0.25\n",
      "Train Epoch: 3 [40800/60000 (68%)] Loss: 0.27\n",
      "Train Epoch: 3 [41200/60000 (69%)] Loss: 0.27\n",
      "Train Epoch: 3 [41600/60000 (69%)] Loss: 0.20\n",
      "Train Epoch: 3 [42000/60000 (70%)] Loss: 0.28\n",
      "Train Epoch: 3 [42400/60000 (71%)] Loss: 0.24\n",
      "Train Epoch: 3 [42800/60000 (71%)] Loss: 0.26\n",
      "Train Epoch: 3 [43200/60000 (72%)] Loss: 0.32\n",
      "Train Epoch: 3 [43600/60000 (73%)] Loss: 0.21\n",
      "Train Epoch: 3 [44000/60000 (73%)] Loss: 0.14\n",
      "Train Epoch: 3 [44400/60000 (74%)] Loss: 0.37\n",
      "Train Epoch: 3 [44800/60000 (75%)] Loss: 0.44\n",
      "Train Epoch: 3 [45200/60000 (75%)] Loss: 0.18\n",
      "Train Epoch: 3 [45600/60000 (76%)] Loss: 0.45\n",
      "Train Epoch: 3 [46000/60000 (77%)] Loss: 0.25\n",
      "Train Epoch: 3 [46400/60000 (77%)] Loss: 0.18\n",
      "Train Epoch: 3 [46800/60000 (78%)] Loss: 0.22\n",
      "Train Epoch: 3 [47200/60000 (79%)] Loss: 0.33\n",
      "Train Epoch: 3 [47600/60000 (79%)] Loss: 0.26\n",
      "Train Epoch: 3 [48000/60000 (80%)] Loss: 0.48\n",
      "Train Epoch: 3 [48400/60000 (81%)] Loss: 0.31\n",
      "Train Epoch: 3 [48800/60000 (81%)] Loss: 0.24\n",
      "Train Epoch: 3 [49200/60000 (82%)] Loss: 0.23\n",
      "Train Epoch: 3 [49600/60000 (83%)] Loss: 0.22\n",
      "Train Epoch: 3 [50000/60000 (83%)] Loss: 0.28\n",
      "Train Epoch: 3 [50400/60000 (84%)] Loss: 0.19\n",
      "Train Epoch: 3 [50800/60000 (85%)] Loss: 0.23\n",
      "Train Epoch: 3 [51200/60000 (85%)] Loss: 0.25\n",
      "Train Epoch: 3 [51600/60000 (86%)] Loss: 0.32\n",
      "Train Epoch: 3 [52000/60000 (87%)] Loss: 0.32\n",
      "Train Epoch: 3 [52400/60000 (87%)] Loss: 0.18\n",
      "Train Epoch: 3 [52800/60000 (88%)] Loss: 0.23\n",
      "Train Epoch: 3 [53200/60000 (89%)] Loss: 0.20\n",
      "Train Epoch: 3 [53600/60000 (89%)] Loss: 0.24\n",
      "Train Epoch: 3 [54000/60000 (90%)] Loss: 0.33\n",
      "Train Epoch: 3 [54400/60000 (91%)] Loss: 0.32\n",
      "Train Epoch: 3 [54800/60000 (91%)] Loss: 0.40\n",
      "Train Epoch: 3 [55200/60000 (92%)] Loss: 0.12\n",
      "Train Epoch: 3 [55600/60000 (93%)] Loss: 0.26\n",
      "Train Epoch: 3 [56000/60000 (93%)] Loss: 0.26\n",
      "Train Epoch: 3 [56400/60000 (94%)] Loss: 0.39\n",
      "Train Epoch: 3 [56800/60000 (95%)] Loss: 0.19\n",
      "Train Epoch: 3 [57200/60000 (95%)] Loss: 0.31\n",
      "Train Epoch: 3 [57600/60000 (96%)] Loss: 0.25\n",
      "Train Epoch: 3 [58000/60000 (97%)] Loss: 0.28\n",
      "Train Epoch: 3 [58400/60000 (97%)] Loss: 0.54\n",
      "Train Epoch: 3 [58800/60000 (98%)] Loss: 0.36\n",
      "Train Epoch: 3 [59200/60000 (99%)] Loss: 0.37\n",
      "Train Epoch: 3 [59600/60000 (99%)] Loss: 0.66\n",
      "Train Epoch: 3 [60000/60000 (100%)] Loss: 0.28\n",
      "Test set: Accuracy: 9338.0/10000 (93%), Average Loss: 0.00\n",
      "Train Epoch: 4 [400/60000 (1%)] Loss: 0.29\n",
      "Train Epoch: 4 [800/60000 (1%)] Loss: 0.23\n",
      "Train Epoch: 4 [1200/60000 (2%)] Loss: 0.21\n",
      "Train Epoch: 4 [1600/60000 (3%)] Loss: 0.23\n",
      "Train Epoch: 4 [2000/60000 (3%)] Loss: 0.18\n",
      "Train Epoch: 4 [2400/60000 (4%)] Loss: 0.21\n",
      "Train Epoch: 4 [2800/60000 (5%)] Loss: 0.27\n",
      "Train Epoch: 4 [3200/60000 (5%)] Loss: 0.28\n",
      "Train Epoch: 4 [3600/60000 (6%)] Loss: 0.24\n",
      "Train Epoch: 4 [4000/60000 (7%)] Loss: 0.22\n",
      "Train Epoch: 4 [4400/60000 (7%)] Loss: 0.32\n",
      "Train Epoch: 4 [4800/60000 (8%)] Loss: 0.25\n",
      "Train Epoch: 4 [5200/60000 (9%)] Loss: 0.35\n",
      "Train Epoch: 4 [5600/60000 (9%)] Loss: 0.54\n",
      "Train Epoch: 4 [6000/60000 (10%)] Loss: 0.27\n",
      "Train Epoch: 4 [6400/60000 (11%)] Loss: 0.20\n",
      "Train Epoch: 4 [6800/60000 (11%)] Loss: 0.35\n",
      "Train Epoch: 4 [7200/60000 (12%)] Loss: 0.21\n",
      "Train Epoch: 4 [7600/60000 (13%)] Loss: 0.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [8000/60000 (13%)] Loss: 0.17\n",
      "Train Epoch: 4 [8400/60000 (14%)] Loss: 0.30\n",
      "Train Epoch: 4 [8800/60000 (15%)] Loss: 0.31\n",
      "Train Epoch: 4 [9200/60000 (15%)] Loss: 0.17\n",
      "Train Epoch: 4 [9600/60000 (16%)] Loss: 0.32\n",
      "Train Epoch: 4 [10000/60000 (17%)] Loss: 0.16\n",
      "Train Epoch: 4 [10400/60000 (17%)] Loss: 0.20\n",
      "Train Epoch: 4 [10800/60000 (18%)] Loss: 0.23\n",
      "Train Epoch: 4 [11200/60000 (19%)] Loss: 0.35\n",
      "Train Epoch: 4 [11600/60000 (19%)] Loss: 0.34\n",
      "Train Epoch: 4 [12000/60000 (20%)] Loss: 0.23\n",
      "Train Epoch: 4 [12400/60000 (21%)] Loss: 0.36\n",
      "Train Epoch: 4 [12800/60000 (21%)] Loss: 0.15\n",
      "Train Epoch: 4 [13200/60000 (22%)] Loss: 0.25\n",
      "Train Epoch: 4 [13600/60000 (23%)] Loss: 0.22\n",
      "Train Epoch: 4 [14000/60000 (23%)] Loss: 0.26\n",
      "Train Epoch: 4 [14400/60000 (24%)] Loss: 0.23\n",
      "Train Epoch: 4 [14800/60000 (25%)] Loss: 0.21\n",
      "Train Epoch: 4 [15200/60000 (25%)] Loss: 0.28\n",
      "Train Epoch: 4 [15600/60000 (26%)] Loss: 0.31\n",
      "Train Epoch: 4 [16000/60000 (27%)] Loss: 0.22\n",
      "Train Epoch: 4 [16400/60000 (27%)] Loss: 0.19\n",
      "Train Epoch: 4 [16800/60000 (28%)] Loss: 0.26\n",
      "Train Epoch: 4 [17200/60000 (29%)] Loss: 0.19\n",
      "Train Epoch: 4 [17600/60000 (29%)] Loss: 0.24\n",
      "Train Epoch: 4 [18000/60000 (30%)] Loss: 0.29\n",
      "Train Epoch: 4 [18400/60000 (31%)] Loss: 0.25\n",
      "Train Epoch: 4 [18800/60000 (31%)] Loss: 0.22\n",
      "Train Epoch: 4 [19200/60000 (32%)] Loss: 0.31\n",
      "Train Epoch: 4 [19600/60000 (33%)] Loss: 0.29\n",
      "Train Epoch: 4 [20000/60000 (33%)] Loss: 0.27\n",
      "Train Epoch: 4 [20400/60000 (34%)] Loss: 0.23\n",
      "Train Epoch: 4 [20800/60000 (35%)] Loss: 0.27\n",
      "Train Epoch: 4 [21200/60000 (35%)] Loss: 0.14\n",
      "Train Epoch: 4 [21600/60000 (36%)] Loss: 0.21\n",
      "Train Epoch: 4 [22000/60000 (37%)] Loss: 0.31\n",
      "Train Epoch: 4 [22400/60000 (37%)] Loss: 0.39\n",
      "Train Epoch: 4 [22800/60000 (38%)] Loss: 0.22\n",
      "Train Epoch: 4 [23200/60000 (39%)] Loss: 0.16\n",
      "Train Epoch: 4 [23600/60000 (39%)] Loss: 0.25\n",
      "Train Epoch: 4 [24000/60000 (40%)] Loss: 0.17\n",
      "Train Epoch: 4 [24400/60000 (41%)] Loss: 0.25\n",
      "Train Epoch: 4 [24800/60000 (41%)] Loss: 0.18\n",
      "Train Epoch: 4 [25200/60000 (42%)] Loss: 0.59\n",
      "Train Epoch: 4 [25600/60000 (43%)] Loss: 0.33\n",
      "Train Epoch: 4 [26000/60000 (43%)] Loss: 0.40\n",
      "Train Epoch: 4 [26400/60000 (44%)] Loss: 0.24\n",
      "Train Epoch: 4 [26800/60000 (45%)] Loss: 0.23\n",
      "Train Epoch: 4 [27200/60000 (45%)] Loss: 0.24\n",
      "Train Epoch: 4 [27600/60000 (46%)] Loss: 0.26\n",
      "Train Epoch: 4 [28000/60000 (47%)] Loss: 0.27\n",
      "Train Epoch: 4 [28400/60000 (47%)] Loss: 0.16\n",
      "Train Epoch: 4 [28800/60000 (48%)] Loss: 0.30\n",
      "Train Epoch: 4 [29200/60000 (49%)] Loss: 0.26\n",
      "Train Epoch: 4 [29600/60000 (49%)] Loss: 0.32\n",
      "Train Epoch: 4 [30000/60000 (50%)] Loss: 0.15\n",
      "Train Epoch: 4 [30400/60000 (51%)] Loss: 0.20\n",
      "Train Epoch: 4 [30800/60000 (51%)] Loss: 0.20\n",
      "Train Epoch: 4 [31200/60000 (52%)] Loss: 0.23\n",
      "Train Epoch: 4 [31600/60000 (53%)] Loss: 0.22\n",
      "Train Epoch: 4 [32000/60000 (53%)] Loss: 0.28\n",
      "Train Epoch: 4 [32400/60000 (54%)] Loss: 0.23\n",
      "Train Epoch: 4 [32800/60000 (55%)] Loss: 0.36\n",
      "Train Epoch: 4 [33200/60000 (55%)] Loss: 0.39\n",
      "Train Epoch: 4 [33600/60000 (56%)] Loss: 0.41\n",
      "Train Epoch: 4 [34000/60000 (57%)] Loss: 0.27\n",
      "Train Epoch: 4 [34400/60000 (57%)] Loss: 0.18\n",
      "Train Epoch: 4 [34800/60000 (58%)] Loss: 0.20\n",
      "Train Epoch: 4 [35200/60000 (59%)] Loss: 0.32\n",
      "Train Epoch: 4 [35600/60000 (59%)] Loss: 0.30\n",
      "Train Epoch: 4 [36000/60000 (60%)] Loss: 0.24\n",
      "Train Epoch: 4 [36400/60000 (61%)] Loss: 0.21\n",
      "Train Epoch: 4 [36800/60000 (61%)] Loss: 0.18\n",
      "Train Epoch: 4 [37200/60000 (62%)] Loss: 0.33\n",
      "Train Epoch: 4 [37600/60000 (63%)] Loss: 0.16\n",
      "Train Epoch: 4 [38000/60000 (63%)] Loss: 0.31\n",
      "Train Epoch: 4 [38400/60000 (64%)] Loss: 0.23\n",
      "Train Epoch: 4 [38800/60000 (65%)] Loss: 0.29\n",
      "Train Epoch: 4 [39200/60000 (65%)] Loss: 0.17\n",
      "Train Epoch: 4 [39600/60000 (66%)] Loss: 0.40\n",
      "Train Epoch: 4 [40000/60000 (67%)] Loss: 0.24\n",
      "Train Epoch: 4 [40400/60000 (67%)] Loss: 0.37\n",
      "Train Epoch: 4 [40800/60000 (68%)] Loss: 0.40\n",
      "Train Epoch: 4 [41200/60000 (69%)] Loss: 0.32\n",
      "Train Epoch: 4 [41600/60000 (69%)] Loss: 0.70\n",
      "Train Epoch: 4 [42000/60000 (70%)] Loss: 0.29\n",
      "Train Epoch: 4 [42400/60000 (71%)] Loss: 0.41\n",
      "Train Epoch: 4 [42800/60000 (71%)] Loss: 0.26\n",
      "Train Epoch: 4 [43200/60000 (72%)] Loss: 0.31\n",
      "Train Epoch: 4 [43600/60000 (73%)] Loss: 0.46\n",
      "Train Epoch: 4 [44000/60000 (73%)] Loss: 0.26\n",
      "Train Epoch: 4 [44400/60000 (74%)] Loss: 0.23\n",
      "Train Epoch: 4 [44800/60000 (75%)] Loss: 0.23\n",
      "Train Epoch: 4 [45200/60000 (75%)] Loss: 0.28\n",
      "Train Epoch: 4 [45600/60000 (76%)] Loss: 0.49\n",
      "Train Epoch: 4 [46000/60000 (77%)] Loss: 0.25\n",
      "Train Epoch: 4 [46400/60000 (77%)] Loss: 0.36\n",
      "Train Epoch: 4 [46800/60000 (78%)] Loss: 0.28\n",
      "Train Epoch: 4 [47200/60000 (79%)] Loss: 0.28\n",
      "Train Epoch: 4 [47600/60000 (79%)] Loss: 0.18\n",
      "Train Epoch: 4 [48000/60000 (80%)] Loss: 0.29\n",
      "Train Epoch: 4 [48400/60000 (81%)] Loss: 0.27\n",
      "Train Epoch: 4 [48800/60000 (81%)] Loss: 0.23\n",
      "Train Epoch: 4 [49200/60000 (82%)] Loss: 0.45\n",
      "Train Epoch: 4 [49600/60000 (83%)] Loss: 0.40\n",
      "Train Epoch: 4 [50000/60000 (83%)] Loss: 0.44\n",
      "Train Epoch: 4 [50400/60000 (84%)] Loss: 0.22\n",
      "Train Epoch: 4 [50800/60000 (85%)] Loss: 0.63\n",
      "Train Epoch: 4 [51200/60000 (85%)] Loss: 0.39\n",
      "Train Epoch: 4 [51600/60000 (86%)] Loss: 0.31\n",
      "Train Epoch: 4 [52000/60000 (87%)] Loss: 0.29\n",
      "Train Epoch: 4 [52400/60000 (87%)] Loss: 0.24\n",
      "Train Epoch: 4 [52800/60000 (88%)] Loss: 0.22\n",
      "Train Epoch: 4 [53200/60000 (89%)] Loss: 0.15\n",
      "Train Epoch: 4 [53600/60000 (89%)] Loss: 0.26\n",
      "Train Epoch: 4 [54000/60000 (90%)] Loss: 0.25\n",
      "Train Epoch: 4 [54400/60000 (91%)] Loss: 0.15\n",
      "Train Epoch: 4 [54800/60000 (91%)] Loss: 0.28\n",
      "Train Epoch: 4 [55200/60000 (92%)] Loss: 0.24\n",
      "Train Epoch: 4 [55600/60000 (93%)] Loss: 0.17\n",
      "Train Epoch: 4 [56000/60000 (93%)] Loss: 0.19\n",
      "Train Epoch: 4 [56400/60000 (94%)] Loss: 0.23\n",
      "Train Epoch: 4 [56800/60000 (95%)] Loss: 0.44\n",
      "Train Epoch: 4 [57200/60000 (95%)] Loss: 0.32\n",
      "Train Epoch: 4 [57600/60000 (96%)] Loss: 0.12\n",
      "Train Epoch: 4 [58000/60000 (97%)] Loss: 0.48\n",
      "Train Epoch: 4 [58400/60000 (97%)] Loss: 0.31\n",
      "Train Epoch: 4 [58800/60000 (98%)] Loss: 0.28\n",
      "Train Epoch: 4 [59200/60000 (99%)] Loss: 0.19\n",
      "Train Epoch: 4 [59600/60000 (99%)] Loss: 0.30\n",
      "Train Epoch: 4 [60000/60000 (100%)] Loss: 0.38\n",
      "Test set: Accuracy: 9396.0/10000 (94%), Average Loss: 0.00\n",
      "Train Epoch: 1 [400/60000 (1%)] Loss: 2.05\n",
      "Train Epoch: 1 [800/60000 (1%)] Loss: 1.11\n",
      "Train Epoch: 1 [1200/60000 (2%)] Loss: 0.85\n",
      "Train Epoch: 1 [1600/60000 (3%)] Loss: 0.69\n",
      "Train Epoch: 1 [2000/60000 (3%)] Loss: 0.66\n",
      "Train Epoch: 1 [2400/60000 (4%)] Loss: 0.59\n",
      "Train Epoch: 1 [2800/60000 (5%)] Loss: 0.49\n",
      "Train Epoch: 1 [3200/60000 (5%)] Loss: 0.51\n",
      "Train Epoch: 1 [3600/60000 (6%)] Loss: 0.67\n",
      "Train Epoch: 1 [4000/60000 (7%)] Loss: 0.45\n",
      "Train Epoch: 1 [4400/60000 (7%)] Loss: 0.34\n",
      "Train Epoch: 1 [4800/60000 (8%)] Loss: 0.50\n",
      "Train Epoch: 1 [5200/60000 (9%)] Loss: 0.41\n",
      "Train Epoch: 1 [5600/60000 (9%)] Loss: 0.32\n",
      "Train Epoch: 1 [6000/60000 (10%)] Loss: 0.28\n",
      "Train Epoch: 1 [6400/60000 (11%)] Loss: 0.46\n",
      "Train Epoch: 1 [6800/60000 (11%)] Loss: 0.44\n",
      "Train Epoch: 1 [7200/60000 (12%)] Loss: 0.37\n",
      "Train Epoch: 1 [7600/60000 (13%)] Loss: 0.42\n",
      "Train Epoch: 1 [8000/60000 (13%)] Loss: 0.32\n",
      "Train Epoch: 1 [8400/60000 (14%)] Loss: 0.33\n",
      "Train Epoch: 1 [8800/60000 (15%)] Loss: 0.29\n",
      "Train Epoch: 1 [9200/60000 (15%)] Loss: 0.31\n",
      "Train Epoch: 1 [9600/60000 (16%)] Loss: 0.33\n",
      "Train Epoch: 1 [10000/60000 (17%)] Loss: 0.38\n",
      "Train Epoch: 1 [10400/60000 (17%)] Loss: 0.38\n",
      "Train Epoch: 1 [10800/60000 (18%)] Loss: 0.42\n",
      "Train Epoch: 1 [11200/60000 (19%)] Loss: 0.25\n",
      "Train Epoch: 1 [11600/60000 (19%)] Loss: 0.29\n",
      "Train Epoch: 1 [12000/60000 (20%)] Loss: 0.27\n",
      "Train Epoch: 1 [12400/60000 (21%)] Loss: 0.31\n",
      "Train Epoch: 1 [12800/60000 (21%)] Loss: 0.42\n",
      "Train Epoch: 1 [13200/60000 (22%)] Loss: 0.39\n",
      "Train Epoch: 1 [13600/60000 (23%)] Loss: 0.34\n",
      "Train Epoch: 1 [14000/60000 (23%)] Loss: 0.34\n",
      "Train Epoch: 1 [14400/60000 (24%)] Loss: 0.62\n",
      "Train Epoch: 1 [14800/60000 (25%)] Loss: 0.36\n",
      "Train Epoch: 1 [15200/60000 (25%)] Loss: 0.28\n",
      "Train Epoch: 1 [15600/60000 (26%)] Loss: 0.21\n",
      "Train Epoch: 1 [16000/60000 (27%)] Loss: 0.36\n",
      "Train Epoch: 1 [16400/60000 (27%)] Loss: 0.42\n",
      "Train Epoch: 1 [16800/60000 (28%)] Loss: 0.37\n",
      "Train Epoch: 1 [17200/60000 (29%)] Loss: 0.25\n",
      "Train Epoch: 1 [17600/60000 (29%)] Loss: 0.38\n",
      "Train Epoch: 1 [18000/60000 (30%)] Loss: 0.23\n",
      "Train Epoch: 1 [18400/60000 (31%)] Loss: 0.30\n",
      "Train Epoch: 1 [18800/60000 (31%)] Loss: 0.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [19200/60000 (32%)] Loss: 0.35\n",
      "Train Epoch: 1 [19600/60000 (33%)] Loss: 0.27\n",
      "Train Epoch: 1 [20000/60000 (33%)] Loss: 0.35\n",
      "Train Epoch: 1 [20400/60000 (34%)] Loss: 0.24\n",
      "Train Epoch: 1 [20800/60000 (35%)] Loss: 0.29\n",
      "Train Epoch: 1 [21200/60000 (35%)] Loss: 0.21\n",
      "Train Epoch: 1 [21600/60000 (36%)] Loss: 0.33\n",
      "Train Epoch: 1 [22000/60000 (37%)] Loss: 0.46\n",
      "Train Epoch: 1 [22400/60000 (37%)] Loss: 0.46\n",
      "Train Epoch: 1 [22800/60000 (38%)] Loss: 0.31\n",
      "Train Epoch: 1 [23200/60000 (39%)] Loss: 0.26\n",
      "Train Epoch: 1 [23600/60000 (39%)] Loss: 0.28\n",
      "Train Epoch: 1 [24000/60000 (40%)] Loss: 0.30\n",
      "Train Epoch: 1 [24400/60000 (41%)] Loss: 0.28\n",
      "Train Epoch: 1 [24800/60000 (41%)] Loss: 0.31\n",
      "Train Epoch: 1 [25200/60000 (42%)] Loss: 0.26\n",
      "Train Epoch: 1 [25600/60000 (43%)] Loss: 0.53\n",
      "Train Epoch: 1 [26000/60000 (43%)] Loss: 0.36\n",
      "Train Epoch: 1 [26400/60000 (44%)] Loss: 0.25\n",
      "Train Epoch: 1 [26800/60000 (45%)] Loss: 0.33\n",
      "Train Epoch: 1 [27200/60000 (45%)] Loss: 0.22\n",
      "Train Epoch: 1 [27600/60000 (46%)] Loss: 0.27\n",
      "Train Epoch: 1 [28000/60000 (47%)] Loss: 0.32\n",
      "Train Epoch: 1 [28400/60000 (47%)] Loss: 0.29\n",
      "Train Epoch: 1 [28800/60000 (48%)] Loss: 0.37\n",
      "Train Epoch: 1 [29200/60000 (49%)] Loss: 0.42\n",
      "Train Epoch: 1 [29600/60000 (49%)] Loss: 0.24\n",
      "Train Epoch: 1 [30000/60000 (50%)] Loss: 0.26\n",
      "Train Epoch: 1 [30400/60000 (51%)] Loss: 0.28\n",
      "Train Epoch: 1 [30800/60000 (51%)] Loss: 0.36\n",
      "Train Epoch: 1 [31200/60000 (52%)] Loss: 0.39\n",
      "Train Epoch: 1 [31600/60000 (53%)] Loss: 0.39\n",
      "Train Epoch: 1 [32000/60000 (53%)] Loss: 0.42\n",
      "Train Epoch: 1 [32400/60000 (54%)] Loss: 0.26\n",
      "Train Epoch: 1 [32800/60000 (55%)] Loss: 0.23\n",
      "Train Epoch: 1 [33200/60000 (55%)] Loss: 0.22\n",
      "Train Epoch: 1 [33600/60000 (56%)] Loss: 0.31\n",
      "Train Epoch: 1 [34000/60000 (57%)] Loss: 0.33\n",
      "Train Epoch: 1 [34400/60000 (57%)] Loss: 0.26\n",
      "Train Epoch: 1 [34800/60000 (58%)] Loss: 0.42\n",
      "Train Epoch: 1 [35200/60000 (59%)] Loss: 0.24\n",
      "Train Epoch: 1 [35600/60000 (59%)] Loss: 0.21\n",
      "Train Epoch: 1 [36000/60000 (60%)] Loss: 0.26\n",
      "Train Epoch: 1 [36400/60000 (61%)] Loss: 0.29\n",
      "Train Epoch: 1 [36800/60000 (61%)] Loss: 0.53\n",
      "Train Epoch: 1 [37200/60000 (62%)] Loss: 0.29\n",
      "Train Epoch: 1 [37600/60000 (63%)] Loss: 0.32\n",
      "Train Epoch: 1 [38000/60000 (63%)] Loss: 0.34\n",
      "Train Epoch: 1 [38400/60000 (64%)] Loss: 0.53\n",
      "Train Epoch: 1 [38800/60000 (65%)] Loss: 0.41\n",
      "Train Epoch: 1 [39200/60000 (65%)] Loss: 0.43\n",
      "Train Epoch: 1 [39600/60000 (66%)] Loss: 0.50\n",
      "Train Epoch: 1 [40000/60000 (67%)] Loss: 0.22\n",
      "Train Epoch: 1 [40400/60000 (67%)] Loss: 0.34\n",
      "Train Epoch: 1 [40800/60000 (68%)] Loss: 0.38\n",
      "Train Epoch: 1 [41200/60000 (69%)] Loss: 0.35\n",
      "Train Epoch: 1 [41600/60000 (69%)] Loss: 0.35\n",
      "Train Epoch: 1 [42000/60000 (70%)] Loss: 0.35\n",
      "Train Epoch: 1 [42400/60000 (71%)] Loss: 0.28\n",
      "Train Epoch: 1 [42800/60000 (71%)] Loss: 0.35\n",
      "Train Epoch: 1 [43200/60000 (72%)] Loss: 0.31\n",
      "Train Epoch: 1 [43600/60000 (73%)] Loss: 0.27\n",
      "Train Epoch: 1 [44000/60000 (73%)] Loss: 0.32\n",
      "Train Epoch: 1 [44400/60000 (74%)] Loss: 0.25\n",
      "Train Epoch: 1 [44800/60000 (75%)] Loss: 0.27\n",
      "Train Epoch: 1 [45200/60000 (75%)] Loss: 0.32\n",
      "Train Epoch: 1 [45600/60000 (76%)] Loss: 0.26\n",
      "Train Epoch: 1 [46000/60000 (77%)] Loss: 0.23\n",
      "Train Epoch: 1 [46400/60000 (77%)] Loss: 0.13\n",
      "Train Epoch: 1 [46800/60000 (78%)] Loss: 0.36\n",
      "Train Epoch: 1 [47200/60000 (79%)] Loss: 0.19\n",
      "Train Epoch: 1 [47600/60000 (79%)] Loss: 0.44\n",
      "Train Epoch: 1 [48000/60000 (80%)] Loss: 0.45\n",
      "Train Epoch: 1 [48400/60000 (81%)] Loss: 0.47\n",
      "Train Epoch: 1 [48800/60000 (81%)] Loss: 0.56\n",
      "Train Epoch: 1 [49200/60000 (82%)] Loss: 0.21\n",
      "Train Epoch: 1 [49600/60000 (83%)] Loss: 0.25\n",
      "Train Epoch: 1 [50000/60000 (83%)] Loss: 0.45\n",
      "Train Epoch: 1 [50400/60000 (84%)] Loss: 0.19\n",
      "Train Epoch: 1 [50800/60000 (85%)] Loss: 0.17\n",
      "Train Epoch: 1 [51200/60000 (85%)] Loss: 0.29\n",
      "Train Epoch: 1 [51600/60000 (86%)] Loss: 0.25\n",
      "Train Epoch: 1 [52000/60000 (87%)] Loss: 0.35\n",
      "Train Epoch: 1 [52400/60000 (87%)] Loss: 0.16\n",
      "Train Epoch: 1 [52800/60000 (88%)] Loss: 0.33\n",
      "Train Epoch: 1 [53200/60000 (89%)] Loss: 0.37\n",
      "Train Epoch: 1 [53600/60000 (89%)] Loss: 0.18\n",
      "Train Epoch: 1 [54000/60000 (90%)] Loss: 0.24\n",
      "Train Epoch: 1 [54400/60000 (91%)] Loss: 0.53\n",
      "Train Epoch: 1 [54800/60000 (91%)] Loss: 0.23\n",
      "Train Epoch: 1 [55200/60000 (92%)] Loss: 0.28\n",
      "Train Epoch: 1 [55600/60000 (93%)] Loss: 0.20\n",
      "Train Epoch: 1 [56000/60000 (93%)] Loss: 0.26\n",
      "Train Epoch: 1 [56400/60000 (94%)] Loss: 0.23\n",
      "Train Epoch: 1 [56800/60000 (95%)] Loss: 0.20\n",
      "Train Epoch: 1 [57200/60000 (95%)] Loss: 0.19\n",
      "Train Epoch: 1 [57600/60000 (96%)] Loss: 0.27\n",
      "Train Epoch: 1 [58000/60000 (97%)] Loss: 0.22\n",
      "Train Epoch: 1 [58400/60000 (97%)] Loss: 0.30\n",
      "Train Epoch: 1 [58800/60000 (98%)] Loss: 0.56\n",
      "Train Epoch: 1 [59200/60000 (99%)] Loss: 0.31\n",
      "Train Epoch: 1 [59600/60000 (99%)] Loss: 0.24\n",
      "Train Epoch: 1 [60000/60000 (100%)] Loss: 0.29\n",
      "Test set: Accuracy: 9333.0/10000 (93%), Average Loss: 0.00\n",
      "Train Epoch: 2 [400/60000 (1%)] Loss: 0.37\n",
      "Train Epoch: 2 [800/60000 (1%)] Loss: 0.20\n",
      "Train Epoch: 2 [1200/60000 (2%)] Loss: 0.29\n",
      "Train Epoch: 2 [1600/60000 (3%)] Loss: 0.27\n",
      "Train Epoch: 2 [2000/60000 (3%)] Loss: 0.32\n",
      "Train Epoch: 2 [2400/60000 (4%)] Loss: 0.27\n",
      "Train Epoch: 2 [2800/60000 (5%)] Loss: 0.23\n",
      "Train Epoch: 2 [3200/60000 (5%)] Loss: 0.29\n",
      "Train Epoch: 2 [3600/60000 (6%)] Loss: 0.19\n",
      "Train Epoch: 2 [4000/60000 (7%)] Loss: 0.33\n",
      "Train Epoch: 2 [4400/60000 (7%)] Loss: 0.22\n",
      "Train Epoch: 2 [4800/60000 (8%)] Loss: 0.26\n",
      "Train Epoch: 2 [5200/60000 (9%)] Loss: 0.15\n",
      "Train Epoch: 2 [5600/60000 (9%)] Loss: 0.42\n",
      "Train Epoch: 2 [6000/60000 (10%)] Loss: 0.30\n",
      "Train Epoch: 2 [6400/60000 (11%)] Loss: 0.22\n",
      "Train Epoch: 2 [6800/60000 (11%)] Loss: 0.23\n",
      "Train Epoch: 2 [7200/60000 (12%)] Loss: 0.34\n",
      "Train Epoch: 2 [7600/60000 (13%)] Loss: 0.16\n",
      "Train Epoch: 2 [8000/60000 (13%)] Loss: 0.32\n",
      "Train Epoch: 2 [8400/60000 (14%)] Loss: 0.27\n",
      "Train Epoch: 2 [8800/60000 (15%)] Loss: 0.24\n",
      "Train Epoch: 2 [9200/60000 (15%)] Loss: 0.23\n",
      "Train Epoch: 2 [9600/60000 (16%)] Loss: 0.28\n",
      "Train Epoch: 2 [10000/60000 (17%)] Loss: 0.16\n",
      "Train Epoch: 2 [10400/60000 (17%)] Loss: 0.16\n",
      "Train Epoch: 2 [10800/60000 (18%)] Loss: 0.41\n",
      "Train Epoch: 2 [11200/60000 (19%)] Loss: 0.30\n",
      "Train Epoch: 2 [11600/60000 (19%)] Loss: 0.61\n",
      "Train Epoch: 2 [12000/60000 (20%)] Loss: 0.63\n",
      "Train Epoch: 2 [12400/60000 (21%)] Loss: 0.39\n",
      "Train Epoch: 2 [12800/60000 (21%)] Loss: 0.47\n",
      "Train Epoch: 2 [13200/60000 (22%)] Loss: 0.51\n",
      "Train Epoch: 2 [13600/60000 (23%)] Loss: 1.24\n",
      "Train Epoch: 2 [14000/60000 (23%)] Loss: 0.43\n",
      "Train Epoch: 2 [14400/60000 (24%)] Loss: 0.31\n",
      "Train Epoch: 2 [14800/60000 (25%)] Loss: 0.23\n",
      "Train Epoch: 2 [15200/60000 (25%)] Loss: 0.23\n",
      "Train Epoch: 2 [15600/60000 (26%)] Loss: 0.33\n",
      "Train Epoch: 2 [16000/60000 (27%)] Loss: 0.27\n",
      "Train Epoch: 2 [16400/60000 (27%)] Loss: 0.27\n",
      "Train Epoch: 2 [16800/60000 (28%)] Loss: 0.14\n",
      "Train Epoch: 2 [17200/60000 (29%)] Loss: 0.31\n",
      "Train Epoch: 2 [17600/60000 (29%)] Loss: 0.21\n",
      "Train Epoch: 2 [18000/60000 (30%)] Loss: 0.26\n",
      "Train Epoch: 2 [18400/60000 (31%)] Loss: 0.22\n",
      "Train Epoch: 2 [18800/60000 (31%)] Loss: 0.29\n",
      "Train Epoch: 2 [19200/60000 (32%)] Loss: 0.42\n",
      "Train Epoch: 2 [19600/60000 (33%)] Loss: 0.27\n",
      "Train Epoch: 2 [20000/60000 (33%)] Loss: 0.27\n",
      "Train Epoch: 2 [20400/60000 (34%)] Loss: 0.22\n",
      "Train Epoch: 2 [20800/60000 (35%)] Loss: 0.33\n",
      "Train Epoch: 2 [21200/60000 (35%)] Loss: 0.31\n",
      "Train Epoch: 2 [21600/60000 (36%)] Loss: 0.20\n",
      "Train Epoch: 2 [22000/60000 (37%)] Loss: 0.21\n",
      "Train Epoch: 2 [22400/60000 (37%)] Loss: 0.19\n",
      "Train Epoch: 2 [22800/60000 (38%)] Loss: 0.24\n",
      "Train Epoch: 2 [23200/60000 (39%)] Loss: 0.25\n",
      "Train Epoch: 2 [23600/60000 (39%)] Loss: 0.25\n",
      "Train Epoch: 2 [24000/60000 (40%)] Loss: 0.20\n",
      "Train Epoch: 2 [24400/60000 (41%)] Loss: 0.37\n",
      "Train Epoch: 2 [24800/60000 (41%)] Loss: 0.50\n",
      "Train Epoch: 2 [25200/60000 (42%)] Loss: 0.25\n",
      "Train Epoch: 2 [25600/60000 (43%)] Loss: 0.27\n",
      "Train Epoch: 2 [26000/60000 (43%)] Loss: 0.22\n",
      "Train Epoch: 2 [26400/60000 (44%)] Loss: 0.21\n",
      "Train Epoch: 2 [26800/60000 (45%)] Loss: 0.26\n",
      "Train Epoch: 2 [27200/60000 (45%)] Loss: 0.25\n",
      "Train Epoch: 2 [27600/60000 (46%)] Loss: 0.30\n",
      "Train Epoch: 2 [28000/60000 (47%)] Loss: 0.14\n",
      "Train Epoch: 2 [28400/60000 (47%)] Loss: 0.28\n",
      "Train Epoch: 2 [28800/60000 (48%)] Loss: 0.27\n",
      "Train Epoch: 2 [29200/60000 (49%)] Loss: 0.29\n",
      "Train Epoch: 2 [29600/60000 (49%)] Loss: 0.43\n",
      "Train Epoch: 2 [30000/60000 (50%)] Loss: 0.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [30400/60000 (51%)] Loss: 0.26\n",
      "Train Epoch: 2 [30800/60000 (51%)] Loss: 0.28\n",
      "Train Epoch: 2 [31200/60000 (52%)] Loss: 0.16\n",
      "Train Epoch: 2 [31600/60000 (53%)] Loss: 0.15\n",
      "Train Epoch: 2 [32000/60000 (53%)] Loss: 0.19\n",
      "Train Epoch: 2 [32400/60000 (54%)] Loss: 0.42\n",
      "Train Epoch: 2 [32800/60000 (55%)] Loss: 0.27\n",
      "Train Epoch: 2 [33200/60000 (55%)] Loss: 0.23\n",
      "Train Epoch: 2 [33600/60000 (56%)] Loss: 0.18\n",
      "Train Epoch: 2 [34000/60000 (57%)] Loss: 0.21\n",
      "Train Epoch: 2 [34400/60000 (57%)] Loss: 0.25\n",
      "Train Epoch: 2 [34800/60000 (58%)] Loss: 0.17\n",
      "Train Epoch: 2 [35200/60000 (59%)] Loss: 0.33\n",
      "Train Epoch: 2 [35600/60000 (59%)] Loss: 0.31\n",
      "Train Epoch: 2 [36000/60000 (60%)] Loss: 0.47\n",
      "Train Epoch: 2 [36400/60000 (61%)] Loss: 0.22\n",
      "Train Epoch: 2 [36800/60000 (61%)] Loss: 0.17\n",
      "Train Epoch: 2 [37200/60000 (62%)] Loss: 0.23\n",
      "Train Epoch: 2 [37600/60000 (63%)] Loss: 0.20\n",
      "Train Epoch: 2 [38000/60000 (63%)] Loss: 0.18\n",
      "Train Epoch: 2 [38400/60000 (64%)] Loss: 0.30\n",
      "Train Epoch: 2 [38800/60000 (65%)] Loss: 0.14\n",
      "Train Epoch: 2 [39200/60000 (65%)] Loss: 0.19\n",
      "Train Epoch: 2 [39600/60000 (66%)] Loss: 0.23\n",
      "Train Epoch: 2 [40000/60000 (67%)] Loss: 0.28\n",
      "Train Epoch: 2 [40400/60000 (67%)] Loss: 0.24\n",
      "Train Epoch: 2 [40800/60000 (68%)] Loss: 0.19\n",
      "Train Epoch: 2 [41200/60000 (69%)] Loss: 0.44\n",
      "Train Epoch: 2 [41600/60000 (69%)] Loss: 0.27\n",
      "Train Epoch: 2 [42000/60000 (70%)] Loss: 0.30\n",
      "Train Epoch: 2 [42400/60000 (71%)] Loss: 0.40\n",
      "Train Epoch: 2 [42800/60000 (71%)] Loss: 0.42\n",
      "Train Epoch: 2 [43200/60000 (72%)] Loss: 0.16\n",
      "Train Epoch: 2 [43600/60000 (73%)] Loss: 0.16\n",
      "Train Epoch: 2 [44000/60000 (73%)] Loss: 0.42\n",
      "Train Epoch: 2 [44400/60000 (74%)] Loss: 0.36\n",
      "Train Epoch: 2 [44800/60000 (75%)] Loss: 0.18\n",
      "Train Epoch: 2 [45200/60000 (75%)] Loss: 0.20\n",
      "Train Epoch: 2 [45600/60000 (76%)] Loss: 0.16\n",
      "Train Epoch: 2 [46000/60000 (77%)] Loss: 0.26\n",
      "Train Epoch: 2 [46400/60000 (77%)] Loss: 0.29\n",
      "Train Epoch: 2 [46800/60000 (78%)] Loss: 0.24\n",
      "Train Epoch: 2 [47200/60000 (79%)] Loss: 0.32\n",
      "Train Epoch: 2 [47600/60000 (79%)] Loss: 0.24\n",
      "Train Epoch: 2 [48000/60000 (80%)] Loss: 0.17\n",
      "Train Epoch: 2 [48400/60000 (81%)] Loss: 0.27\n",
      "Train Epoch: 2 [48800/60000 (81%)] Loss: 0.23\n",
      "Train Epoch: 2 [49200/60000 (82%)] Loss: 0.59\n",
      "Train Epoch: 2 [49600/60000 (83%)] Loss: 0.45\n",
      "Train Epoch: 2 [50000/60000 (83%)] Loss: 0.48\n",
      "Train Epoch: 2 [50400/60000 (84%)] Loss: 0.23\n",
      "Train Epoch: 2 [50800/60000 (85%)] Loss: 0.53\n",
      "Train Epoch: 2 [51200/60000 (85%)] Loss: 0.31\n",
      "Train Epoch: 2 [51600/60000 (86%)] Loss: 0.27\n",
      "Train Epoch: 2 [52000/60000 (87%)] Loss: 0.34\n",
      "Train Epoch: 2 [52400/60000 (87%)] Loss: 0.30\n",
      "Train Epoch: 2 [52800/60000 (88%)] Loss: 0.56\n",
      "Train Epoch: 2 [53200/60000 (89%)] Loss: 0.45\n",
      "Train Epoch: 2 [53600/60000 (89%)] Loss: 0.24\n",
      "Train Epoch: 2 [54000/60000 (90%)] Loss: 0.17\n",
      "Train Epoch: 2 [54400/60000 (91%)] Loss: 0.16\n",
      "Train Epoch: 2 [54800/60000 (91%)] Loss: 0.25\n",
      "Train Epoch: 2 [55200/60000 (92%)] Loss: 0.49\n",
      "Train Epoch: 2 [55600/60000 (93%)] Loss: 0.14\n",
      "Train Epoch: 2 [56000/60000 (93%)] Loss: 0.31\n",
      "Train Epoch: 2 [56400/60000 (94%)] Loss: 0.39\n",
      "Train Epoch: 2 [56800/60000 (95%)] Loss: 0.29\n",
      "Train Epoch: 2 [57200/60000 (95%)] Loss: 0.27\n",
      "Train Epoch: 2 [57600/60000 (96%)] Loss: 0.31\n",
      "Train Epoch: 2 [58000/60000 (97%)] Loss: 0.47\n",
      "Train Epoch: 2 [58400/60000 (97%)] Loss: 0.37\n",
      "Train Epoch: 2 [58800/60000 (98%)] Loss: 0.26\n",
      "Train Epoch: 2 [59200/60000 (99%)] Loss: 0.22\n",
      "Train Epoch: 2 [59600/60000 (99%)] Loss: 0.28\n",
      "Train Epoch: 2 [60000/60000 (100%)] Loss: 0.22\n",
      "Test set: Accuracy: 9544.0/10000 (95%), Average Loss: 0.00\n",
      "Train Epoch: 3 [400/60000 (1%)] Loss: 0.29\n",
      "Train Epoch: 3 [800/60000 (1%)] Loss: 0.25\n",
      "Train Epoch: 3 [1200/60000 (2%)] Loss: 0.35\n",
      "Train Epoch: 3 [1600/60000 (3%)] Loss: 0.16\n",
      "Train Epoch: 3 [2000/60000 (3%)] Loss: 0.09\n",
      "Train Epoch: 3 [2400/60000 (4%)] Loss: 0.24\n",
      "Train Epoch: 3 [2800/60000 (5%)] Loss: 0.26\n",
      "Train Epoch: 3 [3200/60000 (5%)] Loss: 0.22\n",
      "Train Epoch: 3 [3600/60000 (6%)] Loss: 0.22\n",
      "Train Epoch: 3 [4000/60000 (7%)] Loss: 0.17\n",
      "Train Epoch: 3 [4400/60000 (7%)] Loss: 0.30\n",
      "Train Epoch: 3 [4800/60000 (8%)] Loss: 1.43\n",
      "Train Epoch: 3 [5200/60000 (9%)] Loss: 0.54\n",
      "Train Epoch: 3 [5600/60000 (9%)] Loss: 0.25\n",
      "Train Epoch: 3 [6000/60000 (10%)] Loss: 0.28\n",
      "Train Epoch: 3 [6400/60000 (11%)] Loss: 0.27\n",
      "Train Epoch: 3 [6800/60000 (11%)] Loss: 0.34\n",
      "Train Epoch: 3 [7200/60000 (12%)] Loss: 0.17\n",
      "Train Epoch: 3 [7600/60000 (13%)] Loss: 0.17\n",
      "Train Epoch: 3 [8000/60000 (13%)] Loss: 0.31\n",
      "Train Epoch: 3 [8400/60000 (14%)] Loss: 0.48\n",
      "Train Epoch: 3 [8800/60000 (15%)] Loss: 0.29\n",
      "Train Epoch: 3 [9200/60000 (15%)] Loss: 0.25\n",
      "Train Epoch: 3 [9600/60000 (16%)] Loss: 0.27\n",
      "Train Epoch: 3 [10000/60000 (17%)] Loss: 0.14\n",
      "Train Epoch: 3 [10400/60000 (17%)] Loss: 0.28\n",
      "Train Epoch: 3 [10800/60000 (18%)] Loss: 0.21\n",
      "Train Epoch: 3 [11200/60000 (19%)] Loss: 0.18\n",
      "Train Epoch: 3 [11600/60000 (19%)] Loss: 0.18\n",
      "Train Epoch: 3 [12000/60000 (20%)] Loss: 0.49\n",
      "Train Epoch: 3 [12400/60000 (21%)] Loss: 0.61\n",
      "Train Epoch: 3 [12800/60000 (21%)] Loss: 0.50\n",
      "Train Epoch: 3 [13200/60000 (22%)] Loss: 0.72\n",
      "Train Epoch: 3 [13600/60000 (23%)] Loss: 0.27\n",
      "Train Epoch: 3 [14000/60000 (23%)] Loss: 0.24\n",
      "Train Epoch: 3 [14400/60000 (24%)] Loss: 0.21\n",
      "Train Epoch: 3 [14800/60000 (25%)] Loss: 0.10\n",
      "Train Epoch: 3 [15200/60000 (25%)] Loss: 0.19\n",
      "Train Epoch: 3 [15600/60000 (26%)] Loss: 0.22\n",
      "Train Epoch: 3 [16000/60000 (27%)] Loss: 0.32\n",
      "Train Epoch: 3 [16400/60000 (27%)] Loss: 0.24\n",
      "Train Epoch: 3 [16800/60000 (28%)] Loss: 0.22\n",
      "Train Epoch: 3 [17200/60000 (29%)] Loss: 0.20\n",
      "Train Epoch: 3 [17600/60000 (29%)] Loss: 0.17\n",
      "Train Epoch: 3 [18000/60000 (30%)] Loss: 0.35\n",
      "Train Epoch: 3 [18400/60000 (31%)] Loss: 0.22\n",
      "Train Epoch: 3 [18800/60000 (31%)] Loss: 0.26\n",
      "Train Epoch: 3 [19200/60000 (32%)] Loss: 0.33\n",
      "Train Epoch: 3 [19600/60000 (33%)] Loss: 0.24\n",
      "Train Epoch: 3 [20000/60000 (33%)] Loss: 0.25\n",
      "Train Epoch: 3 [20400/60000 (34%)] Loss: 0.21\n",
      "Train Epoch: 3 [20800/60000 (35%)] Loss: 0.54\n",
      "Train Epoch: 3 [21200/60000 (35%)] Loss: 0.17\n",
      "Train Epoch: 3 [21600/60000 (36%)] Loss: 0.14\n",
      "Train Epoch: 3 [22000/60000 (37%)] Loss: 0.28\n",
      "Train Epoch: 3 [22400/60000 (37%)] Loss: 0.16\n",
      "Train Epoch: 3 [22800/60000 (38%)] Loss: 0.30\n",
      "Train Epoch: 3 [23200/60000 (39%)] Loss: 0.25\n",
      "Train Epoch: 3 [23600/60000 (39%)] Loss: 0.33\n",
      "Train Epoch: 3 [24000/60000 (40%)] Loss: 0.23\n",
      "Train Epoch: 3 [24400/60000 (41%)] Loss: 0.18\n",
      "Train Epoch: 3 [24800/60000 (41%)] Loss: 0.36\n",
      "Train Epoch: 3 [25200/60000 (42%)] Loss: 0.22\n",
      "Train Epoch: 3 [25600/60000 (43%)] Loss: 0.15\n",
      "Train Epoch: 3 [26000/60000 (43%)] Loss: 0.34\n",
      "Train Epoch: 3 [26400/60000 (44%)] Loss: 0.42\n",
      "Train Epoch: 3 [26800/60000 (45%)] Loss: 0.42\n",
      "Train Epoch: 3 [27200/60000 (45%)] Loss: 0.33\n",
      "Train Epoch: 3 [27600/60000 (46%)] Loss: 0.30\n",
      "Train Epoch: 3 [28000/60000 (47%)] Loss: 0.51\n",
      "Train Epoch: 3 [28400/60000 (47%)] Loss: 0.39\n",
      "Train Epoch: 3 [28800/60000 (48%)] Loss: 0.31\n",
      "Train Epoch: 3 [29200/60000 (49%)] Loss: 0.23\n",
      "Train Epoch: 3 [29600/60000 (49%)] Loss: 0.27\n",
      "Train Epoch: 3 [30000/60000 (50%)] Loss: 0.20\n",
      "Train Epoch: 3 [30400/60000 (51%)] Loss: 0.16\n",
      "Train Epoch: 3 [30800/60000 (51%)] Loss: 0.33\n",
      "Train Epoch: 3 [31200/60000 (52%)] Loss: 0.20\n",
      "Train Epoch: 3 [31600/60000 (53%)] Loss: 0.23\n",
      "Train Epoch: 3 [32000/60000 (53%)] Loss: 0.25\n",
      "Train Epoch: 3 [32400/60000 (54%)] Loss: 0.15\n",
      "Train Epoch: 3 [32800/60000 (55%)] Loss: 0.32\n",
      "Train Epoch: 3 [33200/60000 (55%)] Loss: 0.18\n",
      "Train Epoch: 3 [33600/60000 (56%)] Loss: 0.34\n",
      "Train Epoch: 3 [34000/60000 (57%)] Loss: 0.33\n",
      "Train Epoch: 3 [34400/60000 (57%)] Loss: 0.25\n",
      "Train Epoch: 3 [34800/60000 (58%)] Loss: 0.16\n",
      "Train Epoch: 3 [35200/60000 (59%)] Loss: 0.35\n",
      "Train Epoch: 3 [35600/60000 (59%)] Loss: 0.23\n",
      "Train Epoch: 3 [36000/60000 (60%)] Loss: 0.36\n",
      "Train Epoch: 3 [36400/60000 (61%)] Loss: 0.21\n",
      "Train Epoch: 3 [36800/60000 (61%)] Loss: 0.36\n",
      "Train Epoch: 3 [37200/60000 (62%)] Loss: 0.33\n",
      "Train Epoch: 3 [37600/60000 (63%)] Loss: 0.21\n",
      "Train Epoch: 3 [38000/60000 (63%)] Loss: 0.37\n",
      "Train Epoch: 3 [38400/60000 (64%)] Loss: 0.22\n",
      "Train Epoch: 3 [38800/60000 (65%)] Loss: 0.29\n",
      "Train Epoch: 3 [39200/60000 (65%)] Loss: 0.25\n",
      "Train Epoch: 3 [39600/60000 (66%)] Loss: 0.14\n",
      "Train Epoch: 3 [40000/60000 (67%)] Loss: 0.37\n",
      "Train Epoch: 3 [40400/60000 (67%)] Loss: 0.63\n",
      "Train Epoch: 3 [40800/60000 (68%)] Loss: 0.28\n",
      "Train Epoch: 3 [41200/60000 (69%)] Loss: 0.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [41600/60000 (69%)] Loss: 0.38\n",
      "Train Epoch: 3 [42000/60000 (70%)] Loss: 0.26\n",
      "Train Epoch: 3 [42400/60000 (71%)] Loss: 0.36\n",
      "Train Epoch: 3 [42800/60000 (71%)] Loss: 0.31\n",
      "Train Epoch: 3 [43200/60000 (72%)] Loss: 0.17\n",
      "Train Epoch: 3 [43600/60000 (73%)] Loss: 0.16\n",
      "Train Epoch: 3 [44000/60000 (73%)] Loss: 0.16\n",
      "Train Epoch: 3 [44400/60000 (74%)] Loss: 0.25\n",
      "Train Epoch: 3 [44800/60000 (75%)] Loss: 0.21\n",
      "Train Epoch: 3 [45200/60000 (75%)] Loss: 0.63\n",
      "Train Epoch: 3 [45600/60000 (76%)] Loss: 0.27\n",
      "Train Epoch: 3 [46000/60000 (77%)] Loss: 0.20\n",
      "Train Epoch: 3 [46400/60000 (77%)] Loss: 0.19\n",
      "Train Epoch: 3 [46800/60000 (78%)] Loss: 0.22\n",
      "Train Epoch: 3 [47200/60000 (79%)] Loss: 0.28\n",
      "Train Epoch: 3 [47600/60000 (79%)] Loss: 0.51\n",
      "Train Epoch: 3 [48000/60000 (80%)] Loss: 0.51\n",
      "Train Epoch: 3 [48400/60000 (81%)] Loss: 0.33\n",
      "Train Epoch: 3 [48800/60000 (81%)] Loss: 0.19\n",
      "Train Epoch: 3 [49200/60000 (82%)] Loss: 0.24\n",
      "Train Epoch: 3 [49600/60000 (83%)] Loss: 0.19\n",
      "Train Epoch: 3 [50000/60000 (83%)] Loss: 0.28\n",
      "Train Epoch: 3 [50400/60000 (84%)] Loss: 0.32\n",
      "Train Epoch: 3 [50800/60000 (85%)] Loss: 0.15\n",
      "Train Epoch: 3 [51200/60000 (85%)] Loss: 0.27\n",
      "Train Epoch: 3 [51600/60000 (86%)] Loss: 0.27\n",
      "Train Epoch: 3 [52000/60000 (87%)] Loss: 0.13\n",
      "Train Epoch: 3 [52400/60000 (87%)] Loss: 0.15\n",
      "Train Epoch: 3 [52800/60000 (88%)] Loss: 0.37\n",
      "Train Epoch: 3 [53200/60000 (89%)] Loss: 0.20\n",
      "Train Epoch: 3 [53600/60000 (89%)] Loss: 0.18\n",
      "Train Epoch: 3 [54000/60000 (90%)] Loss: 0.23\n",
      "Train Epoch: 3 [54400/60000 (91%)] Loss: 0.71\n",
      "Train Epoch: 3 [54800/60000 (91%)] Loss: 0.25\n",
      "Train Epoch: 3 [55200/60000 (92%)] Loss: 0.22\n",
      "Train Epoch: 3 [55600/60000 (93%)] Loss: 0.21\n",
      "Train Epoch: 3 [56000/60000 (93%)] Loss: 0.21\n",
      "Train Epoch: 3 [56400/60000 (94%)] Loss: 0.15\n",
      "Train Epoch: 3 [56800/60000 (95%)] Loss: 0.25\n",
      "Train Epoch: 3 [57200/60000 (95%)] Loss: 0.39\n",
      "Train Epoch: 3 [57600/60000 (96%)] Loss: 0.16\n",
      "Train Epoch: 3 [58000/60000 (97%)] Loss: 0.27\n",
      "Train Epoch: 3 [58400/60000 (97%)] Loss: 0.36\n",
      "Train Epoch: 3 [58800/60000 (98%)] Loss: 0.44\n",
      "Train Epoch: 3 [59200/60000 (99%)] Loss: 0.66\n",
      "Train Epoch: 3 [59600/60000 (99%)] Loss: 0.43\n",
      "Train Epoch: 3 [60000/60000 (100%)] Loss: 0.23\n",
      "Test set: Accuracy: 9532.0/10000 (95%), Average Loss: 0.00\n",
      "Train Epoch: 4 [400/60000 (1%)] Loss: 0.20\n",
      "Train Epoch: 4 [800/60000 (1%)] Loss: 0.23\n",
      "Train Epoch: 4 [1200/60000 (2%)] Loss: 0.12\n",
      "Train Epoch: 4 [1600/60000 (3%)] Loss: 0.32\n",
      "Train Epoch: 4 [2000/60000 (3%)] Loss: 0.27\n",
      "Train Epoch: 4 [2400/60000 (4%)] Loss: 0.15\n",
      "Train Epoch: 4 [2800/60000 (5%)] Loss: 0.17\n",
      "Train Epoch: 4 [3200/60000 (5%)] Loss: 0.18\n",
      "Train Epoch: 4 [3600/60000 (6%)] Loss: 0.24\n",
      "Train Epoch: 4 [4000/60000 (7%)] Loss: 0.26\n",
      "Train Epoch: 4 [4400/60000 (7%)] Loss: 0.26\n",
      "Train Epoch: 4 [4800/60000 (8%)] Loss: 0.22\n",
      "Train Epoch: 4 [5200/60000 (9%)] Loss: 0.27\n",
      "Train Epoch: 4 [5600/60000 (9%)] Loss: 0.18\n",
      "Train Epoch: 4 [6000/60000 (10%)] Loss: 0.13\n",
      "Train Epoch: 4 [6400/60000 (11%)] Loss: 0.51\n",
      "Train Epoch: 4 [6800/60000 (11%)] Loss: 0.20\n",
      "Train Epoch: 4 [7200/60000 (12%)] Loss: 0.19\n",
      "Train Epoch: 4 [7600/60000 (13%)] Loss: 0.34\n",
      "Train Epoch: 4 [8000/60000 (13%)] Loss: 0.35\n",
      "Train Epoch: 4 [8400/60000 (14%)] Loss: 0.37\n",
      "Train Epoch: 4 [8800/60000 (15%)] Loss: 0.23\n",
      "Train Epoch: 4 [9200/60000 (15%)] Loss: 0.20\n",
      "Train Epoch: 4 [9600/60000 (16%)] Loss: 0.14\n",
      "Train Epoch: 4 [10000/60000 (17%)] Loss: 0.25\n",
      "Train Epoch: 4 [10400/60000 (17%)] Loss: 0.32\n",
      "Train Epoch: 4 [10800/60000 (18%)] Loss: 0.13\n",
      "Train Epoch: 4 [11200/60000 (19%)] Loss: 0.81\n",
      "Train Epoch: 4 [11600/60000 (19%)] Loss: 0.26\n",
      "Train Epoch: 4 [12000/60000 (20%)] Loss: 0.42\n",
      "Train Epoch: 4 [12400/60000 (21%)] Loss: 0.36\n",
      "Train Epoch: 4 [12800/60000 (21%)] Loss: 0.33\n",
      "Train Epoch: 4 [13200/60000 (22%)] Loss: 0.39\n",
      "Train Epoch: 4 [13600/60000 (23%)] Loss: 0.36\n",
      "Train Epoch: 4 [14000/60000 (23%)] Loss: 0.24\n",
      "Train Epoch: 4 [14400/60000 (24%)] Loss: 0.26\n",
      "Train Epoch: 4 [14800/60000 (25%)] Loss: 0.19\n",
      "Train Epoch: 4 [15200/60000 (25%)] Loss: 0.19\n",
      "Train Epoch: 4 [15600/60000 (26%)] Loss: 0.20\n",
      "Train Epoch: 4 [16000/60000 (27%)] Loss: 0.11\n",
      "Train Epoch: 4 [16400/60000 (27%)] Loss: 0.38\n",
      "Train Epoch: 4 [16800/60000 (28%)] Loss: 0.19\n",
      "Train Epoch: 4 [17200/60000 (29%)] Loss: 0.29\n",
      "Train Epoch: 4 [17600/60000 (29%)] Loss: 0.33\n",
      "Train Epoch: 4 [18000/60000 (30%)] Loss: 0.16\n",
      "Train Epoch: 4 [18400/60000 (31%)] Loss: 0.32\n",
      "Train Epoch: 4 [18800/60000 (31%)] Loss: 0.28\n",
      "Train Epoch: 4 [19200/60000 (32%)] Loss: 0.43\n",
      "Train Epoch: 4 [19600/60000 (33%)] Loss: 0.20\n",
      "Train Epoch: 4 [20000/60000 (33%)] Loss: 0.51\n",
      "Train Epoch: 4 [20400/60000 (34%)] Loss: 0.40\n",
      "Train Epoch: 4 [20800/60000 (35%)] Loss: 0.23\n",
      "Train Epoch: 4 [21200/60000 (35%)] Loss: 0.15\n",
      "Train Epoch: 4 [21600/60000 (36%)] Loss: 0.15\n",
      "Train Epoch: 4 [22000/60000 (37%)] Loss: 0.47\n",
      "Train Epoch: 4 [22400/60000 (37%)] Loss: 0.48\n",
      "Train Epoch: 4 [22800/60000 (38%)] Loss: 0.42\n",
      "Train Epoch: 4 [23200/60000 (39%)] Loss: 0.36\n",
      "Train Epoch: 4 [23600/60000 (39%)] Loss: 0.45\n",
      "Train Epoch: 4 [24000/60000 (40%)] Loss: 0.31\n",
      "Train Epoch: 4 [24400/60000 (41%)] Loss: 0.26\n",
      "Train Epoch: 4 [24800/60000 (41%)] Loss: 0.21\n",
      "Train Epoch: 4 [25200/60000 (42%)] Loss: 0.28\n",
      "Train Epoch: 4 [25600/60000 (43%)] Loss: 0.14\n",
      "Train Epoch: 4 [26000/60000 (43%)] Loss: 0.38\n",
      "Train Epoch: 4 [26400/60000 (44%)] Loss: 0.39\n",
      "Train Epoch: 4 [26800/60000 (45%)] Loss: 0.26\n",
      "Train Epoch: 4 [27200/60000 (45%)] Loss: 0.20\n",
      "Train Epoch: 4 [27600/60000 (46%)] Loss: 0.30\n",
      "Train Epoch: 4 [28000/60000 (47%)] Loss: 0.14\n",
      "Train Epoch: 4 [28400/60000 (47%)] Loss: 0.15\n",
      "Train Epoch: 4 [28800/60000 (48%)] Loss: 0.16\n",
      "Train Epoch: 4 [29200/60000 (49%)] Loss: 0.34\n",
      "Train Epoch: 4 [29600/60000 (49%)] Loss: 0.74\n",
      "Train Epoch: 4 [30000/60000 (50%)] Loss: 0.44\n",
      "Train Epoch: 4 [30400/60000 (51%)] Loss: 0.25\n",
      "Train Epoch: 4 [30800/60000 (51%)] Loss: 0.24\n",
      "Train Epoch: 4 [31200/60000 (52%)] Loss: 0.30\n",
      "Train Epoch: 4 [31600/60000 (53%)] Loss: 0.30\n",
      "Train Epoch: 4 [32000/60000 (53%)] Loss: 0.23\n",
      "Train Epoch: 4 [32400/60000 (54%)] Loss: 0.21\n",
      "Train Epoch: 4 [32800/60000 (55%)] Loss: 0.28\n",
      "Train Epoch: 4 [33200/60000 (55%)] Loss: 0.37\n",
      "Train Epoch: 4 [33600/60000 (56%)] Loss: 0.21\n",
      "Train Epoch: 4 [34000/60000 (57%)] Loss: 0.30\n",
      "Train Epoch: 4 [34400/60000 (57%)] Loss: 0.40\n",
      "Train Epoch: 4 [34800/60000 (58%)] Loss: 0.27\n",
      "Train Epoch: 4 [35200/60000 (59%)] Loss: 0.33\n",
      "Train Epoch: 4 [35600/60000 (59%)] Loss: 0.36\n",
      "Train Epoch: 4 [36000/60000 (60%)] Loss: 0.58\n",
      "Train Epoch: 4 [36400/60000 (61%)] Loss: 0.23\n",
      "Train Epoch: 4 [36800/60000 (61%)] Loss: 0.55\n",
      "Train Epoch: 4 [37200/60000 (62%)] Loss: 0.20\n",
      "Train Epoch: 4 [37600/60000 (63%)] Loss: 0.26\n",
      "Train Epoch: 4 [38000/60000 (63%)] Loss: 0.34\n",
      "Train Epoch: 4 [38400/60000 (64%)] Loss: 0.23\n",
      "Train Epoch: 4 [38800/60000 (65%)] Loss: 0.50\n",
      "Train Epoch: 4 [39200/60000 (65%)] Loss: 0.32\n",
      "Train Epoch: 4 [39600/60000 (66%)] Loss: 0.26\n",
      "Train Epoch: 4 [40000/60000 (67%)] Loss: 0.19\n",
      "Train Epoch: 4 [40400/60000 (67%)] Loss: 0.15\n",
      "Train Epoch: 4 [40800/60000 (68%)] Loss: 0.18\n",
      "Train Epoch: 4 [41200/60000 (69%)] Loss: 0.21\n",
      "Train Epoch: 4 [41600/60000 (69%)] Loss: 0.19\n",
      "Train Epoch: 4 [42000/60000 (70%)] Loss: 0.12\n",
      "Train Epoch: 4 [42400/60000 (71%)] Loss: 0.13\n",
      "Train Epoch: 4 [42800/60000 (71%)] Loss: 0.18\n",
      "Train Epoch: 4 [43200/60000 (72%)] Loss: 0.31\n",
      "Train Epoch: 4 [43600/60000 (73%)] Loss: 0.34\n",
      "Train Epoch: 4 [44000/60000 (73%)] Loss: 0.46\n",
      "Train Epoch: 4 [44400/60000 (74%)] Loss: 0.19\n",
      "Train Epoch: 4 [44800/60000 (75%)] Loss: 0.36\n",
      "Train Epoch: 4 [45200/60000 (75%)] Loss: 0.14\n",
      "Train Epoch: 4 [45600/60000 (76%)] Loss: 0.13\n",
      "Train Epoch: 4 [46000/60000 (77%)] Loss: 0.12\n",
      "Train Epoch: 4 [46400/60000 (77%)] Loss: 0.36\n",
      "Train Epoch: 4 [46800/60000 (78%)] Loss: 0.23\n",
      "Train Epoch: 4 [47200/60000 (79%)] Loss: 0.15\n",
      "Train Epoch: 4 [47600/60000 (79%)] Loss: 0.20\n",
      "Train Epoch: 4 [48000/60000 (80%)] Loss: 0.36\n",
      "Train Epoch: 4 [48400/60000 (81%)] Loss: 0.27\n",
      "Train Epoch: 4 [48800/60000 (81%)] Loss: 0.28\n",
      "Train Epoch: 4 [49200/60000 (82%)] Loss: 0.26\n",
      "Train Epoch: 4 [49600/60000 (83%)] Loss: 0.18\n",
      "Train Epoch: 4 [50000/60000 (83%)] Loss: 0.27\n",
      "Train Epoch: 4 [50400/60000 (84%)] Loss: 0.23\n",
      "Train Epoch: 4 [50800/60000 (85%)] Loss: 0.26\n",
      "Train Epoch: 4 [51200/60000 (85%)] Loss: 0.17\n",
      "Train Epoch: 4 [51600/60000 (86%)] Loss: 0.27\n",
      "Train Epoch: 4 [52000/60000 (87%)] Loss: 0.17\n",
      "Train Epoch: 4 [52400/60000 (87%)] Loss: 0.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [52800/60000 (88%)] Loss: 0.08\n",
      "Train Epoch: 4 [53200/60000 (89%)] Loss: 0.28\n",
      "Train Epoch: 4 [53600/60000 (89%)] Loss: 0.17\n",
      "Train Epoch: 4 [54000/60000 (90%)] Loss: 0.13\n",
      "Train Epoch: 4 [54400/60000 (91%)] Loss: 0.25\n",
      "Train Epoch: 4 [54800/60000 (91%)] Loss: 0.27\n",
      "Train Epoch: 4 [55200/60000 (92%)] Loss: 0.18\n",
      "Train Epoch: 4 [55600/60000 (93%)] Loss: 0.23\n",
      "Train Epoch: 4 [56000/60000 (93%)] Loss: 0.20\n",
      "Train Epoch: 4 [56400/60000 (94%)] Loss: 0.25\n",
      "Train Epoch: 4 [56800/60000 (95%)] Loss: 0.33\n",
      "Train Epoch: 4 [57200/60000 (95%)] Loss: 0.34\n",
      "Train Epoch: 4 [57600/60000 (96%)] Loss: 0.16\n",
      "Train Epoch: 4 [58000/60000 (97%)] Loss: 0.42\n",
      "Train Epoch: 4 [58400/60000 (97%)] Loss: 0.49\n",
      "Train Epoch: 4 [58800/60000 (98%)] Loss: 0.45\n",
      "Train Epoch: 4 [59200/60000 (99%)] Loss: 0.39\n",
      "Train Epoch: 4 [59600/60000 (99%)] Loss: 0.19\n",
      "Train Epoch: 4 [60000/60000 (100%)] Loss: 0.20\n",
      "Test set: Accuracy: 9555.0/10000 (96%), Average Loss: 0.00\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 216]' is invalid for input of size 600",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-1ce33bc976ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkernelSize\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkernelSizes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernelSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrainAccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestAccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraintest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-67-24d2d2f5f751>\u001b[0m in \u001b[0;36mtraintest_function\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlc\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-6d5f5a2b06c1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 216]' is invalid for input of size 600"
     ]
    }
   ],
   "source": [
    "#instantiate your CNN using kernel size\n",
    "kernelSizes=[3,4,5,6]\n",
    "\n",
    "for kernelSize in kernelSizes:\n",
    "    model = myCNN(kernelSize)\n",
    "    trainAccuracy, testAccuracy = traintest_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#set figure size   \n",
    "fig = plt.figure(figsize=(8,20))\n",
    "row = len(kernelSizes)\n",
    "column = 1\n",
    "subcnt = 4\n",
    "for i in range(1,2):\n",
    "    plt.subplot(row, column, i)\n",
    "    plt.title(\"Kernal Size\"+str(kernelSize))\n",
    "    plt.plot(trainAccuracy,'b--',label='train')\n",
    "    plt.plot(testAccuracy,'g--',label='test')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20   Training loss: 1.4090   Testing loss: 0.8155   Train accuracy: 0.7049   Test accuracy: 0.8422  \n",
      "Epoch: 5/20   Training loss: 0.3388   Testing loss: 0.3149   Train accuracy: 0.9044   Test accuracy: 0.9151  \n",
      "Epoch: 10/20   Training loss: 0.2811   Testing loss: 0.2659   Train accuracy: 0.9188   Test accuracy: 0.9243  \n",
      "Epoch: 15/20   Training loss: 0.2491   Testing loss: 0.2385   Train accuracy: 0.9283   Test accuracy: 0.9316  \n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = [], []\n",
    "accuracy_total_train, accuracy_total_val = [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "   \n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    total = 0\n",
    "    # training our model\n",
    "    for idx, (image, label) in enumerate(train_loader):\n",
    "\n",
    "#         image, label = image.to(device), label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(image)\n",
    "\n",
    "        loss = criterion(pred, label)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "        for i, p in enumerate(pred):\n",
    "            if label[i] == torch.max(p.data, 0)[1]:\n",
    "                total = total + 1\n",
    "                \n",
    "    accuracy_train = total / len(train_set)\n",
    "    accuracy_total_train.append(accuracy_train)\n",
    "\n",
    "    total_train_loss = total_train_loss / (idx + 1)\n",
    "    train_loss.append(total_train_loss)\n",
    "    \n",
    "    # validating our model\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    for idx, (image, label) in enumerate(test_loader):\n",
    "#         image, label = image, label.cuda()\n",
    "        pred = model(image)\n",
    "        loss = criterion(pred, label)\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "        pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "        for i, p in enumerate(pred):\n",
    "            if label[i] == torch.max(p.data, 0)[1]:\n",
    "                total = total + 1\n",
    "\n",
    "    accuracy_val = total / len(test_set)\n",
    "    accuracy_total_val.append(accuracy_val)\n",
    "\n",
    "    total_val_loss = total_val_loss / (idx + 1)\n",
    "    val_loss.append(total_val_loss)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "      print(\"Epoch: {}/{}  \".format(epoch, n_epochs),\n",
    "            \"Training loss: {:.4f}  \".format(total_train_loss),\n",
    "            \"Testing loss: {:.4f}  \".format(total_val_loss),\n",
    "            \"Train accuracy: {:.4f}  \".format(accuracy_train),\n",
    "            \"Test accuracy: {:.4f}  \".format(accuracy_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to have 20 epochs and from that I can see that the training accuracy started with a quite low and reached as good accuracy and the test accuracy started well to and completed with the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAccuracy=list(accuracy_total_train)\n",
    "testAccuracy=list(accuracy_total_val)\n",
    "\n",
    "#more code here"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "now we plot both to test for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e321d75fc8>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAikUlEQVR4nO3de3RV5Z3/8feXhHuAAAlJCAkEJGCcAjLR1laqlV8tun6CtVpvpU4dS53R1emqVh2d1WF1nKWOnen4q1p0po6lM71Z6oDVVmacaWttZUQHigTBAOeQG7khN4mQ5Dy/P/Y5yUlyQk5Izjlh789rrb3OPvuSfLMJHx6e8+xnm3MOERHxr1GZLkBERFJLQS8i4nMKehERn1PQi4j4nIJeRMTnsjNdQCJ5eXluzpw5mS5DROSs8eabb7Y45/IT7RuRQT9nzhy2bt2a6TJERM4aZhbub5+6bkREfE5BLyLicwp6ERGfU9CLiPicgl5ExOcU9CIiPqegFxHxuRE5jl5ExLfa2+HQIWht7bsA3HPPsH9LBb2ISLKcgw8+gPff77ucOAFHjiQO8Pjl6NH+v35hoYJeRGRIPvgAWlqguTnx63vveYGdKMhjYZ7sw5qmTIHp070lLw8WLOh+H7e4adN4f9w4DmdlccKM8hT82Ap6ETl7tbVBYyMcPOi9Njf3De/49ePHE38dMy94c3MhJwcmToTJk6GoyFsfYOkYO5bD7e2819HBe6NG0RKJcOjoUQ4fPtxzqavj8M6dfbZ3dnYCUFhYSENDw7BfJgW9iIws77/vhXZ8gPe3HDuW+GtMnOi1ovPzu1vTsfXYa/z61KmQldV1eltbG83NzQmXlr17+2w7fPjwaX+kCRMmkJub27UUFhaycOHCHttyc3PJy8sbxgvZTUEvIqkTicDhw6fvLolfb2rygj6RadO8PuyCAqis9F5jS2z7jBlecI8f3+f0kydPcvDgwe6lqoqGhoYe2xobG2lubub9fmrIzs4mLy+P/Px88vPzWbp0add6fn4+06dPZ+rUqT3Ce8qUKYwZM2YYL+rgKehFZHDa271AbmjwWtzxr01NPYO7tRWi3RJ9jB/fs5VdXu69xgd3bJkxA0aP7nG6c45jx47R2tpKa2srLc3NNFdVdYV27xB/7733+pRgZuTl5VFUVERhYSHz589nxowZPcI8fpkyZQpmloqrmlIKepEgcw46OrwPKdvavNZ3ogCPf21pSfyB5LRpXiDn58PChf13k8ReJ0yIK8Nx5MgRmpqavNBuaaG1sZHWqipvPRbmceutra20t7cn/LHGjx9PUVERRUVFVFRUcNlll3WFeWwpKioiPz+f0b3+AfEjBb3ISBcb0nfkyMDL++97x8aWtrae7xNtj0T6/96jR3ut68JCKCuDiy7yPqAsLOz5WlAAY8f2OT0SidDc3NzVum74wx9oaGhIuHzwwQcJS8jOzmb69OldS3l5eY/3eXl5Xev5+fkUFRWRk5NzVra8U0VBL5IJnZ1eCzkc9pZQyHttakoc4P20XHuYNMn7EHL8eBg3rvt13DhvqF9sPX57/DJ+fPdIk1iAT50Ko3reQO+c4/DhwzQ2NtLU1ETjli3ea2NjV5dJbGlsbOwaURJvypQpXS3uiy66qGu9oKCgT3hPnjxZoT1ECnqRVGhvh9raniEev15T0ze88/K8gJ0yxXtdsMBbT2aZNKnHqJHB6ujooLm5mYMHD3qh/dZbNDU1dQV4/GtTU1PCLhMz62pRFxUVsWjRoq71+KWwsJDxCT4sldRR0Iv0Fol4N8YcO+aNu44tybxvbPSCvK6ub5fIzJkwezZceCF89rPe+uzZMGcOlJZ6rfFhdvz48e5uk7gPJ3tva25uJpKgC2fs2LEUFBRQUFDAzJkzOf/885kxYwYFBQV9XqdPn07WEP6xkdRR0EtwRCJeENfUeK3t+NfYemtr/8P7EsnO9lrTOTnea14efOITPUN89mwoKUnYh32mOjo6qK+vp7a2lpqamq7X+vr6HiGeaJhgdnZ21weSpaWlXHjhhT0+qIyF94wZM5g0aZK6TXxAQS/+0NbmhXRDQ/8hXlfnjTCJN24czJrlBfEll3hBPWlSd3jHlv7ep2B8dEdHBw0NDX1CPPZaU1PDwYMH+7TAc3JyKC4upqioiMrKyq6RJb1fp02bxqhRmrg2SBT0MrJEIv1PDBUbl51oaWvr+7XGju0O8WXLvNeSku5ts2Z5t72nscXa2dlJY2NjV2DHh3fsfX19fZ8QnzBhAiUlJZSUlHD55Zd3rc+aNavr9Wwd4y2pp6CX9Dl6tLtlHb/Eb2tu7n+436hR3iiQvDwvoEtL4fzze04SVVjYHeJ5eWkNcecczc3NPYK7d6DX1dXR0et/FePGjesK7uXLl/cJ8JKSEnJzcxXicsYU9DI8jh+Hd9/t/iAyUYgnmlBq2jQoLvaW88/3gjrBDH9dE05luMuhs7OTmpoaqqur2bt3L9XV1V3L3r17aev1P4sxY8Z0hfWyZct6hHdsmTZtmkJcUkpBL8lrb4f9+2HPnp7L7t1QX9/z2Oxsbxx2cTF86EOwYkV3oBcXey3umTMTzkmSae3t7YRCoR4BHlvft29fj6GFY8eOZd68eZxzzjlcfvnlzJkzp0eI5+fnK8Ql4xT00pNzXmjHAjw+0Pft6zlvyfTp3vwkn/yk91pe7o0ymTXLuxV+hH7g55yjpaWF/fv3s3//fvbt29djPRwO97jJZ+LEiZxzzjn80R/9EVdffTXnnHMO55xzDvPmzaO4uFgfbMqIp6APuro6+N3v4LXXvNedO70x5DHjx8P8+bB4MVx3nRfmCxZ426ZPz1zdAzh+/HjCII8tvYcd5uXlUVZWxgUXXMBNN93UI8wLCgrUKpezmoI+SDo6YMeOnsEeDnv7xo3zbuT54he9II+10IuLR2zLvL29nerqaqqqqti1axdVVVVUV1ezf/9+Wlpaehw7ceJE5s6dy9y5c1m+fDllZWVdy5w5c5g0aVKGfgqR1FPQ+9mRI/D6693BvmVL9weiM2fCxz4GX/mK97p4cUrGhA+HtrY2du/e3SPQd+3axbvvvttjBMvs2bMpLy/nmmuu6RHkZWVl5OXlqVUugaWg9wvnvA9KYy31116Dt9/2to8aBYsWwS23wEc/6gV7aWlahx4m48iRI7zzzjt9An3//v246LS4WVlZzJs3j4qKCq6++moqKio499xzWbhwIRNTMIWAiB8o6M9WkYjXn/7qq91LXZ23b/Jk+MhH4NprvWD/8Ie9OzlHgJMnT7J371727NnTY9m9ezdNTU1dx40ZM4YFCxZQWVnJ5z//ec4991wqKiqYP38+Y4dxKgGRIFDQny3a2+Gtt+A3v/FC/be/9Z5YD14/+rJl3nLxxXDeeUOayXCoIpEINTU1fYJ8z549hMPhHnd9FhQUUF5ezlVXXUV5eTkLFiygoqKCsrIysrP16ykyHPQ3aaQ6ccLrX48F++uvd4+GKS+Ha67pDveysox0wzjnqKurY9u2bWzfvp3t27eza9cuqqurezxEIicnh/Lycj784Q+zevVqysvLKS8vZ/78+eTm5qa9bpGgUdCPFM7Br34Fv/iFF+5vvumNkjHzPii97bbuFnthYdrLO3XqFLt27eoR6tu2bePQoUNdx8ydO5eKigo+9alPsWDBgq5ALyws1AehIhmkoM8052DzZli71mu1jxkDF1wAX/uaF+wf/aj3YIk0am1t7RHm27dvp6qqquuO0HHjxvGhD32Ia665hiVLlrB48WIWLVrE5MmT01qniCRHQZ8pvQO+tBSeegpWr077tAAtLS28+OKLvPDCC2zZsoXa2tqufUVFRSxevJgVK1Z0hfr8+fPVfy5yFtHf1nTrL+D/5E/SOo69urqajRs3snHjRl577TUikQgzZ87k0ksvZcmSJV2hPmPGjLTVJCKpkVTQm9kK4DEgC/hn59zDvfZPBZ4B5gEfALc6595O5tzAyHDARyIR/ud//oeNGzeyadMmqqqqAFi0aBH3338/q1at4o//+I/Vly7iQwMGvZllAU8AnwRqgTfMbJNzrirusPuBbc65T5vZwujxy5M8198yGPBtbW288sorbNq0iRdeeIGDBw+SlZXFJZdcwpo1a1i5ciVlZWUprUFEMi+ZFv2FQLVzbh+Amf0IWAXEh3UF8BCAc+4dM5tjZgXA3CTO9Sfn4OWXvYDfsiVtAR/rb9+4cSMvv/wyJ06cICcnhyuuuIJVq1Zx5ZVXMnXq1JR9fxEZeZIJ+mKgJu59LfDhXsdsB64BfmtmFwKzgVlJnguAma0B1gCUlpYmU/vI1DvgZ8+Gp5/2ph9IUcA3NDSwYcMGfvrTn/Lqq68SiUQoLi7mlltuYdWqVVx66aW6m1QkwJIJ+kSdtq7X+4eBx8xsG7AD+F+gI8lzvY3OPQ08DVBZWZnwmBHvlVfggQfSEvCxcH/uued49dVXcc5RUVHBAw88wKpVq1i6dKn620UESC7oa4GSuPezgB6PE3LOHQW+AGBeuuyPLhMGOtcXOjq8gP+7v0tpwPcX7n/913/NddddR0VFxbB+PxHxh2SC/g1gvpmVAXXADcBN8QeYWS5wwjl3CrgN+I1z7qiZDXjuWa+hAa6/3pum4Pbb4Vvf8uZ2H7Yvr3AXkaEZMOidcx1mdifwMt4QyWecczvN7Pbo/nXAucB6M+vE+6D1T093bmp+lAz47/+GG2+EY8fgX/8Vbr55WL5sonA/77zzFO4ickYsNs/3SFJZWem2bt2a6TL6F4nAI4/AX/2VN8HYhg0wxPBtb29n/fr1rF+/vke4X3fddQp3ERmQmb3pnKtMtE93xg7WoUPw+c/Diy/CDTfAP/0T5OSc8ZdzzvHCCy9wzz33sHv3bnXLiMiwU9APxhtveA/Irq+HJ56AP/uzIU0PvHXrVu6++25+/etfs2DBAjZu3MhVV12l0TIiMqxG5lOfRxrn4MknvSmCnfMe+vHnf37GIR8Oh7n55pu54IILqKqq4sknn2THjh2sXLlSIS8iw04t+oEcPw5f+hL84Adw5ZWwfj1Mn35GX+rw4cM89NBDPPbYY5gZ999/P/fee6+m9xWRlFLQn05Vlffc1d274W//Fu67z3vQ9iCdOnWKdevW8Y1vfINDhw6xevVqHnzwQUpKSgY+WURkiBT0/fnBD+CLX/Q+aP2P/4DLLhv0l3DO8fzzz3PvvfdSXV3NZZddxje/+U3OP//8FBQsIpKY+uh7O3kS7rjDGxO/dCn87/+eUci//vrrLFu2jM985jOMGTOGF198kf/8z/9UyItI2ino44VC3geuTz7pPcrvv/4LZs4c1JfYt28f119/PRdddBHV1dU89dRTbN++nSuvvFIftIpIRqjrJqatzQv5Y8fg+efh6qsHdbpzjrVr1/LQQw8xevRovv71r3P33XczadKk1NQrIpIkBX3Ms89CXZ03A+UZdNU89dRTfOMb3+DGG2/k0Ucfpbi4ePhrFBE5A5oCAbzZJ8vLoaAAfve7QY+Pf+ONN7j44otZvnw5P//5zxl1BiNzRESGQlMgDOSnP4X9++Ef/mHQId/a2sq1115LYWEh3//+9xXyIjLiKOidg4cfhoULYeXKQZ0aiUT43Oc+x8GDB/ntb3/L9DO8kUpEJJUU9Js3w/bt8Mwzg74Z6sEHH+SXv/wl3/nOd7jgggtSVKCIyNCon+Hhh6G4eNBzyW/evJm1a9fyuc99ji996UspKk5EZOiCHfRbtsCvfgVf/eqgHvt34MABbrrpJs477zzWrVun8fEiMqIFO+gfeQSmTvWmOkjSqVOn+OxnP8upU6fYsGEDEydOTGGBIiJDF9w++nfegX//d++h3oO4qemuu+5iy5YtPPfcc5SXl6euPhGRYRLcFv2jj3oP8f7yl5M+5Yc//CGPP/44X/3qV7n22mtTWJyIyPAJZtDX1cH3vw+33gr5+UmdUlVVxW233cbFF1/Mww8/nOICRUSGTzCD/lvf8h7wfdddSR1+7NgxPvOZz5CTk8OPf/xjRo8eneICRUSGT/D66N97D556Cq6/HsrKBjzcOcdtt93Gnj17eOWVV5g5yNksRUQyLXhB/+ST3uMB7703qcO//e1v85Of/ISHHnqISy+9NLW1iYikQLC6btra4LHH4IorYNGiAQ///e9/z1133cXKlSu555570lCgiMjwC1bQ/8u/QHOz9+zXATQ1NXHddddRWlrK9773PU1WJiJnreB03XR0wDe/CR/5CCxbdtpDOzs7uemmm2htbeX3v/89ubm56alRRCQFghP0zz2X9FTEa9eu5ZVXXuG73/0uS5YsSU99IiIpEoz+COe86Q6SmIr4xRdf5MEHH+TWW2/l1ltvTVOBIiKpE4wW/csvJzUV8f79+1m9ejVLlizh8ccfT2OBIiKpE4wW/SOPJDUV8erVq4lEImzYsIHx48enqTgRkdTyf9AnORXxyZMnee211/jyl7/M3Llz01efiEiK+T/ok5yK+MCBAwDMmzcvHVWJiKSNv4M+NhXxHXcMOBVxOBwGYM6cOamvS0QkjZIKejNbYWa7zazazPrcbWRmU8zsBTPbbmY7zewLcftCZrbDzLaZ2dbhLH5Ag5iKOBQKATB79uwUFyUikl4DjroxsyzgCeCTQC3whpltcs5VxR12B1DlnLvKzPKB3Wb2b865U9H9n3DOtQx38adVW+tNRbxmTVJTEYdCIbKyspg1a1YaihMRSZ9kWvQXAtXOuX3R4P4RsKrXMQ6YZN7DU3OAQ0DHsFY6WP/4j4OaijgcDlNcXEx2djBGnIpIcCQT9MVATdz72ui2eI8D5wL1wA7gL5xzkeg+B2w2szfNbE1/38TM1pjZVjPb2tzcnPQPkNAgpyIGr0Wv/nkR8aNkgj7RfAGu1/tPAduAmcAS4HEzmxzd9zHn3FLgCuAOM/t4om/inHvaOVfpnKvMT/KpT/0a5FTE4LXoFfQi4kfJBH0tUBL3fhZeyz3eF4CfOU81sB9YCOCcq4++NgHP43UFpc4gpyIGaG9vp66uTh/EiogvJRP0bwDzzazMzMYANwCbeh1zAFgOYGYFwAJgn5lNNLNJ0e0TgcuBt4er+IQGMRVxTG1tLZFIRC16EfGlAT95dM51mNmdwMtAFvCMc26nmd0e3b8O+BvgWTPbgdfVc69zrsXM5gLPe5/Rkg38wDn3yxT9LIOaijiehlaKiJ8lNcTEOfcS8FKvbevi1uvxWuu9z9sHLB5ijcmLTUX8rW8NOBVxPN0sJSJ+5p87Y2NTEZ97Llx11aBODYVCmBklJSUDHywicpbxz6DxY8dg/nwv5Af52L9wOMzMmTMZc5pJz0REzlb+CfrJk72umzMQCoXUPy8ivuWfrpsh0M1SIuJngQ/6jo4Oamtr1aIXEd8KfNDX19fT0dGhFr2I+Fbgg15DK0XE7wIf9LpZSkT8LvBBH2vRl5aWZrgSEZHUCHzQh0IhCgoKGD9+fKZLERFJicAHvaYnFhG/C3zQ62YpEfG7QAd9JBLhwIEDatGLiK8FOugPHjzIqVOn1KIXEV8LdNBrDL2IBEGgg15j6EUkCBT0KOhFxN8CHfThcJjp06eTk5OT6VJERFIm0EGv6YlFJAgCHfS6WUpEgiCwQe+cIxwOq39eRHwvsEHf3NxMW1ubWvQi4nuBDXqNuBGRoAhs0OtmKREJisAGvVr0IhIUgQ36cDhMbm4uU6ZMyXQpIiIpFdig1/TEIhIUgQ16jaEXkaAIZNA759SiF5HACGTQHzp0iOPHj6tFLyKBEMig19BKEQmSQAa9hlaKSJAEMujVoheRIAlk0IdCIXJycpg6dWqmSxERSbmkgt7MVpjZbjOrNrP7EuyfYmYvmNl2M9tpZl9I9txMiA2tNLNMlyIiknIDBr2ZZQFPAFcAFcCNZlbR67A7gCrn3GLgUuDvzWxMkuemnYZWikiQJNOivxCods7tc86dAn4ErOp1jAMmmddEzgEOAR1Jnpt2ullKRIIkmaAvBmri3tdGt8V7HDgXqAd2AH/hnIskeS4AZrbGzLaa2dbm5uYkyx+8I0eOcPjwYbXoRSQwkgn6RB3Zrtf7TwHbgJnAEuBxM5uc5LneRueeds5VOucq8/PzkyjrzGjEjYgETTJBXwuUxL2fhddyj/cF4GfOUw3sBxYmeW5aaQy9iARNMkH/BjDfzMrMbAxwA7Cp1zEHgOUAZlYALAD2JXluWqlFLyJBkz3QAc65DjO7E3gZyAKecc7tNLPbo/vXAX8DPGtmO/C6a+51zrUAJDo3NT9KckKhEOPHjyeV3UMiIiPJgEEP4Jx7CXip17Z1cev1wOXJnptJsaGVGkMvIkERuDtjNbRSRIImcEGvm6VEJGgCFfTHjx+ntbVVLXoRCZRABX1sxI1a9CISJIEMerXoRSRIAhX0ullKRIIoUEEfDocZM2YMhYWFmS5FRCRtAhX0oVCI0tJSRo0K1I8tIgEXqMTTGHoRCaJABb3G0ItIEAUm6Nva2mhsbFSLXkQCJzBBf+DAAUBDK0UkeAIT9BpaKSJBFZig181SIhJUgQn6UChEdnY2M2fOzHQpIiJpFZigD4fDlJSUkJWVlelSRETSKjBBr6GVIhJUgQl63SwlIkEViKA/deoU9fX1atGLSCAFIuhrampwzqlFLyKBFIig1xh6EQmyQAS9xtCLSJAFIuhDoRCjRo1i1qxZmS5FRCTtAhH04XCY4uJiRo8enelSRETSLhBBHwqF1G0jIoEVmKDXB7EiElS+D/qOjg7q6urUoheRwPJ90NfW1tLZ2akWvYgElu+DXkMrRSTofB/0ullKRILO90Efa9GXlpZmuBIRkczwfdCHQiGKiooYO3ZspksREckI3we9picWkaBLKujNbIWZ7TazajO7L8H+r5nZtujytpl1mtm06L6Qme2I7ts63D/AQDSGXkSCbsCgN7Ms4AngCqACuNHMKuKPcc496pxb4pxbAvwl8Gvn3KG4Qz4R3V85fKUPrLOzk5qaGrXoRSTQkmnRXwhUO+f2OedOAT8CVp3m+BuBHw5HcUPV0NBAe3u7gl5EAi2ZoC8GauLe10a39WFmE4AVwIa4zQ7YbGZvmtma/r6Jma0xs61mtrW5uTmJsgYWG3GjrhsRCbJkgt4SbHP9HHsV8FqvbpuPOeeW4nX93GFmH090onPuaedcpXOuMj8/P4myBhYbQ68WvYgEWTJBXwuUxL2fBdT3c+wN9Oq2cc7VR1+bgOfxuoLSIhb0GkMvIkGWTNC/Acw3szIzG4MX5pt6H2RmU4BLgI1x2yaa2aTYOnA58PZwFJ6McDjMjBkzmDBhQrq+pYjIiJM90AHOuQ4zuxN4GcgCnnHO7TSz26P710UP/TSw2Tn3ftzpBcDzZhb7Xj9wzv1yOH+A09HQShGRJIIewDn3EvBSr23rer1/Fni217Z9wOIhVTgE4XCYxYsz9u1FREYE394ZG4lECIfDatGLSOD5Nuibmpo4efKkRtyISOD5Nug1PbGIiMe3Qa8HjoiIeHwb9GrRi4h4fBv04XCYadOmMWnSpEyXIiKSUb4N+lAopG4bERF8HPQaWiki4vFl0Dvn1KIXEYnyZdC3tLRw4sQJtehFRPBp0GtopYhIN18GvYZWioh082XQq0UvItLNl0EfCoWYPHkyubm5mS5FRCTjfBn04XBYrXkRkShfBr2GVoqIdPNd0DvndLOUiEgc3wX94cOHOXr0qFr0IiJRvgv62IgbtehFRDy+C/rYGHq16EVEPL4NerXoRUQ8vgv6cDjMxIkTmT59eqZLEREZEXwX9KFQiNmzZ2NmmS5FRGRE8F3Q62YpEZGefBf0sRa9iIh4fBX0R48e5b333lOLXkQkjq+CXmPoRUT68mXQq0UvItLNV0Gvm6VERPryVdCHw2HGjRvHjBkzMl2KiMiI4aug1xh6EZG+fBX0mp5YRKQvXwW9HjgiItKXb4K+s7OTFStWsGzZskyXIiIyomQnc5CZrQAeA7KAf3bOPdxr/9eAm+O+5rlAvnPu0EDnDpesrCzWr1+fii8tInJWG7BFb2ZZwBPAFUAFcKOZVcQf45x71Dm3xDm3BPhL4NfRkB/wXBERSa1kum4uBKqdc/ucc6eAHwGrTnP8jcAPz/BcEREZZskEfTFQE/e+NrqtDzObAKwANpzBuWvMbKuZbW1ubk6iLBERSUYyQZ9oULrr59irgNecc4cGe65z7mnnXKVzrjI/Pz+JskREJBnJBH0tUBL3fhZQ38+xN9DdbTPYc0VEJAWSCfo3gPlmVmZmY/DCfFPvg8xsCnAJsHGw54qISOoMOLzSOddhZncCL+MNkXzGObfTzG6P7l8XPfTTwGbn3PsDnTvcP4SIiPTPnOuvuz1zKisr3datWzNdhojIWcPM3nTOVSbcNxKD3syagfAZnp4HtAxjOcNN9Q2N6hsa1Tc0I7m+2c65hCNZRmTQD4WZbe3vX7WRQPUNjeobGtU3NCO9vv74Zq4bERFJTEEvIuJzfgz6pzNdwABU39CovqFRfUMz0utLyHd99CIi0pMfW/QiIhJHQS8i4nNnZdCb2Qoz221m1WZ2X4L9Zmb/L7r/D2a2NM31lZjZf5vZLjPbaWZ/keCYS83siJltiy5fT3ONITPbEf3efe5Oy+Q1NLMFcddlm5kdNbOv9DomrdfPzJ4xsyYzeztu2zQz+w8zezf6OrWfc0/7+5rC+h41s3eif37Pm1luP+ee9nchhfWtNbO6uD/DK/s5N1PX78dxtYXMbFs/56b8+g2Zc+6sWvCmUtgLzAXGANuBil7HXAn8Am/2zI8AW9JcYxGwNLo+CdiToMZLgZ9n8DqGgLzT7M/oNez1530Q72aQjF0/4OPAUuDtuG1/B9wXXb8PeKSf+k/7+5rC+i4HsqPrjySqL5nfhRTWtxa4O4k//4xcv177/x74eqau31CXs7FFn8zDTFYB653ndSDXzIrSVaBzrsE591Z0/Riwi37m4R/BMnoN4ywH9jrnzvRO6WHhnPsNcKjX5lXA96Lr3wOuTnBqWh6+k6g+59xm51xH9O3reLPHZkQ/1y8ZGbt+MWZmwGfpOTPvWeVsDPpkHmaS9ANPUs3M5gDnA1sS7L7IzLab2S/M7Lz0VoYDNpvZm2a2JsH+kXINe099HS+T1w+gwDnXAN4/7sCMBMeMlOt4K97/0BIZ6Hchle6Mdi0900/X10i4fsuARufcu/3sz+T1S8rZGPTJPMxkMA9LSRkzy8F72tZXnHNHe+1+C687YjHwbeDf01zex5xzS/Ge53uHmX281/6MX0PzprZeCTyXYHemr1+yRsJ1fADoAP6tn0MG+l1Ile8A84AlQANe90hvGb9+9Hw8aiKZun5JOxuDPpmHmWT8gSdmNhov5P/NOfez3vudc0edc8ej6y8Bo80sL131Oefqo69NwPN4/0WOl/FriPcX5y3nXGPvHZm+flGNse6s6GtTgmMyeh3N7Bbg/wI3u2iHcm9J/C6khHOu0TnX6ZyLAP/Uz/fN9PXLBq4BftzfMZm6foNxNgZ9Mg8z2QR8Pjpy5CPAkdh/sdMh2qf3XWCXc+4f+jmmMHocZnYh3p9Fa5rqm2hmk2LreB/avd3rsIxew6h+W1KZvH5xNgG3RNdvoedDd2Iy9vAdM1sB3AusdM6d6OeYZH4XUlVf/Gc+n+7n+2b64UX/B3jHOVebaGcmr9+gZPrT4DNZ8EaE7MH7NP6B6Lbbgduj6wY8Ed2/A6hMc30X4/338g/AtuhyZa8a7wR24o0ieB34aBrrmxv9vtujNYzEazgBL7inxG3L2PXD+wenAWjHa2X+KTAdeAV4N/o6LXrsTOCl0/2+pqm+arz+7djv4Lre9fX3u5Cm+r4f/d36A154F42k6xfd/mzsdy7u2LRfv6EumgJBRMTnzsauGxERGQQFvYiIzynoRUR8TkEvIuJzCnoREZ9T0IuI+JyCXkTE5/4/tKyMgjleNcUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trainAccuracy,'k')\n",
    "plt.plot(testAccuracy,'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot we can see the test accuracy and the train accuracy with is been opted for the kernel size 2 and 20 epochs. In the starting stage the model was overfiting but in the final stages they were almost the same"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tune the parameters above until you get a good training efficiency and not too much overfitting, also do not train more than neccesary\n",
    "\n",
    "In the next cell we define the kernel sizes we want to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have chosen to have different kernel sizes and I felt i have opted best accuracy resluts for the 4 kernel and for the 5th kernel i felt the model has been overfitted and lost some of the accuracy but i felt it can be changed if i had more epochs for the computation to complete i chosen to have only 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Size: 2 Epoch: 0/1   Training loss: 1.4678   Testing loss: 0.8380   Train accuracy: 0.6987   Test accuracy: 0.8474  \n",
      "Kernel Size: 3 Epoch: 0/1   Training loss: 1.5123   Testing loss: 0.8995   Train accuracy: 0.6432   Test accuracy: 0.8128  \n",
      "Kernel Size: 4 Epoch: 0/1   Training loss: 1.6411   Testing loss: 1.0057   Train accuracy: 0.6391   Test accuracy: 0.8275  \n",
      "Kernel Size: 5 Epoch: 0/1   Training loss: 1.4475   Testing loss: 0.8237   Train accuracy: 0.6839   Test accuracy: 0.8323  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kernelSizes=[2,3,4,5]\n",
    "n_epochs = 1\n",
    "\n",
    "for kernelSize in kernelSizes:\n",
    "    \n",
    "#             #now plot this\n",
    "#     plt.figure()\n",
    "#     plt.plot(trainAccuracy,'k')\n",
    "#     plt.plot(testAccuracy,'r')\n",
    "#     plt.title(\"Kernel size: \"+str(kernelSize))\n",
    "    #instantiate your CNN using kernel size\n",
    "    model = myCNN()\n",
    "#     model\n",
    "    \n",
    "    #define optimizer and loss function\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #train and test - you can copy the code \n",
    "    train_loss, val_loss = [], []\n",
    "    accuracy_total_train, accuracy_total_val = [], []\n",
    "            \n",
    "    trainAccuracy=list(accuracy_total_train)\n",
    "    testAccuracy=list(accuracy_total_val)\n",
    "            #now plot this\n",
    "    plt.figure()\n",
    "    plt.plot(trainAccuracy,'k')\n",
    "    plt.plot(testAccuracy,'r')\n",
    "    plt.title(\"Kernel size: \"+str(kernelSize))\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "\n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        total = 0\n",
    "            # training our model\n",
    "        for idx, (image, label) in enumerate(train_loader):\n",
    "\n",
    "    #         image, label = image.to(device), label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = model(image)\n",
    "\n",
    "            loss = criterion(pred, label)\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "            for i, p in enumerate(pred):\n",
    "                if label[i] == torch.max(p.data, 0)[1]:\n",
    "                    total = total + 1\n",
    "\n",
    "        accuracy_train = total / len(train_set)\n",
    "        accuracy_total_train.append(accuracy_train)\n",
    "\n",
    "        total_train_loss = total_train_loss / (idx + 1)\n",
    "        train_loss.append(total_train_loss)\n",
    "\n",
    "        # validating our model\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        for idx, (image, label) in enumerate(test_loader):\n",
    "    #         image, label = image, label.cuda()\n",
    "            pred = model(image)\n",
    "            loss = criterion(pred, label)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "            for i, p in enumerate(pred):\n",
    "                if label[i] == torch.max(p.data, 0)[1]:\n",
    "                    total = total + 1\n",
    "\n",
    "        accuracy_val = total / len(test_set)\n",
    "        accuracy_total_val.append(accuracy_val)\n",
    "\n",
    "        total_val_loss = total_val_loss / (idx + 1)\n",
    "        val_loss.append(total_val_loss)\n",
    "\n",
    "        if epoch % 2 == 0:\n",
    "            print(\"Kernel Size:\", kernelSize,\"Epoch: {}/{}  \".format(epoch, n_epochs),\n",
    "                    \"Training loss: {:.4f}  \".format(total_train_loss),\n",
    "                    \"Testing loss: {:.4f}  \".format(total_val_loss),\n",
    "                    \"Train accuracy: {:.4f}  \".format(accuracy_train),\n",
    "                    \"Test accuracy: {:.4f}  \".format(accuracy_val))\n",
    "#     trainAccuracy=list(accuracy_total_train)\n",
    "#     testAccuracy=list(accuracy_total_val)\n",
    "#             #now plot this\n",
    "#     plt.figure()\n",
    "#     plt.plot(trainAccuracy,'k')\n",
    "#     plt.plot(testAccuracy,'r')\n",
    "#     plt.title(\"Kernel size: \"+str(kernelSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Size: 2 Epoch: 0/1   Training loss: 1.6383   Testing loss: 0.9928   Train accuracy: 0.6571   Test accuracy: 0.8421  \n",
      "Kernel Size: 3 Epoch: 0/1   Training loss: 1.4355   Testing loss: 0.8215   Train accuracy: 0.7150   Test accuracy: 0.8328  \n",
      "Kernel Size: 4 Epoch: 0/1   Training loss: 1.7478   Testing loss: 1.1490   Train accuracy: 0.6082   Test accuracy: 0.8120  \n",
      "Kernel Size: 5 Epoch: 0/1   Training loss: 1.6285   Testing loss: 0.9868   Train accuracy: 0.6460   Test accuracy: 0.8189  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZdklEQVR4nO3df7CddYHf8ffHxGyLIIblgiGJgGxA8Q+jexY7o13ZZdGAYmBHOsm2mqJtTMfMwO7WJWo7Y+vaRRDZdkHTqKnZWSTrFjJE6xJYRrGlFXNCA0mIkWuM5JKYXJrdwrJt2cCnf5znuo8n5+Y8N/fe3Hv5fl4zZ855vj+e8/3OvXM+5/me8zxHtomIiPK8YqoHEBERUyMBEBFRqARAREShEgAREYVKAEREFCoBEBFRqARAxBhJ+qqk3z+Bfn8t6fWTMaaIE5EAiBlD0j5Jv1HbXibpLyW9cyrH1ZTtU23vnaz9S7pQ0r2ShiUdkbRF0kWT9Xwx8yUAYkaStAK4A3iP7YfG2Hf25Ixqyr0G2AxcBJwNfB+4dyoHFNNbAiBmHEkrgVuBd9v+71XZ6ZK+IumgpKcl/b6kWVXdP5X0sKTbJB0BPlUt49wh6b9Iek7SI5IuqD3HGyQ9UL2T3iPpHzUc2y9JekjS/5b0jKQ/rdW5qj+nWg4auf2NJNfafUjS7uroZoukc5s8t+3v2/6K7SO2/xa4DbhI0i826R/lSQDETPMvgE8Dl9lu18o3AEeBXwLeArwL+Ge1+rcBe4GzgM9UZcuBfwPMBQZHyiW9CngA+FrVfjnwBUlvajC+TwP3V/tcAPxRdwPbB6rloFNtnwpsAjZWz3018AngN4EB4L8Cd430lfRNSWsajAPgV4Gf2v5fDdtHYRIAMdNcDnwP2DFSIOls4ArgBtvP2z5M593vslq/A7b/yPZR2/+nKrunetd8FLgTWFyVvxfYZ/s/Ve0fBe4G3t9gfH8LnAucY/v/2v5vx2ss6UbgDcCHqqKPAH9ge3c1rn8HLB45CrD9Xts39RuEpAV0lsh+p8GYo1AJgJhpVgEXAl+WpKrsXOCVwEFJfyXpr4D/SOfd+4j9Pfb109rjvwFOre3vbSP7qvb3j4HXNhjf7wECvi9pl6QPjdZQ0hXA9cDVtVA6F/j3tec9Uu1vfoPnHtnvAJ2jkC/Yvqtf+yjXy/XDsHj5OgxcBjwEfIHOktB+4P8BZ1bvmnsZy2Vv9wMP2b58rIOz/VPgnwNIegfwF5K+a3uw3q76ds4G4Ddt18NpP/AZ23eO9bmr/c6l8+K/2fZn+rWPsuUIIGYc2weAXweWSLrN9kE6L3q3Snq1pFdIumAcXw/9JnChpA9IemV1+xVJb+zXUdK11fILwF/SCZ4Xu9q8ms63c/5VjyWitcDHRz5vqD7cvrbJoKv9bgEett30c4IoWAIgZqTqXfOvA++X9AfAB4E5wBN0Xnj/MzDvBPf9HJ0PkZcBB+gsFX0W+IUG3X8FeETSX9P5Sub1tn/c1eatdL6q+fn6t4Gq595UPddGSc8CO+l8vgGApD+X9IlRnvua6vmv6/qW0euazTxKo/wgTEREmXIEEBFRqARAREShEgAREYVqFACSllSnww/2Ogux+qbCNyQ9Vn33+bpa3T5JOyRtl9SulZ9RnWr/ZHU/d2KmFBERTfT9ELi6nsoP6ZyBOQRsBZbbfqLW5hPA6bZvrE5C2QO81vYLkvYBLdvPdO33ZuCI7ZuqUJlr+8bjjeXMM8/0eeedN9Y5RkQUbdu2bc/YHugub3Ii2CXA4MhlbCVtBJbS+brdCAOnVWdmnkrn7MXRTsgZsRS4tHq8AfgOcNwAOO+882i328drEhERXST9pFd5kyWg+fz8afRDHHta+u3AG+l8Z3oHne8+v1TVGbhf0jZ1ruI44uzqBB6q+/pp+/WBr5TUltQeHh5uMNyIiGiiSQCoR1n3utG7ge3AOXQuqHV7dVYiwNttv5XOySwflfSrYxmg7XW2W7ZbAwPHHMFERMQJahIAQ8DC2vYCOu/0666jc2VFV9c8+TGdKxyOnLZPdYXGTXSWlAAOSZoHUN0fPtFJRETE2DUJgK3AIknnS5pD5/T4zV1tnqJzga6RS/NeBOyV9CpJp1Xlr6Jzev3Oqs9mYEX1eAX55aKIiJOq74fAto9KWk3nIlOzgPW2d0laVdWvpfMjGF+VtIPOktGNtp9R5wewN1VX7Z0NfM32fdWubwK+LunDdAKk0QWvIiJiYsyoawG1Wi3nW0AREWMjaZvtVnd5zgSOiChUAiAiolAJgIiIQiUAIiIKlQCIiChUAiAiolAJgIiIQiUAIiIKlQCIiChUAiAiolAJgIiIQiUAIiIKlQCIiChUAiAiolAJgIiIQiUAIiIKlQCIiChUowCQtETSHkmDktb0qD9d0jckPSZpl6TrqvKFkr4taXdVfn2tz6ckPS1pe3W7cuKmFRER/fT9TWBJs4A7gMuBIWCrpM22n6g1+yjwhO2rJA0AeyTdCRwFftf2o9WPw2+T9ECt7222PzehM4qIiEaaHAFcAgza3mv7BWAjsLSrjYHT1Pn191OBI8BR2wdtPwpg+zlgNzB/wkYfEREnrEkAzAf217aHOPZF/HbgjcABYAdwve2X6g0knQe8BXikVrxa0uOS1kuaO8axR0TEODQJAPUoc9f2u4HtwDnAYuB2Sa/+2Q6kU4G7gRtsP1sVfxG4oGp/ELi155NLKyW1JbWHh4cbDDciIppoEgBDwMLa9gI67/TrrgPucccg8GPgDQCSXknnxf9O2/eMdLB9yPaL1ZHCl+gsNR3D9jrbLdutgYGBpvOKiIg+mgTAVmCRpPMlzQGWAZu72jwFXAYg6WzgImBv9ZnAV4Ddtj9f7yBpXm3zGmDniU0hIiJORN9vAdk+Kmk1sAWYBay3vUvSqqp+LfBp4KuSdtBZMrrR9jOS3gF8ANghaXu1y0/Y/hZws6TFdJaT9gEfmdCZRUTEccnuXs6fvlqtltvt9lQPIyJiRpG0zXaruzxnAkdEFCoBEBFRqARAREShEgAREYVKAEREFCoBEBFRqARAREShEgAREYVKAEREFCoBEBFRqARAREShEgAREYVKAEREFCoBEBFRqARAREShEgAREYVKAEREFCoBEBFRqEYBIGmJpD2SBiWt6VF/uqRvSHpM0i5J1/XrK+kMSQ9IerK6nzsxU4qIiCb6BoCkWcAdwBXAxcBySRd3Nfso8ITtNwOXArdKmtOn7xrgQduLgAer7YiIOEmaHAFcAgza3mv7BWAjsLSrjYHTJAk4FTgCHO3TdymwoXq8Abh6PBOJiIixaRIA84H9te2hqqzuduCNwAFgB3C97Zf69D3b9kGA6v6sXk8uaaWktqT28PBwg+FGREQTTQJAPcrctf1uYDtwDrAYuF3Sqxv2PS7b62y3bLcGBgbG0jUiIo6jSQAMAQtr2wvovNOvuw64xx2DwI+BN/Tpe0jSPIDq/vDYhx8RESeqSQBsBRZJOl/SHGAZsLmrzVPAZQCSzgYuAvb26bsZWFE9XgHcO56JRETE2Mzu18D2UUmrgS3ALGC97V2SVlX1a4FPA1+VtIPOss+Ntp8B6NW32vVNwNclfZhOgFw7sVOLiIjjkT2mJfkp1Wq13G63p3oYEREziqRttlvd5TkTOCKiUAmAiIhCJQAiIgqVAIiIKFQCICKiUAmAiIhCJQAiIgqVAIiIKFQCICKiUAmAiIhCJQAiIgqVAIiIKFQCICKiUAmAiIhCJQAiIgqVAIiIKFQCICKiUAmAiIhCNQoASUsk7ZE0KGlNj/qPSdpe3XZKelHSGZIuqpVvl/SspBuqPp+S9HSt7soJnltERBxH3x+FlzQLuAO4HBgCtkrabPuJkTa2bwFuqdpfBfy27SPAEWBxbT9PA5tqu7/N9ucmZioRETEWTY4ALgEGbe+1/QKwEVh6nPbLgbt6lF8G/Mj2T8Y+zIiImGhNAmA+sL+2PVSVHUPSKcAS4O4e1cs4NhhWS3pc0npJc0fZ50pJbUnt4eHhBsONiIgmmgSAepR5lLZXAQ9Xyz9/twNpDvA+4M9qxV8ELqCzRHQQuLXXDm2vs92y3RoYGGgw3IiIaKJJAAwBC2vbC4ADo7Tt9S4f4ArgUduHRgpsH7L9ou2XgC/RWWqKiIiTpEkAbAUWSTq/eie/DNjc3UjS6cA7gXt77OOYzwUkzattXgPsbDroiIgYv77fArJ9VNJqYAswC1hve5ekVVX92qrpNcD9tp+v968+F7gc+EjXrm+WtJjOctK+HvURETGJZI+2nD/9tFott9vtqR5GRMSMImmb7VZ3ec4EjogoVAIgIqJQCYCIiEIlACIiCpUAiIgoVAIgIqJQCYCIiEIlACIiCpUAiIgoVAIgIqJQCYCIiEIlACIiCpUAiIgoVAIgIqJQCYCIiEIlACIiCpUAiIgoVKMAkLRE0h5Jg5LW9Kj/mKTt1W2npBclnVHV7ZO0o6pr1/qcIekBSU9W93MnbloREdFP3wCQNAu4A7gCuBhYLuniehvbt9hebHsx8HHgIdtHak1+raqv/yTZGuBB24uAB6vtiIg4SZocAVwCDNrea/sFYCOw9DjtlwN3NdjvUmBD9XgDcHWDPhERMUGaBMB8YH9te6gqO4akU4AlwN21YgP3S9omaWWt/GzbBwGq+7PGMvCIiBif2Q3aqEeZR2l7FfBw1/LP220fkHQW8ICkH9j+btMBVqGxEuB1r3td024REdFHkyOAIWBhbXsBcGCUtsvoWv6xfaC6PwxsorOkBHBI0jyA6v5wrx3aXme7Zbs1MDDQYLgREdFEkwDYCiySdL6kOXRe5Dd3N5J0OvBO4N5a2asknTbyGHgXsLOq3gysqB6vqPeLiIjJ13cJyPZRSauBLcAsYL3tXZJWVfVrq6bXAPfbfr7W/Wxgk6SR5/qa7fuqupuAr0v6MPAUcO1ETCgiIpqRPdpy/vTTarXcbrf7N4yIiJ+RtK3ra/hAzgSOiChWAiAiolAJgIiIQiUAIiIKlQCIiChUAiAiolAJgIiIQiUAIiIKlQCIiChUAiAiolAJgIiIQiUAIiIKlQCIiChUAiAiolAJgIiIQiUAIiIKlQCIiChUAiAiolCNAkDSEkl7JA1KWtOj/mOStle3nZJelHSGpIWSvi1pt6Rdkq6v9fmUpKdr/a6cyIlFRMTx9f1ReEmzgDuAy4EhYKukzbafGGlj+xbglqr9VcBv2z4i6ReA37X9qKTTgG2SHqj1vc325yZ4ThER0UCTI4BLgEHbe22/AGwElh6n/XLgLgDbB20/Wj1+DtgNzB/fkCMiYiI0CYD5wP7a9hCjvIhLOgVYAtzdo+484C3AI7Xi1ZIel7Re0txR9rlSUltSe3h4uMFwIyKiiSYBoB5lHqXtVcDDto/83A6kU+mEwg22n62KvwhcACwGDgK39tqh7XW2W7ZbAwMDDYYbERFNNAmAIWBhbXsBcGCUtsuoln9GSHolnRf/O23fM1Ju+5DtF22/BHyJzlJTREScJE0CYCuwSNL5kubQeZHf3N1I0unAO4F7a2UCvgLstv35rvbzapvXADvHPvyIiDhRfb8FZPuopNXAFmAWsN72Lkmrqvq1VdNrgPttP1/r/nbgA8AOSdursk/Y/hZws6TFdJaT9gEfGf90IiKiKdmjLedPP61Wy+12e6qHERExo0jaZrvVXZ4zgSMiCpUAiIgoVAIgIqJQCYCIiEIlACIiCpUAiIgoVAIgIqJQCYCIiEIlACIiCpUAiIgoVAIgIqJQCYCIiEIlACIiCpUAiIgoVAIgIqJQCYCIiEIlACIiCpUAiIgoVKMAkLRE0h5Jg5LW9Kj/mKTt1W2npBclnXG8vpLOkPSApCer+7kTN62IiOinbwBImgXcAVwBXAwsl3RxvY3tW2wvtr0Y+DjwkO0jffquAR60vQh4sNqOiIiTpMkRwCXAoO29tl8ANgJLj9N+OXBXg75LgQ3V4w3A1WMce0REjEOTAJgP7K9tD1Vlx5B0CrAEuLtB37NtHwSo7s8aZZ8rJbUltYeHhxsMNyIimmgSAOpR5lHaXgU8bPvICfTtyfY62y3brYGBgbF0jYiI42gSAEPAwtr2AuDAKG2X8XfLP/36HpI0D6C6P9xkwBERMTGaBMBWYJGk8yXNofMiv7m7kaTTgXcC9zbsuxlYUT1e0dUvIiIm2ex+DWwflbQa2ALMAtbb3iVpVVW/tmp6DXC/7ef79a2qbwK+LunDwFPAtRM1qYiI6E/2mJbkp1Sr1XK73Z7qYUREzCiSttludZfnTOCIiEIlACIiCpUAiIgoVAIgIqJQCYCIiEIlACIiCpUAiIgoVAIgIqJQCYCIiEIlACIiCpUAiIgoVAIgIqJQCYCIiEIlACIiCpUAiIgoVAIgIqJQCYCIiEIlACIiCtUoACQtkbRH0qCkNaO0uVTSdkm7JD1UlV1UlY3cnpV0Q1X3KUlP1+qunLBZRUREX31/FF7SLOAO4HJgCNgqabPtJ2ptXgN8AVhi+ylJZwHY3gMsru3naWBTbfe32f7cxEwlIiLGoskRwCXAoO29tl8ANgJLu9r8FnCP7acAbB/usZ/LgB/Z/sl4BhwREROjSQDMB/bXtoeqsroLgbmSviNpm6QP9tjPMuCurrLVkh6XtF7S3F5PLmmlpLak9vDwcIPhRkREE00CQD3K3LU9G/hl4D3Au4F/LenCn+1AmgO8D/izWp8vAhfQWSI6CNza68ltr7Pdst0aGBhoMNyIiGii72cAdN7xL6xtLwAO9GjzjO3ngeclfRd4M/DDqv4K4FHbh0Y61B9L+hLwzbEPPyIiTlSTI4CtwCJJ51fv5JcBm7va3Av8Q0mzJZ0CvA3YXatfTtfyj6R5tc1rgJ1jHXxERJy4vkcAto9KWg1sAWYB623vkrSqql9re7ek+4DHgZeAL9veCVAFwuXAR7p2fbOkxXSWk/b1qI+IiEkku3s5f/pqtVput9tTPYyIiBlF0jbbre7ynAkcEVGoBEBERKESABERhUoAREQUKgEQEVGoBEBERKESABERhUoAREQUKgEQEVGoBEBERKESABERhUoAREQUKgEQEVGoBEBERKESABERhUoAREQUKgEQEVGoRgEgaYmkPZIGJa0Zpc2lkrZL2iXpoVr5Pkk7qrp2rfwMSQ9IerK6nzv+6URERFN9A0DSLOAO4ArgYmC5pIu72rwG+ALwPttvAq7t2s2v2V7c9ZNka4AHbS8CHqy2IyLiJGlyBHAJMGh7r+0XgI3A0q42vwXcY/spANuHG+x3KbCherwBuLrRiCMiYkI0CYD5wP7a9lBVVnchMFfSdyRtk/TBWp2B+6vylbXys20fBKjuz+r15JJWSmpLag8PDzcYbkRENDG7QRv1KHOP/fwycBnw94H/Iel7tn8IvN32AUlnAQ9I+oHt7zYdoO11wDqAVqvV/bwREXGCmhwBDAELa9sLgAM92txn+3nbzwDfBd4MYPtAdX8Y2ERnSQngkKR5ANV9k2WjiIiYIE2OALYCiySdDzwNLKOz5l93L3C7pNnAHOBtwG2SXgW8wvZz1eN3Af+26rMZWAHcVN3f228g27Zte0bSTxqMebo5E3hmqgdxEpU2X8icSzFT53xur8K+AWD7qKTVwBZgFrDe9i5Jq6r6tbZ3S7oPeBx4Cfiy7Z2SXg9skjTyXF+zfV+165uAr0v6MPAUx35zqNdYBvq1mY4ktbu+AfWyVtp8IXMuxcttzrKzrD7ZXm7/NP2UNl/InEvxcptzzgSOiChUAuDkWDfVAzjJSpsvZM6leFnNOUtAERGFyhFAREShEgAREYVKAEyAplc27XdVVUn/UpIlnTn5ox6f8c5Z0i2SfiDpcUmbqgsKTksN/m6S9B+q+sclvbVp3+nqROcsaaGkb0vaXV0Z+PqTP/oTM56/c1U/S9L/lPTNkzfqcbKd2zhvwM3AmurxGuCzPdrMAn4EvJ7OyXKPARfX6hfSOdfiJ8CZUz2nyZ4znZMCZ1ePP9ur/3S49fu7VW2uBP6czmVT/gHwSNO+0/E2zjnPA95aPT4N+OHLfc61+t8BvgZ8c6rn0/SWI4CJ0eTKpv2uqnob8Hsce52l6Wpcc7Z9v+2jVbvv0bnEyHTU5Gq4S4E/dsf3gNdUlzdp0nc6OuE52z5o+1EA288Buzn24pHT0Xj+zkhaALwH+PLJHPR4JQAmRpMrm456VVVJ7wOetv3YZA90Ao1rzl0+ROed1XTUZA6jtWk6/+lmPHP+GUnnAW8BHpn4IU648c75D+m8gXtpksY3KZpcCygASX8BvLZH1Seb7qJHmSWdUu3jXSc6tskyWXPueo5PAkeBO8c2upOmydVwR2vTpO90NJ45dyqlU4G7gRtsPzuBY5ssJzxnSe8FDtveJunSiR7YZEoANGT7N0ark3Ro5PD3OFc2He2qqhcA5wOPVddMWgA8KukS2z+dsAmcgEmc88g+VgDvBS5ztYg6DTW9Gm6vNnMa9J2OxjNnJL2Szov/nbbvmcRxTqTxzPn9wPskXQn8PeDVkv7E9j+ZxPFOjKn+EOLlcANu4ec/EL25R5vZwF46L/YjHzK9qUe7fcyMD4HHNWdgCfAEMDDVc+kzz75/Nzprv/UPB78/lr/5dLuNc84C/hj4w6mex8mac1ebS5lBHwJP+QBeDjfgF+n8rvGT1f0ZVfk5wLdq7a6k862IHwGfHGVfMyUAxjVnYJDOeur26rZ2qud0nLkeMwdgFbCqeiw6v5v9I2AH0BrL33w63k50zsA76CydPF7721451fOZ7L9zbR8zKgByKYiIiELlW0AREYVKAEREFCoBEBFRqARAREShEgAREYVKAEREFCoBEBFRqP8Pnm1+oBt/HRkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kernelSizes=[2,3,4,5]\n",
    "n_epochs = 1\n",
    "\n",
    "for kernelSize in kernelSizes:\n",
    "    \n",
    "#             #now plot this\n",
    "#     plt.figure()\n",
    "#     plt.plot(trainAccuracy,'k')\n",
    "#     plt.plot(testAccuracy,'r')\n",
    "#     plt.title(\"Kernel size: \"+str(kernelSize))\n",
    "    #instantiate your CNN using kernel size\n",
    "    model = myCNN()\n",
    "#     model\n",
    "    \n",
    "    #define optimizer and loss function\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #train and test - you can copy the code \n",
    "    train_loss, val_loss = [], []\n",
    "    accuracy_total_train, accuracy_total_val = [], []\n",
    "            \n",
    "    trainAccuracy=list(accuracy_total_train)\n",
    "    testAccuracy=list(accuracy_total_val)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "\n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        total = 0\n",
    "            # training our model\n",
    "        for idx, (image, label) in enumerate(train_loader):\n",
    "\n",
    "    #         image, label = image.to(device), label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = model(image)\n",
    "\n",
    "            loss = criterion(pred, label)\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "            for i, p in enumerate(pred):\n",
    "                if label[i] == torch.max(p.data, 0)[1]:\n",
    "                    total = total + 1\n",
    "\n",
    "\n",
    "        accuracy_train = total / len(train_set)\n",
    "        accuracy_total_train.append(accuracy_train)\n",
    "\n",
    "        total_train_loss = total_train_loss / (idx + 1)\n",
    "        train_loss.append(total_train_loss)\n",
    "\n",
    "        # validating our model\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        for idx, (image, label) in enumerate(test_loader):\n",
    "    #         image, label = image, label.cuda()\n",
    "            pred = model(image)\n",
    "            loss = criterion(pred, label)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "            for i, p in enumerate(pred):\n",
    "                if label[i] == torch.max(p.data, 0)[1]:\n",
    "                    total = total + 1\n",
    "\n",
    "        accuracy_val = total / len(test_set)\n",
    "        accuracy_total_val.append(accuracy_val)\n",
    "\n",
    "        total_val_loss = total_val_loss / (idx + 1)\n",
    "        val_loss.append(total_val_loss)\n",
    "\n",
    "        if epoch % 2 == 0:\n",
    "            print(\"Kernel Size:\", kernelSize,\"Epoch: {}/{}  \".format(epoch, n_epochs),\n",
    "                    \"Training loss: {:.4f}  \".format(total_train_loss),\n",
    "                    \"Testing loss: {:.4f}  \".format(total_val_loss),\n",
    "                    \"Train accuracy: {:.4f}  \".format(accuracy_train),\n",
    "                    \"Test accuracy: {:.4f}  \".format(accuracy_val))\n",
    "#     trainAccuracy=list(accuracy_total_train)\n",
    "#     testAccuracy=list(accuracy_total_val)\n",
    "#             #now plot this\n",
    "        plt.figure()\n",
    "        plt.plot(accuracy_total_train,'k')\n",
    "        plt.plot(accuracy_total_val,'r')\n",
    "        plt.title(\"Kernel size: \"+str(kernelSize))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Compare the plots from above and explain which kernel size worked best, hypothesize about why?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your answer here: I felt its better to have the kernel size as 3 or 4 which gave the best output for the fever epochs as we increase the kernel size the model is geting to be over fitted and loosing the accuracy. Pratically I was not able to prove them with my plots but i could see that from my results.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now pick a random image and pipe it through your trained CNN\n",
    "extract the output of the convolutional layer for each output channel and visualize that as an image\n",
    "Observe that the template CNN has a self.features variable ... that might help here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Can you guess the feature that each channel extracted? short text below"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
